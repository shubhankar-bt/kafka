


package com.fincore.JournalService.config;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;

@Configuration
public class AppConfig {

	@Bean
	@Primary
	public ObjectMapper objectMapper() {
		ObjectMapper mapper = new ObjectMapper();

		mapper.registerModule(new JavaTimeModule());

		mapper.disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS);

		return mapper;
	}
}





package com.fincore.JournalService.config;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.scheduling.annotation.EnableAsync;
import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;
import java.util.concurrent.Executor;

@Configuration
@EnableAsync
public class AsyncConfig {

    @Bean(name = "bulkExecutor")
    public Executor bulkExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(5);   // Steady state
        executor.setMaxPoolSize(20);   // Burst capability
        executor.setQueueCapacity(500); // Holds 500 batches in memory if DB is slow
        executor.setThreadNamePrefix("JournalAsync-");
        executor.initialize();
        return executor;
    }
}




package com.fincore.JournalService.config;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.redis.connection.RedisConnectionFactory;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.data.redis.serializer.StringRedisSerializer;

/**
 * Configuration for Redis to handle compressed binary data.
 * This prevents Out-Of-Memory errors by offloading large datasets to Redis.
 */
@Configuration
public class RedisConfig {

    @Bean(name = "byteArrayRedisTemplate")
    public RedisTemplate<String, byte[]> byteArrayRedisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, byte[]> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);

        // Keys are Strings (e.g., "JRNL_DATA_UUID") - Readable in Redis CLI
        template.setKeySerializer(new StringRedisSerializer());

        // Values are raw Byte Arrays (GZIP Compressed Data) - No serialization overhead
        template.setEnableDefaultSerializer(false);

        return template;
    }
}







package com.fincore.JournalService.config;


import org.springframework.cache.annotation.EnableCaching;
import org.springframework.cache.concurrent.ConcurrentMapCacheManager;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
@EnableCaching
public class CacheConfig {

    @Bean
    public ConcurrentMapCacheManager cacheManager() {
      
        return new ConcurrentMapCacheManager("notification_configs");
    }
}






package com.fincore.JournalService.config;

import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.boot.jdbc.DataSourceBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.jdbc.core.JdbcTemplate;

import javax.sql.DataSource;

/**
 * SECONDARY DATASOURCE CONFIGURATION (Hive/Thrift)
 * * Configures the connection to the Data Lake (Delta Lake).
 * Uses JdbcTemplate for direct SQL execution, suitable for analytics queries.
 */
@Configuration
public class HiveDbConfig {

    @Bean(name = "hiveDataSource")
    @ConfigurationProperties(prefix = "spring.datasource.hive")
    public DataSource hiveDataSource() {
        return DataSourceBuilder.create().build();
    }

    @Bean(name = "hiveJdbcTemplate")
    public JdbcTemplate hiveJdbcTemplate(@Qualifier("hiveDataSource") DataSource dataSource) {
        return new JdbcTemplate(dataSource);
    }
}










package com.fincore.JournalService.config;

import jakarta.persistence.EntityManagerFactory;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.boot.jdbc.DataSourceBuilder;
import org.springframework.boot.orm.jpa.EntityManagerFactoryBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import org.springframework.data.jpa.repository.config.EnableJpaRepositories;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.orm.jpa.JpaTransactionManager;
import org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean;
import org.springframework.transaction.PlatformTransactionManager;
import org.springframework.transaction.annotation.EnableTransactionManagement;
import javax.sql.DataSource;
import java.util.HashMap;
import java.util.Map;

/**
 * PRIMARY DATASOURCE CONFIGURATION (Oracle)
 * * This class ensures that all existing JPA Repositories and Entities
 * continue to work using the Oracle database.
 * * We mark beans as @Primary so Spring injects this datasource by default
 * into existing services.
 */
@Configuration
@EnableTransactionManagement
@EnableJpaRepositories(basePackages = "com.fincore.JournalService.Repository", // Location of existing Repos
        entityManagerFactoryRef = "oracleEntityManagerFactory", transactionManagerRef = "oracleTransactionManager")
public class OracleDbConfig {
    @Primary
    @Bean(name = "oracleDataSource")
    @ConfigurationProperties(prefix = "spring.datasource.oracle")
    public DataSource oracleDataSource() {
        return DataSourceBuilder.create().build();
    }

    @Primary
    @Bean(name = "oracleEntityManagerFactory")
    public LocalContainerEntityManagerFactoryBean oracleEntityManagerFactory(
            EntityManagerFactoryBuilder builder,
            @Qualifier("oracleDataSource") DataSource dataSource) {

        Map<String, Object> properties = new HashMap<>();
        properties.put("hibernate.dialect", "org.hibernate.dialect.OracleDialect");
        properties.put("hibernate.hbm2ddl.auto", "update"); // Or 'validate' for prod

        return builder
                .dataSource(dataSource)
                .packages("com.fincore.JournalService.Models") // Location of existing Entities
                .persistenceUnit("oracle")
                .properties(properties)
                .build();
    }

    @Primary
    @Bean(name = "oracleTransactionManager")
    public PlatformTransactionManager oracleTransactionManager(
            @Qualifier("oracleEntityManagerFactory") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }
    @Primary
    @Bean(name = "oracleJdbcTemplate")
    public JdbcTemplate oracleJdbcTemplate(@Qualifier("oracleDataSource") DataSource dataSource) {
        return new JdbcTemplate(dataSource);
    }

}






package com.fincore.JournalService.config;

import jakarta.persistence.AttributeConverter;
import jakarta.persistence.Converter;
import com.fincore.JournalService.Models.enums.RequestStatus;
import java.util.stream.Stream;

@Converter(autoApply = true)
public class RequestStatusConverter implements AttributeConverter<RequestStatus, String> {

    @Override
    public String convertToDatabaseColumn(RequestStatus status) {
        if (status == null) {
            return null;
        }
        return status.getCode(); // Writes "P", "A", "R" to DB
    }

    @Override
    public RequestStatus convertToEntityAttribute(String code) {
        if (code == null) {
            return null;
        }
        return Stream.of(RequestStatus.values())
                .filter(c -> c.getCode().equals(code))
                .findFirst()
                .orElseThrow(() -> new IllegalArgumentException("Unknown status code: " + code));
    }
}









package com.fincore.JournalService.config;

import com.fincore.commonutilities.config.CommonSecurityConfig;
import com.fincore.commonutilities.config.RedisConfig;
import com.fincore.commonutilities.jwt.JwtUtil;
import com.fincore.commonutilities.security.ContextRbacFilter;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Import;
import org.springframework.data.redis.core.StringRedisTemplate;
import org.springframework.security.config.annotation.web.builders.HttpSecurity;
import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;
import org.springframework.security.config.http.SessionCreationPolicy;
import org.springframework.security.web.SecurityFilterChain;
import org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter;

/**
 * Common Security Configuration.
 * * Aligned with the "Distributed Gateway" architecture.
 * It uses the ContextRbacFilter from Common Utilities to enforce:
 * 1. Token Validity
 * 2. Single Session (Redis check)
 * 3. RBAC Permissions
 */
@Configuration
@EnableWebSecurity
@Import({RedisConfig.class, JwtUtil.class, CommonSecurityConfig.class }) // Import logic from JAR
public class SecurityConfig {

    @Autowired
    private StringRedisTemplate redisTemplate;

    @Autowired
    private JwtUtil jwtUtil;

    @Autowired
    private CommonSecurityConfig commonSecurityConfig; // Wire in the CORS config

    @Bean
    public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
        http
                // 1. Disable CSRF (Stateless API)
                .csrf(csrf -> csrf.disable())

                // 2. Apply Centralized CORS Policy
                .cors(cors -> cors.configurationSource(commonSecurityConfig.corsConfigurationSource()))

                // 3. Stateless Session (No JSESSIONID)
                .sessionManagement(s -> s.sessionCreationPolicy(SessionCreationPolicy.STATELESS))

                // 4. Authorization Rules
                .authorizeHttpRequests(authz -> authz
                        // Public Endpoints
                        .requestMatchers("/actuator/**", "/auth/**", "/error").permitAll()
                        // All other endpoints require Authentication (and RBAC filter check)
                        .anyRequest().authenticated()
                )

                // 5. Add the "Distributed Gateway" Filter
                .addFilterBefore(new ContextRbacFilter(redisTemplate, jwtUtil), UsernamePasswordAuthenticationFilter.class);

        return http.build();
    }
}










--***********************************************************************************************
controllers: 
package com.fincore.JournalService.Controllers;

import lombok.RequiredArgsConstructor;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

import com.fincore.JournalService.Dto.BranchDto;
import com.fincore.JournalService.Service.BranchService;

import java.util.Collections; 
import java.util.List;
import java.util.Map;

@RestController
@RequestMapping("/api/branches-journal")
@RequiredArgsConstructor
public class BranchController {

	private final BranchService branchService;

	@GetMapping("/map")
	public Map<String, String> getBranchMap() {
		return branchService.getBranchCodeAndNameMap();
	}

	@GetMapping("/search")
	public List<BranchDto> searchBranches(@RequestParam String query) {
       
        if (query == null || query.trim().isEmpty()) {
            return Collections.emptyList();
        }
     
		return branchService.searchBranches(query);
	}
}













package com.fincore.JournalService.Controllers;


import com.fincore.JournalService.Dto.CglDto;
import com.fincore.JournalService.Service.CglService;
import lombok.RequiredArgsConstructor;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

import java.util.List;


@RestController
@RequestMapping("/api/cgl-journal")
@RequiredArgsConstructor
public class CglController {

    private final CglService cglService;

    @GetMapping("/search")
    public ResponseEntity<List<CglDto>> searchCgl(@RequestParam(value = "query", required = false, defaultValue = "") String query) {
        return ResponseEntity.ok(cglService.searchCgls(query));
    }
}















package com.fincore.JournalService.Controllers;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fincore.JournalService.Dto.*;
import com.fincore.JournalService.Models.JournalRequest;
import com.fincore.JournalService.Service.JournalBulkValidationService;
import com.fincore.JournalService.Service.JournalRequestService;
import com.fincore.commonutilities.jwt.JwtUtil;
import jakarta.servlet.http.HttpServletRequest;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.core.io.ByteArrayResource;
import org.springframework.core.io.Resource;
import org.springframework.data.domain.PageRequest;
import org.springframework.http.HttpHeaders;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;
import org.springframework.web.multipart.MultipartFile;

import java.io.IOException;
import java.time.LocalDate;
import java.time.format.DateTimeFormatter;
import java.util.List;
import java.util.Map;

/**
 * Controller for Journal Request Management.
 * Implements High-Performance Async Endpoints for Bulk Operations.
 */
@RestController
@RequestMapping("/api/journals")
@RequiredArgsConstructor
@Slf4j
public class JournalRequestController {

    private final JournalRequestService journalRequestService;
    private final JournalBulkValidationService journalBulkValidationService;
    private final JwtUtil jwtUtil;

    // ==================================================================================
    // 1. ASYNC BULK OPERATIONS (Fire-and-Forget)
    // ==================================================================================

    /**
     * Create Bulk Batch from Cached Validation Data.
     * Response: Immediate (HTTP 202) with Batch ID. Processing happens in background.
     */
    @PostMapping("/create-batch-from-cache")
    public ResponseEntity<Map<String, Object>> createBatchFromCache(
            @RequestBody Map<String, String> payload,
            @RequestHeader("Authorization") String token) {

        try {
            String userId = jwtUtil.getUserIdFromToken(token);
            Integer userRole = jwtUtil.getUserRoleFromToken(token);

            String batchId = journalRequestService.createBatchFromCacheAsync(
                    payload.get("requestId"),
                    payload.get("commonBatchRemarks"),
                    userId,
                    userRole
            );

            return ResponseEntity.status(HttpStatus.ACCEPTED).body(Map.of(
                    "status", "PROCESSING",
                    "message", "Batch creation initiated in background.",
                    "batchId", batchId
            ));

        } catch (IllegalStateException e) {
            // Handles Redis Lock collisions (Duplicate clicks)
            return ResponseEntity.status(HttpStatus.CONFLICT).body(Map.of("status", "ERROR", "message", e.getMessage()));
        } catch (Exception e) {
            log.error("Batch Init Error", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(Map.of("status", "ERROR", "message", "System error: " + e.getMessage()));
        }
    }

    /**
     * Process (Approve/Reject) Bulk Batch.
     * Response: Immediate (HTTP 202). Uses Oracle Parallel execution in background.
     */
    @PostMapping("/process-bulk")
    public ResponseEntity<?> processBulkRequests(
            @RequestHeader("Authorization") String token,
            @Valid @RequestBody BulkProcessJournalRequestDto dto) {
        try {
            String userId = jwtUtil.getUserIdFromToken(token);
            Integer userRole = jwtUtil.getUserRoleFromToken(token);

            journalRequestService.processBulkRequestsAsync(dto, userId, userRole);

            return ResponseEntity.status(HttpStatus.ACCEPTED).body(Map.of(
                    "status", "PROCESSING",
                    "message", "Approval process started. Notifications will be sent upon completion."
            ));
        } catch (Exception e) {
            log.error("Process Error", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(Map.of("status", "ERROR", "message", "Processing failed."));
        }
    }

    /**
     * Delete/Cancel Batch Asynchronously.
     * Prevents DB Timeouts on large batches by deleting in chunks.
     */
    @DeleteMapping("/my-requests/by-batch/{batchId}")
    public ResponseEntity<?> cancelMyRequestsByBatch(
            @RequestHeader("Authorization") String token,
            @PathVariable String batchId) {
        try {
            String userId = jwtUtil.getUserIdFromToken(token);
            journalRequestService.cancelMyRequestsByBatchIdAsync(batchId, userId);

            return ResponseEntity.status(HttpStatus.ACCEPTED).body(Map.of(
                    "status", "DELETING",
                    "message", "Batch deletion queued. Status will update shortly."
            ));
        } catch (Exception e) {
            log.error("Delete Batch Error", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(Map.of("status", "ERROR", "message", "Cancel failed: " + e.getMessage()));
        }
    }

    // ==================================================================================
    // 2. SAFEGUARDED FETCH API
    // ==================================================================================

    /**
     * Get All Requests for a Batch.
     * SAFEGUARD: If batch > 2000 rows, forces Frontend to use Pagination to prevent Server OOM.
     */
    @GetMapping("/by-batch/{batchId}")
    public ResponseEntity<?> getRequestsByBatchId(@PathVariable String batchId) {
        long count = journalRequestService.getRequestCountByBatchId(batchId);

        if (count > 2000) {
            return ResponseEntity.status(HttpStatus.PAYLOAD_TOO_LARGE).body(Map.of(
                    "error", "Batch too large (" + count + " rows). Please use Paginated API.",
                    "suggestion", "/api/journals/by-batch-paginated/" + batchId
            ));
        }
        return ResponseEntity.ok(journalRequestService.getRequestsByBatchId(batchId));
    }

    // ==================================================================================
    // 3. VALIDATION & STATUS
    // ==================================================================================

    @PostMapping(value = "/bulk-validate-init", consumes = MediaType.MULTIPART_FORM_DATA_VALUE)
    public ResponseEntity<?> initiateValidation(
            @RequestParam("file") MultipartFile file,
            @RequestParam("postingDate") String date,
            HttpServletRequest request) {
        try {
            if (file == null || file.isEmpty())
                return ResponseEntity.badRequest().body(Map.of("error", "File is missing"));

            String reqId = journalBulkValidationService.initiateValidation(
                    file.getBytes(),
                    file.getOriginalFilename(),
                    LocalDate.parse(date)
            );
            return ResponseEntity.ok(Map.of("status", "QUEUED", "requestId", reqId));
        } catch (Exception e) {
            return ResponseEntity.badRequest().body(Map.of("error", e.getMessage()));
        }
    }

    @GetMapping("/bulk-status/{requestId}")
    public ResponseEntity<BulkUploadStateDto> checkStatus(@PathVariable String requestId) {
        BulkUploadStateDto state = journalBulkValidationService.getState(requestId);
        return state != null ? ResponseEntity.ok(state) : ResponseEntity.notFound().build();
    }

    // ==================================================================================
    // 4. UTILS & LEGACY SUPPORT
    // ==================================================================================

    @GetMapping("/current-posting-date")
    public String getCurrentPostingDate() {
        return journalRequestService.getCurrentPostingDate().format(DateTimeFormatter.ISO_LOCAL_DATE);
    }

    @GetMapping("/pending-requests-summary")
    public ResponseEntity<?> getPendingBatchSummaries() {
        return ResponseEntity.ok(journalRequestService.getPendingBatchSummaries());
    }

    @GetMapping("/all-requests-summary")
    public ResponseEntity<?> getAllBatchSummaries() {
        return ResponseEntity.ok(journalRequestService.getAllBatchSummaries());
    }

    @GetMapping("/by-batch-paginated/{batchId}")
    public ResponseEntity<?> getRequestsByBatchIdPaginated(
            @PathVariable String batchId,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "10") int size) {
        return ResponseEntity.ok(journalRequestService.getRequestsByBatchIdPaginated(batchId, PageRequest.of(page, size)));
    }

    @GetMapping("/download-bulk-file/{requestId}")
    public ResponseEntity<Resource> downloadFile(@PathVariable String requestId, @RequestParam String type) {
        byte[] data = journalBulkValidationService.getFileBytes(requestId, type);
        if (data == null) return ResponseEntity.notFound().build();

        String filename = type.equalsIgnoreCase("ERROR") ? "Error_Report.xlsx" : "Success.csv";
        MediaType mediaType = type.equalsIgnoreCase("ERROR")
                ? MediaType.parseMediaType("application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
                : MediaType.TEXT_PLAIN;

        return ResponseEntity.ok()
                .header(HttpHeaders.CONTENT_DISPOSITION, "attachment; filename=\"" + filename + "\"")
                .contentType(mediaType)
                .body(new ByteArrayResource(data));
    }

    @GetMapping("/download-template")
    public ResponseEntity<Resource> downloadTemplate() throws IOException {
        return ResponseEntity.ok()
                .header(HttpHeaders.CONTENT_DISPOSITION, "attachment; filename=\"Journal_Template.xlsx\"")
                .body(new ByteArrayResource(journalBulkValidationService.generateTemplateBytes()));
    }

    // --- Manual/Small Batch Endpoints ---
    @PostMapping("/create-batch")
    public ResponseEntity<?> createBatchRequest(@Valid @RequestBody BatchRequestDto batchDto, @RequestHeader("Authorization") String token) throws JsonProcessingException {
        return ResponseEntity.status(HttpStatus.CREATED).body(journalRequestService.createBatchRequest(batchDto, jwtUtil.getUserIdFromToken(token), jwtUtil.getUserRoleFromToken(token)));
    }

    @GetMapping("/my-requests")
    public List<JournalRequest> getMyRequests(@RequestHeader("Authorization") String token) {
        return journalRequestService.getMyRequests(jwtUtil.getUserIdFromToken(token));
    }

    @GetMapping("/pending-requests")
    public List<JournalRequest> getPendingRequests(@RequestHeader("Authorization") String token) {
        return journalRequestService.getPendingRequests(jwtUtil.getUserIdFromToken(token), jwtUtil.getUserRoleFromToken(token));
    }

    @PatchMapping("/update-request")
    public JournalRequest updateRequestStatus(@RequestHeader("Authorization") String token, @RequestBody ProcessJournalRequestDto dto) throws JsonProcessingException {
        return journalRequestService.updateRequestStatus(dto, jwtUtil.getUserIdFromToken(token), jwtUtil.getUserRoleFromToken(token)).get();
    }

    @DeleteMapping("/my-request/{requestId}")
    public ResponseEntity<Void> cancelMyRequest(@RequestHeader("Authorization") String token, @PathVariable Long requestId) {
        journalRequestService.cancelMyRequest(requestId, jwtUtil.getUserIdFromToken(token));
        return ResponseEntity.noContent().build();
    }

    @DeleteMapping("/my-requests/by-journal-list")
    public ResponseEntity<?> cancelMyRequestsByJournalPrefixes(@RequestHeader("Authorization") String token, @RequestBody List<String> list) {
        journalRequestService.cancelMyRequestsByJournalPrefixes(list, jwtUtil.getUserIdFromToken(token));
        return ResponseEntity.ok(Map.of("status", "SUCCESS"));
    }

    @GetMapping("/status")
    public ResponseEntity<List<JournalRequestStatusDto>> getJournalStatusList() {
        return ResponseEntity.ok(journalRequestService.getJournalRequestStatusList());
    }
}




















// ***************************************************************************************************
dtos :




package com.fincore.JournalService.Dto;

import com.fincore.JournalService.Models.enums.ChangeType;
import jakarta.validation.Valid;
import jakarta.validation.constraints.NotNull;
import jakarta.validation.constraints.Size;
import java.math.BigDecimal;
import java.time.LocalDate;
import lombok.Data;
import java.util.List;

@Data
public class BatchRequestDto {

    @Data
    public static class JournalRequestRow {
        @NotNull
        private ChangeType changeType = ChangeType.ADD;

        private Long masterJournalId;

        private LocalDate csvDate;

        @NotNull @Size(min = 1, max = 50)
        private String branch;

        @NotNull @Size(min = 3, max = 3)
        private String currency;

        @NotNull @Size(min = 1, max = 50)
        private String cgl;

        @NotNull
        private BigDecimal amount;

        @Size(max = 20)
        private String productType;

        @Size(max = 200)
        private String remarks;

        @Size(min = 1, max = 1)
        private String arFlag = "A";

        @Size(min = 1, max = 1)
        private String acClassification;
    }

    @Size(max = 200)
    private String commonBatchRemarks;

    @Valid
    @NotNull
    private List<JournalRequestRow> rows;
}










package com.fincore.JournalService.Dto;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;


@Data
@NoArgsConstructor
@AllArgsConstructor
public class BranchDto {
    private String code;
    private String name;
    
}









package com.fincore.JournalService.Dto;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.io.Serializable;

@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class TaskProgressDto implements Serializable {
    private String taskId;      // Unique Job ID (e.g., Batch ID)
    private String userId;      // The User receiving the progress
    private int percentage;     // 0-100
    private String status;      // PROCESSING, COMPLETED, FAILED
    private String message;     // User-facing update (e.g., "Row 500/1000...")
}

















package com.fincore.JournalService.Dto;

import java.util.List;

import com.fincore.JournalService.Models.enums.RequestStatus;

import jakarta.validation.constraints.NotNull;
import jakarta.validation.constraints.Size;
import lombok.Data;

@Data
public class BulkProcessJournalRequestDto {
     
    private String batchId;
    
    private List<String> journalIdPrefixes;

    @NotNull
    private RequestStatus status; 
    
    @Size(max = 50)
    private String remarks;
}










package com.fincore.JournalService.Dto;

import lombok.Data;
import java.io.Serializable;

@Data
public class BulkUploadStateDto implements Serializable {
    private String requestId;
    private String status; // PROCESSING, SUCCESS, ERROR
    private Integer currentStage; // 1..4
    private Integer totalRows;
    private Long errorCount;
    private String message;
    private String previewDataJson;

    private boolean hasErrorFile;
    private boolean hasSuccessFile;

    private transient String errorFilePath;
    private transient String successFilePath;
}













package com.fincore.JournalService.Dto;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;


@Data
@NoArgsConstructor
@AllArgsConstructor
public class CglDto {
    private String cglNumber;
    private String description;
}






package com.fincore.JournalService.Dto;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;
import java.math.BigDecimal;
import java.time.LocalDate;

@Data
@AllArgsConstructor
@NoArgsConstructor
public class HdfsSyncDto {
    private String branch;
    private String currency;
    private String cgl;
    private LocalDate balanceDate;
    private BigDecimal newBalance;
    private BigDecimal newInrBalance;
}






package com.fincore.JournalService.Dto;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;
import lombok.Data;
import lombok.Getter;
import lombok.Setter;
import jakarta.validation.constraints.NotNull;
import jakarta.validation.constraints.Size;
import java.math.BigDecimal;
import java.time.LocalDate;

import com.fincore.JournalService.Models.enums.ChangeType;

@Data
@Getter
@Setter
@JsonIgnoreProperties(ignoreUnknown = true)
public class CreateJournalRequestDto {

    @NotNull
    private ChangeType changeType;

    private Long masterJournalId;

    @NotNull
    @JsonProperty("pDate")
    private LocalDate pDate;

    @NotNull @Size(min = 1, max = 50)
    private String branch;

    @NotNull @Size(min = 3, max = 3)
    private String currency;

    @NotNull @Size(min = 1, max = 50)
    private String cgl;

    @NotNull
    private BigDecimal amount;

    @Size(max = 20)
    private String productType;

    @Size(max = 50)
    private String remarks;

    @Size(min = 1, max = 1)
    private String arFlag;

    @Size(min = 1, max = 1)
    private String acClassification;

    @NotNull @Size(min = 1, max = 50)
    private String batchId;

    @NotNull @Size(min = 1, max = 50)
    private String journalId;

    @Size(max = 50)
    private String commonBatchRemarks;

    private Integer transactionCount;

    private String transactionType;
}














package com.fincore.JournalService.Dto;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.math.BigDecimal;
import java.time.LocalDate;

@Data
@AllArgsConstructor
@NoArgsConstructor
public class GlAggregatedDataDto {
    private Long id; // Add this field
    private String branch;
    private String currency;
    private String cgl;
    private LocalDate txnDate;
    private BigDecimal rawAmount;
    private BigDecimal convertedAmount;

    // Existing Constructor used in map (id will be null initially)
    public GlAggregatedDataDto(String branch, String currency, String cgl, LocalDate txnDate, BigDecimal rawAmount, BigDecimal convertedAmount) {
        this.branch = branch;
        this.currency = currency;
        this.cgl = cgl;
        this.txnDate = txnDate;
        this.rawAmount = rawAmount;
        this.convertedAmount = convertedAmount;
    }
}












package com.fincore.JournalService.Dto;

import lombok.Data;
import java.time.LocalDateTime;

import com.fincore.JournalService.Models.JournalRequest;
import com.fincore.JournalService.Models.enums.RequestStatus;

@Data
public class JournalRequestStatusDto {

    private RequestStatus requestStatus;
    private LocalDateTime requestDate;
    private String creatorId;
    private String executorId;
    private String executorRemarks;
    private String batchId;
    private String journalId;

    // A helper constructor 
    public JournalRequestStatusDto(JournalRequest request) {
        this.requestStatus = request.getRequestStatus();
        this.requestDate = request.getRequestDate();
        this.creatorId = request.getCreatorId();
        this.executorId = request.getExecutorId();
        this.executorRemarks = request.getExecutorRemarks();
        this.batchId = request.getBatchId();
        this.journalId = request.getJournalId();
    }
}














package com.fincore.JournalService.Dto;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;

@Data
@AllArgsConstructor
@NoArgsConstructor
public class NotificationConfigDto {
    private String targetUrl;
    private String targetRoles; // Comma separated string: "51,52,55"
}











package com.fincore.JournalService.Dto;

import com.fincore.JournalService.Models.enums.RequestStatus;

import jakarta.persistence.Column;
import jakarta.validation.constraints.Size;
import lombok.Data;
import lombok.Getter;
import lombok.Setter;

@Data
@Getter
@Setter
public class ProcessJournalRequestDto {
	@Column(nullable = false)
    private Long requestId;
    
    @Column(nullable = false)
    private RequestStatus status; 
    
    @Size(max = 50)
    private String remarks;

}






********************************************************************************************




package com.fincore.JournalService.Exception;


import org.springframework.http.HttpStatus;
import org.springframework.web.bind.annotation.ResponseStatus;

@ResponseStatus(HttpStatus.NOT_FOUND)
public class ResourceNotFoundException extends RuntimeException {
    public ResourceNotFoundException(String message) {
        super(message);
    }
}




********************************************************************************************

models:



package com.fincore.JournalService.Models.enums;

import lombok.Getter;

import java.util.Arrays;
import java.util.Map;
import java.util.stream.Collectors;

@Getter
public enum AcClassification {
	  ASSET('A', "Asset"),
	    LIABILITY('L', "Liability"),
	    INCOME('I', "Income"),
	    EXPENSE('E', "Expense"),
	    MEMO('M', "Memo A/c");

	    private final char code;
	    private final String description;

	    AcClassification(char code, String description) {
	        this.code = code;
	        this.description = description;
	    }

	  
	    public static Map<String, String> getClassificationMap() {
	        return Arrays.stream(AcClassification.values())
	                .collect(Collectors.toMap(
	                        ac -> String.valueOf(ac.getCode()), 
	                        AcClassification::getDescription
	                ));
	    }
}







package com.fincore.JournalService.Models.enums;

import lombok.Getter;

@Getter
public enum ChangeType {
	    ADD("A"),
	    UPDATE("U"),
	    DELETE("D");

	    private final String code;

	    ChangeType(String code) {
	        this.code = code;
	    }
}





package com.fincore.JournalService.Models.enums;


import lombok.Getter;

@Getter
public enum RequestStatus {
    PENDING("P"),
    ACCEPTED("A"),
    REJECTED("R");

    private final String code;

    RequestStatus(String code) {
        this.code = code;
    }
}





package com.fincore.JournalService.Models;

import jakarta.persistence.Column;
import jakarta.persistence.Entity;
import jakarta.persistence.Id;
import jakarta.persistence.Table;
import lombok.Data;
import java.time.LocalDate;


@Entity
@Table(name = "BRANCH_MASTER")
@Data
public class BranchMaster {

    @Id
    @Column(name = "CODE" )
    private String code;

    @Column(name = "NAME")
    private String name;

    @Column(name = "CIRCLE_CODE")
    private String circleCode;

    @Column(name = "STATE")
    private String state;

    @Column(name = "CITY")
    private String city;

    @Column(name = "ADDRESS")
    private String address;

    @Column(name = "PINCODE")
    private String pincode;

    @Column(name = "PHONE_NUMBER")
    private String phoneNumber;

    @Column(name = "EMAIL_ID")
    private String emailId;

    @Column(name = "NMR_CODE" )
    private String nmrCode;

    @Column(name = "STATUS")
    private Integer status;

    @Column(name = "OPEN_DATE")
    private LocalDate openDate;

    @Column(name = "CLOSE_DATE")
    private LocalDate closeDate;

    @Column(name = "MERGE_DATE")
    private LocalDate mergeDate;

    @Column(name = "MERGED_WITH_BRANCH" )
    private String mergedWithBranch;

    @Column(name = "LAST_CHANGE_DATE")
    private LocalDate lastChangeDate;

    @Column(name = "CPC_FLAG")
    private Boolean cpcFlag;

    @Column(name = "FOOD_CREDIT_FLAG")
    private Boolean foodCreditFlag;

    @Column(name = "CURR_CHEST_FLAG")
    private Boolean currChestFlag;

    @Column(name = "BRANCH_TYPE")
    private String branchType;
}












package com.fincore.JournalService.Models;

import jakarta.persistence.Column;
import jakarta.persistence.Entity;
import jakarta.persistence.Id;
import jakarta.persistence.Table;
import lombok.Data;
import java.time.LocalDate;


@Entity
@Table(name = "CGL_MASTER")
@Data
public class CglMaster {

    @Id
    @Column(name = "CGL_NUMBER", length = 10, updatable = false, insertable = false)
    private String cglNumber;

    @Column(name = "COMP_1", length = 4, nullable = false)
    private String comp1;

    @Column(name = "SEGMENT_CODE", length = 4, nullable = false)
    private String segmentCode;

    @Column(name = "COMP_2", length = 2, nullable = false)
    private String comp2;

    @Column(name = "DESCRIPTION", length = 100, nullable = false)
    private String description;

    @Column(name = "AC_CLASSIFICATION", length = 1, nullable = false)
    private String acClassification;

    @Column(name = "BAL_FWD", nullable = false)
    private Boolean balFwd = false;

    @Column(name = "DEF_BAL_TYPE", length = 1, nullable = false)
    private String defBalType;

    @Column(name = "STATUS", nullable = false)
    private Boolean status = true;

    @Column(name = "OPEN_DATE", nullable = false)
    private LocalDate openDate;

    @Column(name = "CLOSE_DATE")
    private LocalDate closeDate;

    @Column(name = "BAL_COMPARE", nullable = false)
    private Boolean balCompare = true;

    @Column(name = "MANUAL_POSTING", nullable = false)
    private Boolean manualPosting = true;
}












package com.fincore.JournalService.Models;

import jakarta.persistence.Column;
import jakarta.persistence.Entity;
import jakarta.persistence.Id;
import jakarta.persistence.Table;
import lombok.Data;

@Entity
@Table(name = "CURRENCY_MASTER")
@Data
public class CurrencyMaster {
    @Id
    @Column(name = "CURRENCY_CODE", length = 3)
    private String currencyCode;

    @Column(name = "CURRENCY_NAME", length = 50)
    private String currencyName;
    
    // 1 = Active, 0 = Inactive
    @Column(name = "FLAG")
    private Integer flag; 
}









package com.fincore.JournalService.Models;
import jakarta.persistence.Column;
import jakarta.persistence.Entity;
import jakarta.persistence.Id;
import jakarta.persistence.Table;
import lombok.Data;
import java.time.LocalDate;

@Entity
@Table(name = "FINCORE_DATE")
@Data
public class FincoreDate {

    @Id
    @Column(name = "USERS_DATE")
    private LocalDate usersDate;

    @Column(name = "ETL_DATE")
    private LocalDate etlDate;
}









package com.fincore.JournalService.Models;

import jakarta.persistence.*;
import lombok.Data;
import java.math.BigDecimal;
import java.time.LocalDate;

@Entity
@Table(name = "GL_BALANCE")
@Data
public class GlBalance {

    @Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "GL_BALANCE_SEQ")
    @SequenceGenerator(name = "GL_BALANCE_SEQ", sequenceName = "GL_BALANCE_SEQ", allocationSize = 1)
    @Column(name = "ID")
    private Long id;

    @Column(name = "BALANCE_DATE")
    private LocalDate balanceDate;

    @Column(name = "BRANCH_CODE", length = 5, nullable = false)
    private String branchCode;

    @Column(name = "CURRENCY", length = 3, nullable = false)
    private String currency;

    @Column(name = "CGL", length = 10, nullable = false)
    private String cgl;

    @Column(name = "BALANCE", precision = 25, scale = 4)
    private BigDecimal balance;
}











package com.fincore.JournalService.Models;

import jakarta.persistence.*;
import lombok.Data;
import java.math.BigDecimal;
import java.time.LocalDate;
import java.time.LocalDateTime;

@Entity
@Table(name = "GL_TRANSACTIONS")
@Data

public class GlTransaction {

    @Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "GL_TRANSACTIONS_SEQ")
    @SequenceGenerator(name = "GL_TRANSACTIONS_SEQ", sequenceName = "GL_TRANSACTIONS_SEQ", allocationSize = 1)
    @Column(name = "TRANSACTION_ID")
    private Long transactionId;

    @Column(name = "BATCH_ID", length = 50) 
    private String batchId;

    @Column(name = "JOURNAL_ID", length = 50)
    private String journalId;
   
    @Column(name = "TRANSACTION_DATE")
    @Temporal(TemporalType.DATE)
    private LocalDate transactionDate;

    @Column(name = "POST_DATE")
    @Temporal(TemporalType.TIMESTAMP)
    private LocalDateTime postDate;

    @Column(name = "BRANCH_CODE", length = 50)
    private String branchCode;

    @Column(name = "CURRENCY", length = 3)
    private String currency;

    @Column(name = "CGL", length = 10)
    private String cgl;

    @Column(name = "NARRATION", length = 40)
    private String narration;

    @Column(name = "DEBIT_AMOUNT", precision = 25, scale = 4)
    private BigDecimal debitAmount;

    @Column(name = "CREDIT_AMOUNT", precision = 25, scale = 4)
    private BigDecimal creditAmount;

    @Column(name = "TRANSACTION_COUNT")
    private Integer transactionCount;

    @Column(name = "SOURCE_FLAG", length = 1)
    private String sourceFlag;
}







package com.fincore.JournalService.Models;

import jakarta.persistence.*;
import lombok.Data;
import java.time.LocalDateTime;

@Entity
@Table(name = "AUDIT_LOG")
@Data
public class JournalLog {


    
	@Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "AUDIT_LOG_SEQ")
    @SequenceGenerator(
        name = "AUDIT_LOG_SEQ", 
        sequenceName = "AUDIT_LOG_SEQ", 
        allocationSize = 1
    )
    @Column(name = "LOG_ID")
    private Long id;

    @Column(name = "ACTION_TIME")
    private LocalDateTime actionTime;

    @Column(name = "ACTION_TYPE", length = 255)
    private String actionType; // e.g., CREATE, APPROVE, REJECT

    @Column(name = "CHANGE_TYPE", length = 255)
    private String changeType; // e.g., BATCH_UPLOAD, STATUS_CHANGE

    @Column(name = "IP_ADDRESS", length = 255)
    private String ipAddress;

    @Column(name = "NEW_VALUE", length = 4000) // Adjusted length for safety
    private String newValue;

    @Column(name = "OLD_VALUE", length = 4000)
    private String oldValue;

    @Column(name = "REQUEST_ID")
    private Long requestId; // Links to specific Journal Request ID (PK)

    @Column(name = "USER_ID", length = 255)
    private String userId;
}








package com.fincore.JournalService.Models;

import jakarta.persistence.*;
import lombok.Data;
import java.math.BigDecimal;
import java.time.LocalDate;
import java.time.LocalDateTime;

import com.fincore.JournalService.Models.enums.ChangeType;
import com.fincore.JournalService.Models.enums.RequestStatus;
import com.fincore.JournalService.config.RequestStatusConverter;

@Entity
@Table(name = "JOURNAL_REQUEST")
@Data
public class JournalRequest {
    @Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "JOURNAL_REQUEST_SEQ")
    @SequenceGenerator(name = "JOURNAL_REQUEST_SEQ", sequenceName = "JOURNAL_REQUEST_SEQ", allocationSize = 1)
    @Column(name = "REQ_ID")
    private Long id;

    @Convert(converter = RequestStatusConverter.class)
    @Column(name = "REQ_STATUS", length = 10)
    private RequestStatus requestStatus;

    @Enumerated(EnumType.STRING)
    @Column(name = "CHANGE_TYPE", length = 10)
    private ChangeType changeType;

    @Column(name = "REQ_DATE", updatable = false)
    private LocalDateTime requestDate = LocalDateTime.now();

    @Column(name = "CREATOR_ID", length = 12, updatable = false)
    private String creatorId;

    @Column(name = "CREATOR_ROLE", updatable = false)
    private Integer creatorRole;

    @Column(name = "EXECUTOR_ID", length = 12)
    private String executorId;

    @Column(name = "EXECUTION_DATE")
    private LocalDateTime executionDate;

    @Column(name = "EXECUTOR_REMARKS", length = 50)
    private String executorRemarks;

    // Legacy JSON Payload (Kept for Frontend Display)
    @Lob
    @Column(name = "PAYLOAD")
    private String payload;

    @Column(name = "BATCH_ID", length = 50)
    private String batchId;

    @Column(name = "JOURNAL_ID", length = 50)
    private String journalId;

    @Column(name = "COMMON_BATCH_REMARKS" , length = 50)
    private String commonBatchRemarks;

    // --- NEW OPTIMIZED COLUMNS ---
    @Column(name = "REQ_BRANCH_CODE", length = 50)
    private String branchCode;

    @Column(name = "REQ_CURRENCY", length = 3)
    private String currency;

    @Column(name = "REQ_CGL", length = 50)
    private String cgl;

    @Column(name = "REQ_AMOUNT", precision = 25, scale = 4)
    private BigDecimal amount;

    @Column(name = "REQ_CSV_DATE")
    private LocalDate csvDate;

    @Column(name = "REQ_NARRATION", length = 200)
    private String narration;

    @Column(name = "REQ_PRODUCT", length = 50)
    private String productCode;
}













package com.fincore.JournalService.Models;

import java.math.BigDecimal;
import java.time.LocalDate;
import jakarta.persistence.*;
import lombok.Data;

@Entity
@Table(name = "MASTER_JOURNAL")
@Data
public class MasterJournal {
      @Id
      @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "MASTER_JOURNAL_SEQ")
      @SequenceGenerator(name = "MASTER_JOURNAL_SEQ", sequenceName = "MASTER_JOURNAL_SEQ", allocationSize = 1)
      @Column(name = "JOURNAL_ID")
      private Long id;

      @Column(name = "JOURNAL_PDATE")
      private LocalDate pDate;

      @Column(name = "JOURNAL_BRANCH", length = 50)
      private String branch;

      @Column(name = "CURRENCY", length = 3)
      private String currency;

      @Column(name = "JOURNAL_CGL", length = 50)
      private String cgl;

      @Column(name = "JOURNAL_AMT")
      private BigDecimal amount;

      @Column(name = "TXN_TYPE", length = 20)
      private String transactionType;

      @Column(name = "PRODUCT_TYPE", length = 20)
      private String productType;

      @Column(name = "REMARKS", length = 50)
      private String remarks;

      @Column(name = "AR_FLAG", length = 1)
      private String arFlag;
      
      @Column(name= "AC_CLASSIFICATION")
      private String acClassification;

      @Column(name = "BATCH_ID", length = 50)
      private String batchId;

      @Column(name = "JOURNAL_ID_REF", length = 50) // Renamed to avoid conflict with PK if needed, or just JOURNAL_ID
      private String journalId;
      
      @Column(name = "COMMON_BATCH_REMARKS" , length = 50)
      private String commonBatchRemarks;
}










package com.fincore.JournalService.Models;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.springframework.data.jpa.domain.support.AuditingEntityListener;

import java.io.Serializable;
import java.time.Instant;
import java.util.UUID;

@Entity
@Table(name = "NOTIFICATIONS")
@Builder
@Data
@NoArgsConstructor
@AllArgsConstructor
@EntityListeners(AuditingEntityListener.class)
public class Notifications implements Serializable {

    @Id
    @GeneratedValue(strategy = GenerationType.UUID)
    @Column(name = "EVENT_ID", nullable = false, updatable = false)
    private UUID eventId; // Maps to RAW(16)

    @Column(name = "USER_ID", length = 255)
    private String userId;

    @Column(name = "MESSAGE", length = 1024, nullable = false)
    private String message;

    @Column(name = "LINK_URL", length = 1024)
    private String linkUrl;

    @Column(name = "EVENT_SOURCE", length = 100)
    private String eventSource;

    @Column(name = "AGGREGATE_ID", length = 255)
    private String aggregateId;

    @CreationTimestamp
    @Column(name = "EVENT_TIMESTAMP", nullable = false, updatable = false)
    private Instant eventTimestamp; // Maps to TIMESTAMP(6) WITH TIME ZONE

    @Column(name = "TARGET_ROLE", length = 100)
    private String targetRole;
}

















package com.fincore.JournalService.Models;

import jakarta.persistence.*;
import lombok.*;

@Entity
@Table(name = "MENU_ITEMS") // Mapped to your existing table
@Getter
@Setter
@NoArgsConstructor
@AllArgsConstructor
@ToString
public class Permissions {

    @Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "menu_item_seq_gen")
    @SequenceGenerator(name = "menu_item_seq_gen", sequenceName = "FINCORE.MENU_ITEMS_SEQ", allocationSize = 1)
    @Column(name = "MENU_ID", nullable = false)
    private int menuId;

    @Column(name = "MENU_TITLE", length = 100)
    private String menuTitle;

    @Column(name = "MENU_ICON", length = 100)
    private String menuIcon;

    @Column(name = "MENU_SUBMENU", length = 100)
    private String menuSubmenu;

    @Column(name = "MENU_ACTION", length = 200)
    private String menuAction;

    @Column(name = "MENU_URL", length = 200)
    private String menuUrl;

    @Column(name = "MENU_COMPONENT_PATH", length = 200)
    private String menuComponentPath;

    @Column(name = "MENU_DESCRIPTION", length = 255)
    private String menuDescription;

    @Column(name = "MENU_DEPENDANT")
    private Integer menuDependant;

    // This is the key field used for Journal Authorization logic
    @Column(name = "MAPPED_REQUEST_TYPE", length = 50)
    private String mappedRequestType; 
}












******************************************************************************

repository:

package com.fincore.JournalService.Repository;

import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import com.fincore.JournalService.Models.BranchMaster;

import java.util.List;

@Repository
public interface BranchMasterRepository extends JpaRepository<BranchMaster, String> {

    List<BranchMaster> findByNameContainingIgnoreCaseOrCodeContainingIgnoreCase(String name, String code, Pageable pageable);
}





package com.fincore.JournalService.Repository;

import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import com.fincore.JournalService.Models.CglMaster;

import java.util.List;

@Repository
public interface CglMasterRepository extends JpaRepository<CglMaster, String> {

    List<CglMaster> findByCglNumberContainingOrDescriptionContainingIgnoreCase(String cglNumber, String description, Pageable pageable);
}







package com.fincore.JournalService.Repository;

import com.fincore.JournalService.Models.CurrencyMaster;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface CurrencyMasterRepository extends JpaRepository<CurrencyMaster, String> {
}










package com.fincore.JournalService.Repository;


import com.fincore.JournalService.Models.FincoreDate;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;
import org.springframework.stereotype.Repository;

import java.time.LocalDate;

@Repository
public interface FincoreDateRepository extends JpaRepository<FincoreDate, LocalDate> {

    @Query(value = "SELECT USERS_DATE FROM FINCORE_DATE FETCH FIRST 1 ROWS ONLY", nativeQuery = true)
    LocalDate findCurrentPostingDate();
}










package com.fincore.JournalService.Repository;

import com.fincore.JournalService.Models.GlBalance;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface GlBalanceRepository extends JpaRepository<GlBalance, Long> {
}











package com.fincore.JournalService.Repository;

import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import com.fincore.JournalService.Models.GlTransaction;

@Repository
public interface GlTransactionRepository extends JpaRepository<GlTransaction, Long> {
}









package com.fincore.JournalService.Repository;

import com.fincore.JournalService.Models.JournalLog;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;


@Repository
public interface JournalLogRepository extends JpaRepository<JournalLog, Long> {
}







package com.fincore.JournalService.Repository;

import java.util.List;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.jpa.repository.JpaSpecificationExecutor;
import org.springframework.stereotype.Repository;
import org.springframework.data.repository.query.Param;

import com.fincore.JournalService.Models.JournalRequest;
import com.fincore.JournalService.Models.enums.RequestStatus;

@Repository
public interface JournalRequestRepository extends JpaRepository<JournalRequest, Long>, JpaSpecificationExecutor<JournalRequest> {

    // --- 1. APPROVAL SCREEN (Pending Batches) ---
    // Logic: Debit = Amount > 0. Credit = Amount < 0 (Taken as Absolute for display)
    @Query(value = """
        SELECT 
            BATCH_ID, 
            MAX(CREATOR_ID) as CREATOR, 
            MAX(REQ_DATE) as RDATE, 
            MAX(COMMON_BATCH_REMARKS) as REM,
            COUNT(*) as CNT,
            SUM(CASE WHEN TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount')) > 0 THEN TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount')) ELSE 0 END) as TOT_DR,
            SUM(CASE WHEN TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount')) < 0 THEN ABS(TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount'))) ELSE 0 END) as TOT_CR
        FROM JOURNAL_REQUEST 
        WHERE TRIM(REQ_STATUS) IN ('P', 'PENDING')
        GROUP BY BATCH_ID
        ORDER BY MAX(REQ_DATE) DESC
    """, nativeQuery = true)
    List<Object[]> findPendingBatchSummariesNative();

    // --- 2. STATUS SCREEN (All History) ---
    @Query(value = """
        SELECT 
            BATCH_ID, 
            MAX(CREATOR_ID) as CREATOR, 
            MAX(REQ_DATE) as RDATE, 
            MAX(COMMON_BATCH_REMARKS) as REM,
            COUNT(*) as CNT,
            SUM(CASE WHEN TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount')) > 0 THEN TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount')) ELSE 0 END) as TOT_DR,
            SUM(CASE WHEN TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount')) < 0 THEN ABS(TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount'))) ELSE 0 END) as TOT_CR,
            MAX(REQ_STATUS) as STATUS,
            MAX(EXECUTOR_ID) as EXEC_ID,
            MAX(EXECUTOR_REMARKS) as EXEC_REM
        FROM JOURNAL_REQUEST 
        GROUP BY BATCH_ID
        ORDER BY MAX(REQ_DATE) DESC
    """, nativeQuery = true)
    List<Object[]> findAllBatchSummariesNative();

    // --- 3. PAGINATED DETAIL FETCH ---
    @Query(value = "SELECT * FROM JOURNAL_REQUEST WHERE BATCH_ID = :batchId ORDER BY JOURNAL_ID ASC",
            countQuery = "SELECT COUNT(*) FROM JOURNAL_REQUEST WHERE BATCH_ID = :batchId",
            nativeQuery = true)
    Page<JournalRequest> findByBatchIdPaginated(@Param("batchId") String batchId, Pageable pageable);

    // --- 4. FAST DELETE ---
    @Modifying
    @Query(value = "DELETE FROM JOURNAL_REQUEST WHERE BATCH_ID = :batchId AND UPPER(TRIM(CREATOR_ID)) = UPPER(TRIM(:userId)) AND TRIM(REQ_STATUS) IN ('P', 'PENDING')", nativeQuery = true)
    void deleteBatchNative(@Param("batchId") String batchId, @Param("userId") String userId);

    @Modifying
    @Query(value = "DELETE FROM JOURNAL_REQUEST WHERE JOURNAL_ID IN (:journalIds) AND UPPER(TRIM(CREATOR_ID)) = UPPER(TRIM(:userId)) AND TRIM(REQ_STATUS) IN ('P', 'PENDING')", nativeQuery = true)
    void deleteJournalsNative(@Param("journalIds") List<String> journalIds, @Param("userId") String userId);

    // --- 5. STANDARD METHODS ---
    @Query(value = "SELECT * FROM JOURNAL_REQUEST WHERE TRIM(REQ_STATUS) IN ('P', 'PENDING')", nativeQuery = true)
    List<JournalRequest> findAllPendingNative();

    @Query(value = "SELECT * FROM JOURNAL_REQUEST WHERE UPPER(TRIM(CREATOR_ID)) = UPPER(TRIM(:creatorId))", nativeQuery = true)
    List<JournalRequest> findAllByCreatorIdNative(@Param("creatorId") String creatorId);

    // --- 6. RESTORED MISSING METHODS ---
    List<JournalRequest> findByBatchId(String batchId);
    List<JournalRequest> findByCreatorId(String creatorId);
    List<JournalRequest> findByRequestStatus(RequestStatus status);

    // This was the specific one causing your error:
    List<JournalRequest> findByJournalIdStartingWithAndCreatorIdAndRequestStatus(String journalIdPrefix, String creatorId, RequestStatus status);

    // Keeping this just in case:
    List<JournalRequest> findByJournalIdStartingWithAndRequestStatus(String journalIdPrefix, RequestStatus status);
}








package com.fincore.JournalService.Repository;

import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import com.fincore.JournalService.Models.MasterJournal;

@Repository
public interface MasterJournalRepository extends JpaRepository<MasterJournal, Long>{

}











package com.fincore.JournalService.Repository;

import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;
import com.fincore.JournalService.Models.Notifications;
import java.util.UUID;

@Repository
public interface NotificationRepository extends JpaRepository<Notifications, UUID> {
}





package com.fincore.JournalService.Repository;

import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import com.fincore.JournalService.Models.Permissions;

import java.util.List;

@Repository
public interface PermissionRepository extends JpaRepository<Permissions, Integer> {

    @Query(value = """
       SELECT p.MENU_URL, rp.ROLE_ID
       FROM PERMISSIONS p
       JOIN ROLE_PERMISSIONS rp ON p.MENU_ID = rp.PERMISSION_ID
       JOIN ROLES r ON rp.ROLE_ID = r.ROLE_ID
       WHERE p.MAPPED_REQUEST_TYPE = :requestType
         AND (p.MENU_ACTION LIKE '%approve%' OR p.MENU_ACTION LIKE '%reject%')
         AND r.ROLE_STATUS = 'ACTIVE'
   """, nativeQuery = true)
    List<Object[]> findUrlAndRolesByRequestType(@Param("requestType") String requestType);
}














***********************************************************************************************************

service :



package com.fincore.JournalService.Service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.stereotype.Service;

import com.fincore.JournalService.Dto.BranchDto;
import com.fincore.JournalService.Models.BranchMaster;
import com.fincore.JournalService.Repository.BranchMasterRepository;

import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

@Service
@RequiredArgsConstructor
@Slf4j
public class BranchService {

    private final BranchMasterRepository branchMasterRepository;

    public Map<String, String> getBranchCodeAndNameMap() {
        // This method is likely cached or used rarely, leaving as is
        List<BranchMaster> branches = branchMasterRepository.findAll();
        return branches.stream()
                .collect(Collectors.toMap(BranchMaster::getCode, BranchMaster::getName));
    }

    public List<BranchDto> searchBranches(String query) {
        log.info("Searching branches for: '{}'", query);

        Pageable limit = PageRequest.of(0, 50);


        List<BranchMaster> results = branchMasterRepository
                .findByNameContainingIgnoreCaseOrCodeContainingIgnoreCase(query, query, limit);

        return results.stream()
                .map(branch -> new BranchDto(branch.getCode(), branch.getName()))
                .collect(Collectors.toList());
    }
}







package com.fincore.JournalService.Service;

import com.fincore.JournalService.Dto.HdfsSyncDto;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.jdbc.core.BatchPreparedStatementSetter;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;

import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.List;

@Service
@RequiredArgsConstructor
@Slf4j
public class HdfsSyncService {

    @Autowired
    @Qualifier("hiveJdbcTemplate")
    private JdbcTemplate hiveJdbcTemplate;

    @Async("bulkExecutor") // Runs in background thread, UI returns immediately
    public void syncToDataLake(List<HdfsSyncDto> data) {
        if (data == null || data.isEmpty()) return;

        log.info("HDFS SYNC: Starting background update for {} aggregated records...", data.size());
        long start = System.currentTimeMillis();

        try {
            // Updating the Data Lake.
            // Note: Ensure your Hive/Thrift table supports updates.
            // If not (e.g. raw HDFS), change this to an INSERT statement.
            String sql = "UPDATE GL_BALANCE_HDFS SET BALANCE = ?, INR_BALANCE = ? " +
                    "WHERE BRANCH_CODE = ? AND CURRENCY = ? AND CGL = ? AND BALANCE_DATE = ?";

            hiveJdbcTemplate.batchUpdate(sql, new BatchPreparedStatementSetter() {
                @Override
                public void setValues(PreparedStatement ps, int i) throws SQLException {
                    HdfsSyncDto dto = data.get(i);
                    ps.setBigDecimal(1, dto.getNewBalance());
                    ps.setBigDecimal(2, dto.getNewInrBalance());
                    ps.setString(3, dto.getBranch());
                    ps.setString(4, dto.getCurrency());
                    ps.setString(5, dto.getCgl());
                    ps.setDate(6, java.sql.Date.valueOf(dto.getBalanceDate()));
                }

                @Override
                public int getBatchSize() {
                    return data.size();
                }
            });

            log.info("HDFS SYNC: Completed successfully in {} ms.", System.currentTimeMillis() - start);

        } catch (Exception e) {
            log.error("HDFS SYNC FAILED: {}. Data might be out of sync.", e.getMessage());
        }
    }
}










package com.fincore.JournalService.Service;

import com.fincore.JournalService.Dto.HdfsSyncDto;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;

import java.util.List;

@Service
@RequiredArgsConstructor
@Slf4j
public class HdfsRecoveryService {

    private final HdfsSyncService hdfsSyncService;

    @Autowired
    @Qualifier("oracleJdbcTemplate")
    private JdbcTemplate oracleJdbcTemplate;

    // Run every 5 minutes (300,000 ms)
    @Scheduled(fixedDelay = 300000)
    public void retryFailedHdfsSyncs() {
        // 1. Fetch PENDING batches
        String fetchSql = "SELECT BATCH_ID FROM HDFS_SYNC_RETRY_QUEUE WHERE STATUS = 'PENDING' AND RETRY_COUNT < 5";
        List<String> failedBatches = oracleJdbcTemplate.query(fetchSql, (rs, rowNum) -> rs.getString("BATCH_ID"));

        if (failedBatches.isEmpty()) return;  // 99% of the time, code stops here. Instant return.

        log.info("HDFS RECOVERY: Found {} failed batches. Starting self-healing...", failedBatches.size());

        for (String batchId : failedBatches) {
            try {
                processRetry(batchId);
            } catch (Exception e) {
                log.error("HDFS RECOVERY: Failed to recover batch {}", batchId, e);
                // Increment retry count
                oracleJdbcTemplate.update("UPDATE HDFS_SYNC_RETRY_QUEUE SET RETRY_COUNT = RETRY_COUNT + 1 WHERE BATCH_ID = ?", batchId);
            }
        }
    }

    private void processRetry(String batchId) {
        // 2. SELF-HEALING QUERY
        // Instead of using old data, we join JOURNAL_REQUEST with GL_BALANCE
        // to get the CURRENT REAL-TIME balance from Oracle.
        // This fixes the "Race Condition" issue.
        String freshDataSql = """
            SELECT 
                j.REQ_BRANCH_CODE AS BRANCH, 
                j.REQ_CURRENCY AS CURRENCY, 
                j.REQ_CGL AS CGL, 
                j.REQ_CSV_DATE AS BAL_DATE,
                g.BALANCE AS NEW_BALANCE,
                g.INR_BALANCE AS NEW_INR_BALANCE
            FROM (
                SELECT DISTINCT REQ_BRANCH_CODE, REQ_CURRENCY, REQ_CGL, REQ_CSV_DATE 
                FROM JOURNAL_REQUEST 
                WHERE BATCH_ID = ?
            ) j
            JOIN GL_BALANCE g ON 
                g.BRANCH_CODE = j.REQ_BRANCH_CODE AND 
                g.CURRENCY = j.REQ_CURRENCY AND 
                g.CGL = j.REQ_CGL AND 
                g.BALANCE_DATE = j.REQ_CSV_DATE
        """;

        List<HdfsSyncDto> freshSyncData = oracleJdbcTemplate.query(freshDataSql,
                (rs, rowNum) -> new HdfsSyncDto(
                        rs.getString("BRANCH"),
                        rs.getString("CURRENCY"),
                        rs.getString("CGL"),
                        rs.getDate("BAL_DATE").toLocalDate(),
                        rs.getBigDecimal("NEW_BALANCE"),
                        rs.getBigDecimal("NEW_INR_BALANCE")
                ), batchId);

        if (!freshSyncData.isEmpty()) {
            // 3. Push the LATEST/CURRENT balance to HDFS
            hdfsSyncService.syncToDataLake(freshSyncData);
            log.info("HDFS RECOVERY: Successfully synced batch {}. Data is now consistent.", batchId);
        }

        // 4. Mark as Completed
        oracleJdbcTemplate.update("UPDATE HDFS_SYNC_RETRY_QUEUE SET STATUS = 'COMPLETED' WHERE BATCH_ID = ?", batchId);
    }
}













package com.fincore.JournalService.Service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.stereotype.Service;
import org.springframework.util.StringUtils;

import com.fincore.JournalService.Dto.CglDto;
import com.fincore.JournalService.Models.CglMaster;
import com.fincore.JournalService.Repository.CglMasterRepository;

import java.util.List;
import java.util.stream.Collectors;

@Service
@RequiredArgsConstructor
@Slf4j
public class CglService {

    private final CglMasterRepository cglMasterRepository;

    private CglDto convertToDto(CglMaster cgl) {
        return new CglDto(cgl.getCglNumber(), cgl.getDescription());
    }

    public List<CglDto> searchCgls(String query) {



        if (!StringUtils.hasText(query)) {
            return List.of();
        }

        log.info("Searching CGLs for: '{}'", query);

        Pageable limit = PageRequest.of(0, 50);

        List<CglMaster> results = cglMasterRepository
                .findByCglNumberContainingOrDescriptionContainingIgnoreCase(query, query, limit);

        return results.stream()
                .map(this::convertToDto)
                .collect(Collectors.toList());
    }
}

















//-----

package com.fincore.JournalService.Service;

import com.fasterxml.jackson.annotation.JsonFormat;
import com.fasterxml.jackson.annotation.JsonIgnore;
import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonPropertyOrder;
import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fincore.JournalService.Dto.BulkUploadStateDto;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.poi.ss.usermodel.*;
import org.apache.poi.xssf.streaming.SXSSFWorkbook;
import org.apache.poi.xssf.usermodel.XSSFWorkbook;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.context.annotation.Lazy;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;

import java.io.*;
import java.math.BigDecimal;
import java.math.RoundingMode;
import java.nio.charset.StandardCharsets;
import java.time.LocalDate;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.LongAdder;
import java.util.regex.Pattern;
import java.util.stream.Collectors;
import java.util.zip.GZIPInputStream;
import java.util.zip.GZIPOutputStream;


@Service
@RequiredArgsConstructor
@Slf4j
public class JournalBulkValidationService {

    // --- REGEX PATTERNS ---
    private static final Pattern CLEAN_AMOUNT_REGEX = Pattern.compile("[^0-9.]");
    private static final Pattern PRODUCT_CODE_REGEX = Pattern.compile("^\\d{8}$");
    private static final Pattern CGL_FORMAT_REGEX = Pattern.compile("^\\d{10}$");
    private static final DateTimeFormatter SYSTEM_DATE_FMT = DateTimeFormatter.ofPattern("ddMMyyyy");

    private final ValidationMasterService validationMasterService;
    private final ObjectMapper objectMapper;
    // Status is lightweight, keep in memory for speed
    private final Map<String, BulkUploadStateDto> statusCache = new ConcurrentHashMap<>();
    @Autowired
    @Qualifier("byteArrayRedisTemplate")
    private RedisTemplate<String, byte[]> redisTemplate;
    @Autowired
    @Lazy
    private JournalBulkValidationService self;

    public BulkUploadStateDto getState(String reqId) {
        return statusCache.get(reqId);
    }

    public byte[] getFileBytes(String reqId, String type) {
        return redisTemplate.opsForValue().get("FILE_" + reqId + "_" + type);
    }

    // --- REDIS COMPRESSION (Fixes OOM) ---
    public List<ExcelRowData> getValidRowsFromCache(String requestId) {
        long start = System.currentTimeMillis();
        byte[] compressed = redisTemplate.opsForValue().get("DATA_" + requestId);
        if (compressed == null) return Collections.emptyList();

        try (GZIPInputStream gis = new GZIPInputStream(new ByteArrayInputStream(compressed))) {
            List<ExcelRowData> rows = objectMapper.readValue(gis, new TypeReference<List<ExcelRowData>>() {
            });
            log.info("Decompressed {} rows from Redis in {}ms", rows.size(), System.currentTimeMillis() - start);
            return rows;
        } catch (IOException e) {
            log.error("Failed to decompress rows from Redis", e);
            return Collections.emptyList();
        }
    }

    private void saveRowsToRedis(String requestId, List<ExcelRowData> rows) {
        long start = System.currentTimeMillis();
        try (ByteArrayOutputStream baos = new ByteArrayOutputStream();
             GZIPOutputStream gos = new GZIPOutputStream(baos)) {
            objectMapper.writeValue(gos, rows);
            gos.finish();
            byte[] compressed = baos.toByteArray();
            redisTemplate.opsForValue().set("DATA_" + requestId, compressed, 1, TimeUnit.HOURS);
            log.info("Compressed and saved {} rows to Redis in {}ms (Size: {} bytes)", rows.size(), System.currentTimeMillis() - start, compressed.length);
        } catch (IOException e) {
            log.error("Failed to save rows to Redis", e);
        }
    }


    public String initiateValidation(byte[] fileBytes, String filename, LocalDate postingDate) throws IOException {
        String requestId = UUID.randomUUID().toString();
        BulkUploadStateDto state = new BulkUploadStateDto();
        state.setRequestId(requestId);
        state.setStatus("PROCESSING");
        state.setCurrentStage(1);
        state.setMessage("Initializing Upload...");
        state.setTotalRows(0);
        statusCache.put(requestId, state);

        self.processAsync(requestId, fileBytes, filename, postingDate);
        return requestId;
    }

    @Async("bulkExecutor")
    public void processAsync(String requestId, byte[] fileBytes, String filename, LocalDate postingDate) {
        log.info("Starting Async Validation for ReqID: {}", requestId);
        try {
            updateState(requestId, s -> s.setMessage("Parsing File..."));
            List<ExcelRowData> parsedRows;
            boolean isCsv = filename != null && (filename.toLowerCase().endsWith(".csv") || filename.toLowerCase().endsWith(".txt"));

            if (isCsv) parsedRows = parseCsvBytes(fileBytes, postingDate);
            else parsedRows = parseExcelBytes(fileBytes, postingDate);

            updateState(requestId, s -> {
                s.setTotalRows(parsedRows.size());
                s.setMessage("Validating Formats...");
            });

            if (runFormatCheck(parsedRows)) {
                failRequest(requestId, parsedRows, "Format Validation Failed", 1);
                return;
            }

            updateState(requestId, s -> {
                s.setCurrentStage(2);
                s.setMessage("Checking Database...");
            });

            if (runDbCheck(parsedRows)) {
                failRequest(requestId, parsedRows, "Database Validation Failed", 2);
                return;
            }

            updateState(requestId, s -> {
                s.setCurrentStage(3);
                s.setMessage("Checking Balances...");
            });

            if (runBalanceCheck(parsedRows)) {
                failRequest(requestId, parsedRows, "Debit/Credit Balance Mismatch", 3);
                return;
            }

            completeRequest(requestId, parsedRows, postingDate, isCsv);

        } catch (Exception e) {
            log.error("Async Validation Error", e);
            updateState(requestId, s -> {
                s.setStatus("ERROR");
                s.setMessage("System Error: " + e.getMessage());
                s.setHasErrorFile(false);
            });
        }
    }

    private boolean runDbCheck(List<ExcelRowData> rows) {
        // FAST: Load Sets only ONCE
        Set<String> validBranches = validationMasterService.getAllActiveBranches();
        Set<String> validCurrs = validationMasterService.getAllActiveCurrencies();
        Set<String> validCgls = validationMasterService.getAllActiveCgls();

        rows.parallelStream().forEach(d -> {
            if (!validBranches.contains(d.branch)) d.dbErrors.add("Branch Not Found/Inactive: " + d.branch);
            if (!validCurrs.contains(d.currency)) d.dbErrors.add("Currency Not Found/Inactive: " + d.currency);
            if (!validCgls.contains(d.cgl)) d.dbErrors.add("CGL Not Found/Inactive: " + d.cgl);
        });
        return rows.stream().anyMatch(ExcelRowData::hasErrors);
    }


    // --- STANDARD UTILS ---

    private List<ExcelRowData> parseCsvBytes(byte[] bytes, LocalDate postingDate) throws IOException {
        List<ExcelRowData> list = new ArrayList<>();
        try (BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(bytes), StandardCharsets.UTF_8))) {
            String line;
            int i = 0;
            while ((line = br.readLine()) != null) {
                if (line.trim().isEmpty()) continue;
                String[] c = line.split(",", -1);
                if (i == 0) {
                    String h = c.length > 0 ? c[0].trim().toLowerCase() : "";
                    if ((h.contains("branch") || h.contains("batch")) && !h.equals("01")) {
                        i++;
                        continue;
                    }
                }
                ExcelRowData d = new ExcelRowData();
                d.rowIndex = i++;
                if (c.length > 0) d.sysSite = c[0].trim();
                if (c.length > 1) d.sysDate = c[1].trim();
                if (c.length > 2) d.sysYear = c[2].trim();
                if (c.length > 3) d.sysPeriod = c[3].trim();
                if (c.length > 4) d.branch = c[4].trim();
                if (c.length > 5) d.currency = c[5].trim().toUpperCase();
                if (c.length > 6) d.cgl = c[6].trim();
                String amtRaw = (c.length > 7) ? c[7].trim() : "";
                String txnRaw = (c.length > 8) ? c[8].trim() : "";
                parseAmount(d, amtRaw, txnRaw);
                if (c.length > 9) d.remarks = c[9].trim();
                String rawProd = (c.length > 10) ? c[10].trim() : "";
                d.productCode = rawProd.isEmpty() ? "A" : rawProd;
                if (c.length != 14)
                    d.formatErrors.add("Invalid CSV Format: Row has " + c.length + " columns. Expected 14.");
                else {
                    d.isSystemFormat = true;
                    if (d.sysSite.isEmpty()) d.formatErrors.add("Batch ID is Mandatory");
                    if (d.branch.isEmpty()) d.formatErrors.add("Branch is Mandatory");
                    if (d.currency.isEmpty()) d.formatErrors.add("Currency is Mandatory");
                    if (d.cgl.isEmpty()) d.formatErrors.add("CGL is Mandatory");
                    if (amtRaw.isEmpty()) d.formatErrors.add("Amount is Mandatory");
                    validateSystemColumns(d, d.sysSite, d.sysDate, d.sysYear, d.sysPeriod, c[11], c[12], c[13], postingDate);
                }
                list.add(d);
            }
        }
        return list;
    }

    private List<ExcelRowData> parseExcelBytes(byte[] bytes, LocalDate postingDate) throws IOException {
        try (Workbook wb = new XSSFWorkbook(new ByteArrayInputStream(bytes))) {
            return parseSheet(wb.getSheetAt(0), postingDate);
        }
    }

    private List<ExcelRowData> parseSheet(Sheet sheet, LocalDate postingDate) {
        List<ExcelRowData> list = new ArrayList<>();
        DataFormatter fmt = new DataFormatter();
        for (Row r : sheet) {
            if (isRowEmpty(r)) continue;
            if (r.getRowNum() == 0) {
                String h = fmt.formatCellValue(r.getCell(0)).toLowerCase();
                if ((h.contains("branch") || h.contains("batch")) && !h.equals("01")) continue;
            }
            ExcelRowData d = new ExcelRowData();
            d.rowIndex = r.getRowNum();
            String col0 = parseCode(r.getCell(0), fmt);
            if (col0.equals("01")) {
                d.isSystemFormat = true;
                d.sysSite = col0;
                d.sysDate = parseCode(r.getCell(1), fmt);
                d.sysYear = parseCode(r.getCell(2), fmt);
                d.sysPeriod = parseCode(r.getCell(3), fmt);
                d.branch = parseCode(r.getCell(4), fmt);
                d.currency = fmt.formatCellValue(r.getCell(5)).trim().toUpperCase();
                d.cgl = parseCode(r.getCell(6), fmt);
                parseAmount(d, fmt.formatCellValue(r.getCell(7)), fmt.formatCellValue(r.getCell(8)));
                d.remarks = fmt.formatCellValue(r.getCell(9));
                String rawProd = parseCode(r.getCell(10), fmt);
                d.productCode = rawProd.isEmpty() ? "A" : rawProd;
                if (isCellEmpty(r.getCell(0))) d.formatErrors.add("Batch ID is Mandatory");
                validateSystemColumns(d, col0, d.sysDate, d.sysYear, d.sysPeriod,
                        parseCode(r.getCell(11), fmt), parseCode(r.getCell(12), fmt), parseCode(r.getCell(13), fmt), postingDate);
            } else {
                d.branch = col0;
                d.currency = fmt.formatCellValue(r.getCell(1)).trim().toUpperCase();
                d.cgl = parseCode(r.getCell(2), fmt);
                parseAmount(d, fmt.formatCellValue(r.getCell(3)), fmt.formatCellValue(r.getCell(4)));
                d.remarks = fmt.formatCellValue(r.getCell(5));
                String rawProd = parseCode(r.getCell(6), fmt);
                d.productCode = rawProd.isEmpty() ? "A" : rawProd;
            }
            list.add(d);
        }
        return list;
    }

    private void parseAmount(ExcelRowData d, String amountRaw, String typeRaw) {
        try {
            if (amountRaw == null) amountRaw = "";
            if (typeRaw == null) typeRaw = "";
            if (amountRaw.contains("-") || amountRaw.contains("(") || amountRaw.contains(")"))
                d.formatErrors.add("Amount cannot be negative");
            String clean = CLEAN_AMOUNT_REGEX.matcher(amountRaw).replaceAll("");
            if (clean.isEmpty()) {
                d.amount = BigDecimal.ZERO;
                return;
            }
            BigDecimal v = new BigDecimal(clean);
            if (v.signum() < 0) d.formatErrors.add("Amount cannot be negative");
            boolean isCredit = typeRaw.toUpperCase().contains("C") || typeRaw.toUpperCase().contains("CR");
            d.txnType = isCredit ? "Credit" : "Debit";
            d.amount = v;
        } catch (Exception e) {
            d.amount = BigDecimal.ZERO;
        }
    }

    private boolean runFormatCheck(List<ExcelRowData> rows) {
        rows.parallelStream().forEach(d -> {
            if (d.amount == null || d.amount.compareTo(BigDecimal.ZERO) == 0)
                d.formatErrors.add("Amount cannot be Zero or Null");
            else {
                if (d.amount.signum() < 0) d.formatErrors.add("Amount cannot be negative");
                if (d.amount.precision() > 20 || d.amount.scale() > 4)
                    d.formatErrors.add("Amount exceeds format (Max 16.4)");
                if ("INR".equalsIgnoreCase(d.currency) && d.amount.stripTrailingZeros().scale() > 2)
                    d.formatErrors.add("INR Amount > 2 decimal places");
            }
            if (d.productCode != null && !d.productCode.isEmpty() && !d.productCode.equals("A") && !PRODUCT_CODE_REGEX.matcher(d.productCode).matches())
                d.formatErrors.add("Product Code must be 8 digits or Keep it Blank ");
            if (d.remarks == null || d.remarks.trim().isEmpty()) d.formatErrors.add("Remarks is Mandatory");
            else if (d.remarks.length() > 30) d.formatErrors.add("Remarks length must be <= 30 chars");
            if (d.currency == null || d.currency.length() != 3)
                d.formatErrors.add("Currency must be exactly 3 characters");
            if (d.cgl == null || d.cgl.isEmpty()) d.formatErrors.add("CGL is Mandatory");
            else if (!CGL_FORMAT_REGEX.matcher(d.cgl).matches()) d.formatErrors.add("CGL must be exactly 10 digits");
            if (d.branch == null || d.branch.trim().isEmpty()) d.formatErrors.add("Branch is Mandatory");
        });
        return rows.stream().anyMatch(ExcelRowData::hasErrors);
    }

    private void validateSystemColumns(ExcelRowData d, String c0, String cDate, String cYear, String cMonth, String c11, String c12, String c13, LocalDate postingDate) {
        if (!"01".equals(c0)) d.formatErrors.add("Batch ID must be '01'");
        LocalDate parsedDate = null;
        if (cDate == null || cDate.trim().length() != 8) d.formatErrors.add("Invalid Date Format (ddMMyyyy)");
        else {
            try {
                parsedDate = LocalDate.parse(cDate.trim(), SYSTEM_DATE_FMT);
                if (!parsedDate.equals(postingDate))
                    d.formatErrors.add("Date Mismatch with Posting Date (" + postingDate + ")");
            } catch (Exception e) {
                d.formatErrors.add("Invalid Calendar Date");
            }
        }
        if (parsedDate != null) {
            String expectedYear = (parsedDate.getMonthValue() >= 4) ? String.valueOf(parsedDate.getYear()) : String.valueOf(parsedDate.getYear() - 1);
            String expectedMonth = String.format("%02d", (parsedDate.getMonthValue() >= 4) ? (parsedDate.getMonthValue() - 3) : (parsedDate.getMonthValue() + 9));
            if (cYear == null || !cYear.trim().equals(expectedYear))
                d.formatErrors.add("Invalid Fin Year. Expected: " + expectedYear);
            if (cMonth == null || !cMonth.trim().equals(expectedMonth))
                d.formatErrors.add("Invalid Fin Month. Expected: " + expectedMonth);
        }
        if (c11 == null || !"B".equalsIgnoreCase(c11.trim())) d.formatErrors.add("Col 12 must be 'B'");
        if (c12 == null || !"C".equalsIgnoreCase(c12.trim())) d.formatErrors.add("Col 13 must be 'C'");
        if (c13 == null || !"D".equalsIgnoreCase(c13.trim())) d.formatErrors.add("Col 14 must be 'D'");
    }

    private boolean runBalanceCheck(List<ExcelRowData> rows) {
        ConcurrentHashMap<String, LongAdder> balanceMap = new ConcurrentHashMap<>();
        rows.parallelStream().forEach(d -> {
            if (d.amount != null) {
                BigDecimal val = d.amount;
                if ("Credit".equals(d.txnType)) val = val.negate();
                String type = (d.cgl != null && d.cgl.startsWith("5")) ? "MEMO" : "NORMAL";
                String key = d.branch + "_" + d.currency + "_" + type;
                balanceMap.computeIfAbsent(key, k -> new LongAdder()).add(val.multiply(new BigDecimal("10000")).longValue());
            }
        });
        rows.parallelStream().forEach(d -> {
            String type = (d.cgl != null && d.cgl.startsWith("5")) ? "MEMO" : "NORMAL";
            String key = d.branch + "_" + d.currency + "_" + type;
            LongAdder adder = balanceMap.get(key);
            if (adder != null && adder.sum() != 0) {
                BigDecimal diff = BigDecimal.valueOf(adder.sum()).divide(new BigDecimal("10000"));
                synchronized (d.balErrors) {
                    if (d.balErrors.isEmpty())
                        d.balErrors.add(type + " Balance Mismatch: Total " + diff.toPlainString());
                }
            }
        });
        return rows.stream().anyMatch(ExcelRowData::hasErrors);
    }

    private void failRequest(String reqId, List<ExcelRowData> rows, String msg, int stage) throws IOException {
        byte[] excel = generateErrorExcelFast(rows);
        redisTemplate.opsForValue().set("FILE_" + reqId + "_ERROR", excel, 30, TimeUnit.MINUTES);
        updateState(reqId, s -> {
            s.setStatus("ERROR");
            s.setMessage(msg);
            s.setCurrentStage(stage);
            s.setErrorCount(rows.stream().filter(ExcelRowData::hasErrors).count());
            s.setHasErrorFile(true);
        });
    }

    private void completeRequest(String reqId, List<ExcelRowData> rows, LocalDate pDate, boolean isCsv) throws IOException {
        saveRowsToRedis(reqId, rows);
        if (!isCsv) {
            byte[] csv = generateSuccessCsv(rows, pDate);
            redisTemplate.opsForValue().set("FILE_" + reqId + "_SUCCESS", csv, 30, TimeUnit.MINUTES);
        }
        List<Map<String, Object>> preview = rows.stream().limit(2000).map(this::mapToPreview).collect(Collectors.toList());
        updateState(reqId, s -> {
            s.setCurrentStage(4);
            s.setStatus("SUCCESS");
            s.setMessage("Validation Successful");
            s.setHasSuccessFile(!isCsv);
            try {
                s.setPreviewDataJson(objectMapper.writeValueAsString(preview));
            } catch (Exception e) {
            }
        });
    }

    private void updateState(String requestId, java.util.function.Consumer<BulkUploadStateDto> updater) {
        BulkUploadStateDto state = statusCache.getOrDefault(requestId, new BulkUploadStateDto());
        updater.accept(state);
        statusCache.put(requestId, state);
    }

    private byte[] generateSuccessCsv(List<ExcelRowData> rows, LocalDate postingDate) {
        StringBuilder csv = new StringBuilder(rows.size() * 100);
        DateTimeFormatter ddMMyyyy = DateTimeFormatter.ofPattern("ddMMyyyy");
        String col2 = postingDate.format(ddMMyyyy);
        String year = (postingDate.getMonthValue() >= 4) ? String.valueOf(postingDate.getYear()) : String.valueOf(postingDate.getYear() - 1);
        String month = String.format("%02d", (postingDate.getMonthValue() >= 4) ? (postingDate.getMonthValue() - 3) : (postingDate.getMonthValue() + 9));
        for (ExcelRowData row : rows) {
            String cDate = row.isSystemFormat ? row.sysDate : col2;
            String cYear = row.isSystemFormat ? row.sysYear : year;
            String cMonth = row.isSystemFormat ? row.sysPeriod : month;
            String col5 = String.format("%5s", row.branch).replace(' ', '0');
            String col6 = (row.currency == null || row.currency.isEmpty()) ? "INR" : row.currency;
            String plainAmt = row.amount.abs().setScale(4, RoundingMode.HALF_UP).toPlainString();
            int dotIndex = plainAmt.indexOf('.');
            String intPart = (dotIndex == -1) ? plainAmt : plainAmt.substring(0, dotIndex);
            String decPart = (dotIndex == -1) ? "0000" : plainAmt.substring(dotIndex + 1);
            String col8 = String.format("%16s", intPart).replace(' ', '0') + "." + decPart;
            String col9 = "Credit".equalsIgnoreCase(row.txnType) ? "Cr" : "Dr";
            String col10 = (row.remarks != null) ? row.remarks.replace(",", " ") : "";
            String col11 = (row.productCode != null && !row.productCode.isEmpty()) ? row.productCode : "A";
            csv.append("01,").append(cDate).append(",").append(cYear).append(",").append(cMonth).append(",")
                    .append(col5).append(",").append(col6).append(",").append(row.cgl).append(",").append(col8).append(",")
                    .append(col9).append(",").append(col10).append(",").append(col11).append(",B,C,D\n");
        }
        return csv.toString().getBytes(StandardCharsets.UTF_8);
    }

    private byte[] generateErrorExcelFast(List<ExcelRowData> rows) throws IOException {
        try (SXSSFWorkbook workbook = new SXSSFWorkbook(100)) {
            Sheet sheet = workbook.createSheet("Error Report");
            CellStyle errStyle = workbook.createCellStyle();
            Font f = workbook.createFont();
            f.setColor(IndexedColors.RED.getIndex());
            errStyle.setFont(f);
            Row h = sheet.createRow(0);
            List<String> headerList = new ArrayList<>(Arrays.asList("Branch", "Currency", "CGL", "Amount", "TxnType", "Remarks", "Product", "ERRORS"));
            if (rows.stream().anyMatch(r -> r.isSystemFormat))
                headerList.addAll(0, Arrays.asList("BatchID", "Date", "Year", "Month"));
            for (int i = 0; i < headerList.size(); i++) h.createCell(i).setCellValue(headerList.get(i));
            int idx = 1;
            boolean isSystem = rows.stream().anyMatch(r -> r.isSystemFormat);
            for (ExcelRowData d : rows) {
                Row r = sheet.createRow(idx++);
                int col = 0;
                if (isSystem) {
                    r.createCell(col++).setCellValue(d.sysSite);
                    r.createCell(col++).setCellValue(d.sysDate);
                    r.createCell(col++).setCellValue(d.sysYear);
                    r.createCell(col++).setCellValue(d.sysPeriod);
                }
                r.createCell(col++).setCellValue(d.branch);
                r.createCell(col++).setCellValue(d.currency);
                r.createCell(col++).setCellValue(d.cgl);
                r.createCell(col++).setCellValue(d.amount != null ? d.amount.toString() : "");
                r.createCell(col++).setCellValue(d.txnType);
                r.createCell(col++).setCellValue(d.remarks);
                r.createCell(col++).setCellValue(d.productCode);
                Cell c = r.createCell(col);
                if (d.hasErrors()) {
                    c.setCellValue(d.getAllErrors());
                    c.setCellStyle(errStyle);
                }
            }
            ByteArrayOutputStream out = new ByteArrayOutputStream();
            workbook.write(out);
            return out.toByteArray();
        }
    }

    public byte[] generateTemplateBytes() throws IOException {
        try (SXSSFWorkbook wb = new SXSSFWorkbook()) {
            Sheet sheet = wb.createSheet("Journal Template");
            Row row = sheet.createRow(0);
            String[] headers = {"Branch", "Currency", "CGL", "Amount", "TxnType", "Remarks", "Product"};
            for (int i = 0; i < headers.length; i++) {
                Cell cell = row.createCell(i);
                cell.setCellValue(headers[i]);
                sheet.setColumnWidth(i, 4000);
            }
            ByteArrayOutputStream out = new ByteArrayOutputStream();
            wb.write(out);
            return out.toByteArray();
        }
    }

    private String parseCode(Cell c, DataFormatter f) {
        if (c == null) return "";
        if (c.getCellType() == CellType.NUMERIC)
            return BigDecimal.valueOf(c.getNumericCellValue()).toPlainString().split("\\.")[0];
        return f.formatCellValue(c).trim();
    }

    private boolean isRowEmpty(Row r) {
        if (r == null) return true;
        for (int c = r.getFirstCellNum(); c < r.getLastCellNum(); c++)
            if (r.getCell(c) != null && r.getCell(c).getCellType() != CellType.BLANK && !r.getCell(c).toString().trim().isEmpty())
                return false;
        return true;
    }

    private boolean isCellEmpty(Cell c) {
        return c == null || c.getCellType() == CellType.BLANK || c.toString().trim().isEmpty();
    }

    private Map<String, Object> mapToPreview(ExcelRowData r) {
        Map<String, Object> m = new HashMap<>();
        m.put("id", r.rowIndex);
        m.put("branch", r.branch);
        m.put("currency", r.currency);
        m.put("cgl", r.cgl);
        m.put("amount", r.amount != null ? r.amount.toString() : "");
        m.put("txnType", r.txnType);
        m.put("remarks", r.remarks);
        m.put("productCode", r.productCode);
        return m;
    }


    // --- DTO: OPTIMIZED FOR REDIS ---
    @JsonIgnoreProperties(ignoreUnknown = true)
    @JsonFormat(shape = JsonFormat.Shape.ARRAY) // CRITICAL: This reduces JSON size by 50%
    @JsonPropertyOrder({"rowIndex", "branch", "currency", "cgl", "txnType", "remarks", "productCode", "amount", "isSystemFormat", "sysSite", "sysDate", "sysYear", "sysPeriod", "formatErrors", "dbErrors", "balErrors"})
    public static class ExcelRowData {
        public int rowIndex;
        public String branch = "", currency = "", cgl = "", txnType = "", remarks = "", productCode = "";
        public BigDecimal amount;
        public boolean isSystemFormat = false;
        public String sysSite = "", sysDate = "", sysYear = "", sysPeriod = "";
        public List<String> formatErrors = Collections.synchronizedList(new ArrayList<>());
        public List<String> dbErrors = Collections.synchronizedList(new ArrayList<>());
        public List<String> balErrors = Collections.synchronizedList(new ArrayList<>());

        public ExcelRowData() {
        }

        public boolean hasErrors() {
            return !formatErrors.isEmpty() || !dbErrors.isEmpty() || !balErrors.isEmpty();
        }

        @JsonIgnore
        public String getAllErrors() {
            List<String> all = new ArrayList<>(formatErrors);
            all.addAll(dbErrors);
            all.addAll(balErrors);
            return String.join("; ", all);
        }
    }

}
// ---------------






















package com.fincore.JournalService.Service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fincore.JournalService.Dto.*;
import com.fincore.JournalService.Models.JournalRequest;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;

import java.io.IOException;
import java.time.LocalDate;
import java.util.List;
import java.util.Map;
import java.util.Optional;

/**
 * Interface for Journal Request Service.
 * Includes both Legacy (Sync) and Optimized (Async) methods.
 */
public interface JournalRequestService {

    // --- 1. ASYNC & OPTIMIZED METHODS ---
    /**
     * Starts background batch creation from cached Excel data.
     * @return batchId (String) immediately.
     */
    String createBatchFromCacheAsync(String requestId, String commonRemarks, String creatorId, Integer creatorRole) throws IOException;

    /**
     * Starts background approval/rejection logic.
     */
    void processBulkRequestsAsync(BulkProcessJournalRequestDto dto, String executorId, Integer executorRole);

    /**
     * Starts background chunked deletion of a batch.
     */
    void cancelMyRequestsByBatchIdAsync(String batchId, String userId);

    /**
     * Internal method: Deletes one chunk (10k rows) in a new transaction.
     */
//    int deleteBatchChunk(String batchId, String userId);

    /**
     * Fast count using Summary Table or Index.
     */
    long getRequestCountByBatchId(String batchId);

    // --- 2. INTERNAL ASYNC EXECUTORS (Public for Spring Proxy) ---
    void executeAsyncBatchCreation(String batchId, String requestId, String commonRemarks, String creatorId, Integer creatorRole);
    void executeAsyncBatchProcessing(BulkProcessJournalRequestDto dto, String executorId);
    void executeAsyncBatchCancellation(String batchId, String userId);

    // --- 3. LEGACY / STANDARD METHODS ---
    LocalDate getCurrentPostingDate();
    List<Map<String, Object>> getPendingBatchSummaries();
    List<Map<String, Object>> getAllBatchSummaries();

    // Manual Creation (Sync)
    List<JournalRequest> createBatchRequest(BatchRequestDto dto, String creatorId, Integer creatorRole) throws JsonProcessingException;
    String createBulkBatchRequest(BatchRequestDto dto, String creatorId, Integer creatorRole) throws JsonProcessingException;
    String createBatchFromCache(String requestId, String commonRemarks, String creatorId, Integer creatorRole) throws IOException; // Deprecated by Async version

    List<JournalRequest> processBulkRequests(BulkProcessJournalRequestDto dto, String executorId, Integer executorRole);
    Optional<JournalRequest> updateRequestStatus(ProcessJournalRequestDto dto, String executorId, Integer executorRole) throws JsonProcessingException;

    List<JournalRequest> getMyRequests(String userId);
    List<JournalRequest> getPendingRequests(String userId, Integer userRole);
    List<JournalRequest> getRequestsByBatchId(String batchId);
    Page<JournalRequest> getRequestsByBatchIdPaginated(String batchId, Pageable pageable);
    List<JournalRequestStatusDto> getJournalRequestStatusList();

    void cancelMyRequest(Long requestId, String userId);
    void cancelMyRequestsByBatchId(String batchId, String userId); // Deprecated by Async version
    void cancelMyRequestsByJournalPrefixes(List<String> journalIdPrefixes, String userId);
    void cancelMyRequestsByJournalPrefix(String journalIdPrefix, String userId);
}



















package com.fincore.JournalService.Service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fincore.JournalService.Dto.*;
import com.fincore.JournalService.Models.JournalLog;
import com.fincore.JournalService.Models.JournalRequest;
import com.fincore.JournalService.Models.enums.ChangeType;
import com.fincore.JournalService.Models.enums.RequestStatus;
import com.fincore.JournalService.Repository.JournalLogRepository;
import com.fincore.JournalService.Repository.JournalRequestRepository;
import com.fincore.JournalService.Service.JournalBulkValidationService.ExcelRowData;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.context.annotation.Lazy;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.jdbc.core.CallableStatementCallback;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import javax.sql.DataSource;
import java.io.IOException;
import java.math.BigDecimal;
import java.sql.*;
import java.time.LocalDate;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.Executor;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.LongAdder;

/**
 * High-Performance Implementation of Journal Service.
 * Handles 600k+ row batches using Async execution, JDBC Batching, and Oracle Parallelism.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class JournalRequestServiceImpl implements JournalRequestService {

    private final JournalRequestRepository journalRequestRepository;
    private final JournalLogRepository journalLogRepository;
    private final SequenceService sequenceService;
    private final NotificationWriterService notificationWriterService;
    private final PermissionConfigService permissionConfigService;
    private final JournalBulkValidationService journalBulkValidationService;
    private final HdfsSyncService hdfsSyncService;
    private final ProgressService progressService;

    @Autowired
    private ObjectMapper objectMapper;

    @Autowired
    @Lazy
    private JournalRequestService self;

    @Autowired
    @Qualifier("oracleJdbcTemplate")
    private JdbcTemplate jdbcTemplate;

    @Autowired
    @Qualifier("byteArrayRedisTemplate")
    private RedisTemplate<String, byte[]> redisTemplate;

    @Autowired
    @Qualifier("bulkExecutor") // Using your specific ThreadPool
    private Executor bulkExecutor;

    // Direct DataSource access for Raw JDBC performance
    @Autowired
    @Qualifier("oracleDataSource")
    private DataSource dataSource;

    // ==================================================================================
    // 1. ASYNC BATCH CREATION (PARALLEL OPTIMIZATION)
    // ==================================================================================
    @Override
    @Transactional
    public String createBatchFromCacheAsync(String requestId, String commonRemarks, String creatorId, Integer creatorRole) throws IOException {
        String lockKey = "LOCK_REQ_" + requestId;
        Boolean acquired = redisTemplate.opsForValue().setIfAbsent(lockKey, new byte[0], 5, TimeUnit.MINUTES);

        if (Boolean.FALSE.equals(acquired)) {
            log.warn("Duplicate creation attempt blocked for ReqID: {}", requestId);
            throw new IllegalStateException("Batch creation is already in progress.");
        }

        try {
            String batchId = sequenceService.getNextBatchId();
            // Start Progress
            progressService.sendProgress(batchId, creatorId, 0, "PROCESSING", "Initializing Batch Creation...");
            self.executeAsyncBatchCreation(batchId, requestId, commonRemarks, creatorId, creatorRole);
            return batchId;
        } catch (Exception e) {
            redisTemplate.delete(lockKey);
            throw e;
        }
    }

    @Override
    @Async("bulkExecutor")
    @Transactional
    public void executeAsyncBatchCreation(String batchId, String requestId, String commonRemarks, String creatorId, Integer creatorRole) {
        log.info("ASYNC CREATE: Starting Batch {} (Source: {})", batchId, requestId);
        long start = System.currentTimeMillis();

        try {
            progressService.sendProgress(batchId, creatorId, 5, "PROCESSING", "Preparing Data...");

            // 1. Fetch from Redis
            List<ExcelRowData> cachedRows = journalBulkValidationService.getValidRowsFromCache(requestId);
            if (cachedRows == null || cachedRows.isEmpty()) {
                logAudit(creatorId, "CREATE_FAIL", "BATCH_ASYNC", "Cache Expired for " + batchId);
                progressService.sendProgress(batchId, creatorId, 0, "FAILED", "Data cache expired or empty.");
                return;
            }

            // 2. Prepare Parallel Execution
            int totalRows = cachedRows.size();
            // 5000 is optimal for Oracle JDBC batching (avoids packet fragmentation)
            int batchSize = 5000;

            List<CompletableFuture<Void>> futures = new ArrayList<>();
            LongAdder totalDebit = new LongAdder();
            LongAdder totalCredit = new LongAdder();

            AtomicInteger completedChunks = new AtomicInteger(0);
            int totalChunks = (int) Math.ceil((double) totalRows / batchSize);

            // 3. Submit Chunks
            for (int i = 0; i < totalRows; i += batchSize) {
                final int startIdx = i;
                final int endIdx = Math.min(i + batchSize, totalRows);
                final List<ExcelRowData> chunk = cachedRows.subList(startIdx, endIdx);

                CompletableFuture<Void> future = CompletableFuture.runAsync(() ->
                                processChunkRaw(chunk, batchId, commonRemarks, creatorId, creatorRole, startIdx, totalDebit, totalCredit), bulkExecutor)
                        .thenRun(() -> {
                            // Update Progress
                            int done = completedChunks.incrementAndGet();
                            int percent = 5 + (done * 90 / totalChunks); // Map 5% -> 95%
                            progressService.sendProgress(batchId, creatorId, percent, "PROCESSING",
                                    String.format("Created %d/%d records...", (done * batchSize), totalRows));
                        });
                futures.add(future);
            }
            CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();

            // 5. Insert Master Record
            Timestamp ts = Timestamp.valueOf(LocalDateTime.now());
            String sumSql = "INSERT INTO JOURNAL_BATCH_MASTER (BATCH_ID, CREATOR_ID, REQ_DATE, BATCH_REMARKS, TOTAL_ROWS, TOTAL_DEBIT, TOTAL_CREDIT, BATCH_STATUS) VALUES (?, ?, ?, ?, ?, ?, ?, ?)";
            jdbcTemplate.update(sumSql, batchId, creatorId, ts, commonRemarks, totalRows,
                    BigDecimal.valueOf(totalDebit.sum()), BigDecimal.valueOf(totalCredit.sum()), "PENDING");

            // 6. Clean & Log
            redisTemplate.delete("DATA_" + requestId);
            redisTemplate.delete("LOCK_REQ_" + requestId);
            logAudit(creatorId, "CREATE_SUCCESS", "BATCH_ASYNC", "Created Batch " + batchId);


            // --- FINAL NOTIFICATIONS ---
            // 1. Progress Bar Complete
            progressService.sendProgress(batchId, creatorId, 100, "COMPLETED", "Batch Created Successfully");

            // 2. Persistent Notification to CREATOR (Success)
            sendNotification(creatorId, null, "Batch " + batchId + " created successfully.", batchId);

            // 3. Persistent Notification to EXECUTORS (Action Required)
            NotificationConfigDto config = permissionConfigService.getConfig("JOURNAL_AUTH");
            if (config != null) {
                String msg = String.format("New Batch %s by %s requires approval.", batchId, creatorId);
                sendNotification(creatorId, config.getTargetRoles(), msg, batchId);
            }

//            createNotification(batchId, creatorId, totalRows);
            log.info(" Batch {} Created. Rows: {}. Time: {}ms", batchId, totalRows, System.currentTimeMillis() - start);

        } catch (Exception e) {
            log.error("Async Create Failed", e);
            logAudit(creatorId, "CREATE_FAIL", "BATCH_ASYNC", e.getMessage());
            redisTemplate.delete("LOCK_REQ_" + requestId);
            cleanupFailedBatch(batchId);
            progressService.sendProgress(batchId, creatorId, 0, "FAILED", "Creation Error: " + e.getMessage());

        }
    }

    /**
     * RAW JDBC INSERT
     * This bypasses Spring overhead and guarantees setAutoCommit(false) for true batching.
     */
    private void processChunkRaw(List<ExcelRowData> chunk, String batchId, String commonRemarks, String creatorId, Integer creatorRole, int globalOffset, LongAdder debit, LongAdder credit) {
        String sql = "INSERT INTO JOURNAL_REQUEST (REQ_ID, REQ_STATUS, CHANGE_TYPE, REQ_DATE, CREATOR_ID, CREATOR_ROLE, BATCH_ID, JOURNAL_ID, COMMON_BATCH_REMARKS, PAYLOAD, REQ_BRANCH_CODE, REQ_CURRENCY, REQ_CGL, REQ_AMOUNT, REQ_CSV_DATE, REQ_NARRATION, REQ_PRODUCT) VALUES (JOURNAL_REQUEST_SEQ.nextval, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)";

        try (Connection conn = dataSource.getConnection();
             PreparedStatement ps = conn.prepareStatement(sql)) {

            // CRITICAL: Force manual commit to ensure driver sends batch packet
            conn.setAutoCommit(false);

            Timestamp ts = Timestamp.valueOf(LocalDateTime.now());
            final DateTimeFormatter jsonFmt = DateTimeFormatter.ISO_DATE;

            for (int i = 0; i < chunk.size(); i++) {
                ExcelRowData r = chunk.get(i);
                int globalIdx = globalOffset + i + 1;
                String jId = batchId + "-" + globalIdx;

                // Logic
                BigDecimal absAmt = (r.amount != null) ? r.amount.abs() : BigDecimal.ZERO;
                boolean isCredit = "Credit".equalsIgnoreCase(r.txnType) || "Cr".equalsIgnoreCase(r.txnType);
                BigDecimal signedAmt = isCredit ? absAmt.negate() : absAmt;

                if (isCredit) credit.add(absAmt.longValue());
                else debit.add(absAmt.longValue());

                LocalDate rDate = LocalDate.now();
                if (r.isSystemFormat && r.sysDate != null && r.sysDate.length() == 8) {
                    try {
                        rDate = LocalDate.parse(r.sysDate, DateTimeFormatter.ofPattern("ddMMyyyy"));
                    } catch (Exception e) {
                    }
                }

                // Bind Params (1-based index)
                ps.setString(1, "P");
                ps.setString(2, "ADD");
                ps.setTimestamp(3, ts);
                ps.setString(4, creatorId);
                ps.setInt(5, creatorRole != null ? creatorRole : 0);
                ps.setString(6, batchId);
                ps.setString(7, jId);
                ps.setString(8, commonRemarks);
                ps.setString(9, buildJsonPayloadFast(r, signedAmt, rDate, batchId, jId, commonRemarks, globalIdx, jsonFmt));
                ps.setString(10, r.branch);
                ps.setString(11, r.currency);
                ps.setString(12, r.cgl);
                ps.setBigDecimal(13, signedAmt);
                ps.setDate(14, java.sql.Date.valueOf(rDate));
                ps.setString(15, r.remarks);
                ps.setString(16, r.productCode);

                ps.addBatch();
            }

            ps.executeBatch();
            conn.commit(); // Single commit per chunk

        } catch (SQLException e) {
            log.error("Raw JDBC Batch Insert Failed for chunk starting at " + globalOffset, e);
            throw new RuntimeException("DB Error", e);
        }
    }

    private void cleanupFailedBatch(String batchId) {
        try {
            jdbcTemplate.update("DELETE FROM JOURNAL_REQUEST WHERE BATCH_ID = ?", batchId);
            jdbcTemplate.update("DELETE FROM JOURNAL_BATCH_MASTER WHERE BATCH_ID = ?", batchId);
        } catch (Exception e) {
            log.warn("Cleanup failed", e);
        }
    }


    // ==================================================================================
    // 2. ASYNC APPROVAL (With Parallel DB & HDFS Retry)
    // ==================================================================================

    @Override
    @Transactional
    public void processBulkRequestsAsync(BulkProcessJournalRequestDto dto, String executorId, Integer executorRole) {
        // --- STEP 1: SYNCHRONOUS LOCKING  ---
        // We update the status to 'QUEUED' *before* returning to the UI.
        // If another user tries this, updated rows will be 0, and we throw error.
        String lockSql = "UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'QUEUED' WHERE BATCH_ID = ? AND BATCH_STATUS = 'PENDING'";
        int updated = jdbcTemplate.update(lockSql, dto.getBatchId());

        if (updated == 0) {
            log.warn("Race Condition detected! Batch {} is already being processed.", dto.getBatchId());
            throw new IllegalStateException("Batch " + dto.getBatchId() + " is already queued or processed by another user.");
        }
        // Start Progress for Executor
        progressService.sendProgress(dto.getBatchId(), executorId, 0, "PROCESSING", "Approval Queued...");

        // --- STEP 2: Trigger Background Process ---
        log.info("Batch {} Locked. Status set to QUEUED. Starting Async Process.", dto.getBatchId());
        self.executeAsyncBatchProcessing(dto, executorId);
    }

    @Async("bulkExecutor")
    @Transactional
    public void executeAsyncBatchProcessing(BulkProcessJournalRequestDto dto, String executorId) {
        String batchId = dto.getBatchId();
        long start = System.currentTimeMillis();

        try {
            // Fetch Creator ID to notify them later
            String creatorId = jdbcTemplate.queryForObject("SELECT CREATOR_ID FROM JOURNAL_BATCH_MASTER WHERE BATCH_ID = ?", String.class, batchId);

            progressService.sendProgress(batchId, executorId, 20, "PROCESSING", "Processing on Database...");

            if (RequestStatus.ACCEPTED.equals(dto.getStatus())) {

                // 1. Call Oracle PL/SQL (Parallel Mode)
                log.info("Calling Oracle Procedure for Batch {}", batchId);

                List<HdfsSyncDto> syncData = jdbcTemplate.execute(
                        "{call PROCESS_JOURNAL_BATCH(?, ?, ?, ?, ?)}",
                        (CallableStatementCallback<List<HdfsSyncDto>>) cs -> {
                            cs.setString(1, batchId);
                            cs.setString(2, executorId);
                            cs.setString(3, dto.getRemarks());
                            cs.setString(4, "A");
                            cs.registerOutParameter(5, -10); // Cursor
                            cs.execute();

                            List<HdfsSyncDto> list = new ArrayList<>();
                            try (ResultSet rs = (ResultSet) cs.getObject(5)) {
                                while (rs.next()) {
                                    list.add(new HdfsSyncDto(
                                            rs.getString("BRANCH"), rs.getString("CURRENCY"), rs.getString("CGL"),
                                            rs.getDate("BAL_DATE").toLocalDate(),
                                            rs.getBigDecimal("NEW_BALANCE"), rs.getBigDecimal("NEW_INR_BALANCE")
                                    ));
                                }
                            }
                            return list;
                        }
                );

                // 2. Update Summary Table
                jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'ACCEPTED', EXECUTOR_ID = ?, EXECUTION_DATE = SYSTIMESTAMP, EXECUTOR_REMARKS = ? WHERE BATCH_ID = ?",
                        executorId, dto.getRemarks(), batchId);

//                // 3. Sync HDFS (With Retry Queue Safety)
//                try {
//                    hdfsSyncService.syncToDataLake(syncData);
//                } catch (Exception e) {
//                    log.error("HDFS Sync Failed for Batch {}. Queuing for Retry.", batchId, e);
//                    // Insert into Retry Queue for Recovery Service
//                    jdbcTemplate.update("INSERT INTO HDFS_SYNC_RETRY_QUEUE (BATCH_ID, STATUS) VALUES (?, 'PENDING')", batchId);
//                }

                // --- NOTIFICATIONS ---
                // 1. Progress Done (Executor)
                progressService.sendProgress(batchId, executorId, 100, "COMPLETED", "Batch Approved Successfully");

                // 2. Persistent Notification to EXECUTOR (Task Done)
                sendNotification(executorId, null, "You approved Batch " + batchId, batchId);

                // 3. Persistent Notification to CREATOR (Outcome)
                if (creatorId != null && !creatorId.equals(executorId)) {
                    sendNotification(creatorId, null, "Your Batch " + batchId + " has been ACCEPTED.", batchId);
                }

                logAudit(executorId, "APPROVE_SUCCESS", "BATCH_ASYNC", "Approved Batch " + batchId);
                log.info(" Batch {} Processed & ACCEPTED in {}ms", batchId, System.currentTimeMillis() - start);

            } else if (RequestStatus.REJECTED.equals(dto.getStatus())) {
                // Reject Rows
                jdbcTemplate.update("UPDATE JOURNAL_REQUEST SET REQ_STATUS = 'R', EXECUTOR_ID = ?, EXECUTOR_REMARKS = ?, EXECUTION_DATE = SYSDATE WHERE BATCH_ID = ? AND REQ_STATUS = 'P'",
                        executorId, dto.getRemarks(), batchId);

                // Reject Summary
                jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'REJECTED', EXECUTOR_ID = ?, EXECUTION_DATE = SYSTIMESTAMP, EXECUTOR_REMARKS = ? WHERE BATCH_ID = ?",
                        executorId, dto.getRemarks(), batchId);

                logAudit(executorId, "REJECT_OK", "BATCH_ASYNC", "Rejected Batch " + batchId);

                // --- NOTIFICATIONS ---
                progressService.sendProgress(batchId, executorId, 100, "COMPLETED", "Batch Rejected");
                sendNotification(executorId, null, "You rejected Batch " + batchId, batchId);
                if (creatorId != null && !creatorId.equals(executorId)) {
                    sendNotification(creatorId, null, "Your Batch " + batchId + " was REJECTED.", batchId);
                }

                log.info(" Batch {} Processed & REJECTED in {}ms", batchId, System.currentTimeMillis() - start);

            }
        } catch (Exception e) {
            log.error("Async Process Failed for Batch {}", batchId, e);
            // Fallback status so it doesn't stay stuck in QUEUED
            jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'ERROR', EXECUTOR_REMARKS = ? WHERE BATCH_ID = ?", "System Error: " + e.getMessage(), batchId);
            logAudit(executorId, "PROCESS_FAIL", "BATCH_ASYNC", e.getMessage());
            // No rollback here for Java exceptions if PL/SQL succeeded,
            // but PL/SQL handles its own rollback on error.
            progressService.sendProgress(batchId, executorId, 0, "FAILED", "Error: " + e.getMessage());
        }
    }

    // ==================================================================================
    // 3. ASYNC DELETION (Chunked & Safe +   (Progress + Notification))
    // ==================================================================================

    @Override
    @Transactional
    public void cancelMyRequestsByBatchIdAsync(String batchId, String userId) {
        // Lock it immediately so user sees "Processing" and can't double click
        int updated = jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'QUEUED' WHERE BATCH_ID = ? AND BATCH_STATUS IN ('PENDING', 'ERROR')", batchId);

        if (updated == 0) {
            log.warn("Delete Race Condition: Batch {} already locked.", batchId);
            throw new IllegalStateException("Batch is already being processed or deleted.");
        }

        progressService.sendProgress(batchId, userId, 0, "PROCESSING", "Deleting Batch...");
        self.executeAsyncBatchCancellation(batchId, userId);
    }


    @Override
    @Async("bulkExecutor")
    public void executeAsyncBatchCancellation(String batchId, String userId) {
        long start = System.currentTimeMillis();
        try {
            progressService.sendProgress(batchId, userId, 20, "PROCESSING", "Purging Data...");
            log.info("Calling PURGE_JOURNAL_BATCH for {}", batchId);

            jdbcTemplate.execute(
                    "{call PURGE_JOURNAL_BATCH(?, ?)}",
                    (CallableStatementCallback<Object>) cs -> {
                        cs.setString(1, batchId);
                        cs.setString(2, userId);
                        cs.execute();
                        return null;
                    }
            );

            logAudit(userId, "CANCEL_SUCCESS", "BATCH_ASYNC", "Deleted " + batchId);

            // Final Notifications
            progressService.sendProgress(batchId, userId, 100, "COMPLETED", "Batch Deleted");
            sendNotification(userId, null, "Batch " + batchId + " deleted successfully.", batchId);

            log.info(" Batch {} DELETED in {}ms", batchId, System.currentTimeMillis() - start);

        } catch (Exception e) {
            log.error("Delete Failed", e);
            // VISIBILITY : Set to ERROR so it reappears in list
            jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'ERROR', EXECUTOR_REMARKS = ? WHERE BATCH_ID = ?", "Delete Failed: " + e.getMessage(), batchId);
            progressService.sendProgress(batchId, userId, 0, "FAILED", "Delete Failed: " + e.getMessage());
        }
    }


    // ==================================================================================
    // 4. OPTIMIZED SUMMARIES (Using Master Table)
    // ==================================================================================

    @Override
    public List<Map<String, Object>> getPendingBatchSummaries() {
        // Show 'ERROR' status too.
        // This allows users to see failed background tasks (Creation, Accept, or Delete failures)
        String sql = """
            SELECT BATCH_ID, CREATOR_ID, REQ_DATE, BATCH_REMARKS, TOTAL_ROWS, TOTAL_DEBIT, TOTAL_CREDIT, BATCH_STATUS, EXECUTOR_REMARKS
            FROM JOURNAL_BATCH_MASTER 
            WHERE BATCH_STATUS IN ('PENDING', 'ERROR') 
            ORDER BY REQ_DATE DESC
        """;

        return jdbcTemplate.query(sql, (rs, rowNum) -> {
            Map<String, Object> map = new HashMap<>();
            map.put("batchId", rs.getString("BATCH_ID"));
            map.put("creatorId", rs.getString("CREATOR_ID"));
            map.put("requestDate", rs.getTimestamp("REQ_DATE"));
            map.put("commonBatchRemarks", rs.getString("BATCH_REMARKS"));
            map.put("requestCount", rs.getLong("TOTAL_ROWS"));
            map.put("totalDebit", rs.getBigDecimal("TOTAL_DEBIT"));
            map.put("totalCredit", rs.getBigDecimal("TOTAL_CREDIT"));
            map.put("requestStatus", rs.getString("BATCH_STATUS"));

            // Helpful for frontend to show why it failed
            if ("ERROR".equals(rs.getString("BATCH_STATUS"))) {
                map.put("errorMessage", rs.getString("EXECUTOR_REMARKS"));
            }
            return map;
        });
    }


    @Override
    public List<Map<String, Object>> getAllBatchSummaries() {
        String sql = """
                    SELECT BATCH_ID, CREATOR_ID, REQ_DATE, BATCH_REMARKS, TOTAL_ROWS, TOTAL_DEBIT, TOTAL_CREDIT, 
                           BATCH_STATUS, EXECUTOR_ID, EXECUTOR_REMARKS 
                    FROM JOURNAL_BATCH_MASTER 
                    ORDER BY REQ_DATE DESC 
                    FETCH FIRST 100 ROWS ONLY
                """;

        return jdbcTemplate.query(sql, (rs, rowNum) -> {
            Map<String, Object> map = new HashMap<>();
            map.put("batchId", rs.getString("BATCH_ID"));
            map.put("creatorId", rs.getString("CREATOR_ID"));
            map.put("requestDate", rs.getTimestamp("REQ_DATE"));
            map.put("commonBatchRemarks", rs.getString("BATCH_REMARKS"));
            map.put("requestCount", rs.getLong("TOTAL_ROWS"));
            map.put("totalDebit", rs.getBigDecimal("TOTAL_DEBIT"));
            map.put("totalCredit", rs.getBigDecimal("TOTAL_CREDIT"));
            map.put("requestStatus", rs.getString("BATCH_STATUS"));
            map.put("executorId", rs.getString("EXECUTOR_ID"));
            map.put("executorRemarks", rs.getString("EXECUTOR_REMARKS"));
            return map;
        });
    }

    @Override
    public long getRequestCountByBatchId(String batchId) {
        try {
            return jdbcTemplate.queryForObject("SELECT TOTAL_ROWS FROM JOURNAL_BATCH_MASTER WHERE BATCH_ID = ?", Long.class, batchId);
        } catch (Exception e) {
            return 0; // Or fallback to count(*)
        }
    }

    // ==================================================================================
    // 5. HELPERS
    // ==================================================================================

//    private String buildJsonPayloadFast(ExcelRowData row, BigDecimal amount, LocalDate pDate, String batchId, String jId, String rem, int count, DateTimeFormatter fmt) {
//        return "{\"changeType\":\"ADD\",\"masterJournalId\":null,\"csvDate\":\"" + pDate.format(fmt) + "\"," +
//                "\"branch\":\"" + row.branch + "\",\"currency\":\"" + row.currency + "\"," +
//                "\"cgl\":\"" + row.cgl + "\",\"amount\":" + amount + "," +
//                "\"productType\":\"" + (row.productCode == null ? "" : row.productCode) + "\"," +
//                "\"remarks\":\"" + (row.remarks == null ? "" : escapeJson(row.remarks)) + "\"," +
//                "\"arFlag\":\"A\",\"acClassification\":\"A\",\"batchId\":\"" + batchId + "\"," +
//                "\"journalId\":\"" + jId + "\",\"commonBatchRemarks\":\"" + escapeJson(rem) + "\"," +
//                "\"transactionCount\":" + count + "}";
//    }

    // ---------------------------------------------------------
    // OPTIMIZED PAYLOAD GENERATION (Bulk)
    // ---------------------------------------------------------
    private String buildJsonPayloadFast(ExcelRowData row, BigDecimal amount, LocalDate pDate, String batchId, String jId, String rem, int count, DateTimeFormatter fmt) {
        return "{\"csvDate\":\"" + pDate.format(fmt) + "\"," +
                "\"branch\":\"" + row.branch + "\",\"currency\":\"" + row.currency + "\"," +
                "\"cgl\":\"" + row.cgl + "\",\"amount\":" + amount + "," +
                "\"productType\":\"" + (row.productCode == null ? "" : row.productCode) + "\"," +
                "\"remarks\":\"" + (row.remarks == null ? "" : escapeJson(row.remarks)) + "\"," +
                "\"arFlag\":\"A\",\"acClassification\":\"A\",\"batchId\":\"" + batchId + "\"," +
                "\"journalId\":\"" + jId + "\",\"commonBatchRemarks\":\"" + escapeJson(rem) + "\"," +
                "\"transactionCount\":" + count + "}";
    }

    private String escapeJson(String s) {
        return s == null ? "" : s.replace("\"", "\\\"").replace("\\", "\\\\");
    }

    private void createNotification(String batchId, String creatorId, int size) {
        try {
            NotificationConfigDto config = permissionConfigService.getConfig("JOURNAL_AUTH");
            String message = String.format("Batch %s (%d rows) by %s pending.", batchId, size, creatorId);
            notificationWriterService.createNotification(null, config.getTargetRoles(), message, config.getTargetUrl(), batchId, "JournalService");
        } catch (Exception e) {
            log.error("Notification Failed", e);
        }
    }

    private void sendNotification(String userId, String roles, String message, String batchId) {
        try {
            // Using a generic URL or null if handled by FE routing
            String url = "/journal-view/" + batchId;
            notificationWriterService.createNotification(userId, roles, message, url, batchId, "JournalService");
        } catch (Exception e) {
            log.warn("Failed to send persistent notification for {}: {}", batchId, e.getMessage());
        }
    }

    private void logAudit(String user, String action, String type, String val) {
        try {
            JournalLog l = new JournalLog();
            l.setUserId(user);
            l.setActionType(action);
            l.setChangeType(type);
            l.setNewValue(val.length() > 3900 ? val.substring(0, 3900) : val);
            l.setActionTime(LocalDateTime.now());
            journalLogRepository.save(l);
        } catch (Exception e) {
            log.error("Audit Failed", e);
        }
    }

    // --- Legacy / Unchanged Implementations ---
    @Override
    public LocalDate getCurrentPostingDate() {
        try {
            String sql = "SELECT USERS_DATE FROM FINCORE_DATE FETCH FIRST 1 ROWS ONLY";
            return jdbcTemplate.queryForObject(sql, (rs, rowNum) -> {
                Timestamp ts = rs.getTimestamp("USERS_DATE");
                if (ts != null) return ts.toLocalDateTime().toLocalDate();
                return LocalDate.now();
            });
        } catch (Exception e) {
            log.error("Error fetching USERS_DATE. Using System Date.", e);
            return LocalDate.now();
        }
    }

    @Override
    public Page<JournalRequest> getRequestsByBatchIdPaginated(String b, Pageable p) {
        return journalRequestRepository.findByBatchIdPaginated(b, p);
    }

    @Override
    public List<JournalRequest> getRequestsByBatchId(String b) {
        return journalRequestRepository.findByBatchId(b);
    }

    @Override
    public List<JournalRequest> getMyRequests(String u) {
        return new ArrayList<>(); /* Optimized: Don't load 600k rows */
    }

    @Override
    public List<JournalRequest> getPendingRequests(String u, Integer r) {
        return new ArrayList<>();
    }

    @Override
    public List<JournalRequestStatusDto> getJournalRequestStatusList() {
        return new ArrayList<>();
    }

    @Override
    @Transactional
    public void cancelMyRequest(Long r, String u) {
        journalRequestRepository.deleteById(r);
    }

    @Override
    @Transactional
    public void cancelMyRequestsByBatchId(String b, String u) {
        cancelMyRequestsByBatchIdAsync(b, u);
    }

    @Override
    @Transactional
    public void cancelMyRequestsByJournalPrefixes(List<String> l, String u) {
        journalRequestRepository.deleteJournalsNative(l, u);
    }

    @Override
    @Transactional
    public void cancelMyRequestsByJournalPrefix(String p, String u) {
    }

    @Override
    public Optional<JournalRequest> updateRequestStatus(ProcessJournalRequestDto d, String u, Integer r) {
        return Optional.empty();
    }

    // ---------------------------------------------------------
    // MANUAL BATCH CREATION (ADD ONLY - SIMPLIFIED)
    // ---------------------------------------------------------
    @Override
    @Transactional
    public List<JournalRequest> createBatchRequest(BatchRequestDto dto, String creatorId, Integer creatorRole) throws JsonProcessingException {

        // 1. Generate New Batch ID
        String batchId = sequenceService.getNextBatchId();
        Timestamp ts = Timestamp.valueOf(LocalDateTime.now());
        ObjectMapper localMapper = new ObjectMapper();

        List<JournalRequest> entities = new ArrayList<>();
        BigDecimal totalDebit = BigDecimal.ZERO;
        BigDecimal totalCredit = BigDecimal.ZERO;
        int rowIndex = 1;

        // 2. Iterate through DTO rows
        for (BatchRequestDto.JournalRequestRow row : dto.getRows()) {
            JournalRequest entity = new JournalRequest();

            // --- Metadata ---
            entity.setRequestStatus(RequestStatus.PENDING);
            entity.setChangeType(ChangeType.ADD);
            entity.setRequestDate(LocalDateTime.now());
            entity.setCreatorId(creatorId);
            entity.setCreatorRole(creatorRole);
            entity.setBatchId(batchId);
            entity.setCommonBatchRemarks(dto.getCommonBatchRemarks());

            // --- ID Generation ---
            String journalId = batchId + "-" + rowIndex;
            entity.setJournalId(journalId);

            // --- Optimized Columns ---
            entity.setBranchCode(row.getBranch());
            entity.setCurrency(row.getCurrency());
            entity.setCgl(row.getCgl());

            // Amount Handling
            BigDecimal amount = row.getAmount();
            entity.setAmount(amount);

            entity.setCsvDate(row.getCsvDate() != null ? row.getCsvDate() : LocalDate.now());
            entity.setNarration(row.getRemarks());
            entity.setProductCode(row.getProductType());

            // --- PAYLOAD GENERATION (Simplified) ---
            Map<String, Object> payloadMap = new HashMap<>();

            payloadMap.put("csvDate", entity.getCsvDate().toString());
            payloadMap.put("branch", row.getBranch());
            payloadMap.put("currency", row.getCurrency());
            payloadMap.put("cgl", row.getCgl());
            payloadMap.put("amount", amount);
            payloadMap.put("productType", row.getProductType());
            payloadMap.put("remarks", row.getRemarks());

            // Keep strictly necessary static flags if your system expects them defaults
            payloadMap.put("arFlag", "A");
            payloadMap.put("acClassification", "A");

            payloadMap.put("batchId", batchId);
            payloadMap.put("journalId", journalId);
            payloadMap.put("commonBatchRemarks", dto.getCommonBatchRemarks());
            payloadMap.put("transactionCount", rowIndex);

            // Generate JSON String (Cleaner now)
            entity.setPayload(localMapper.writeValueAsString(payloadMap));

            // --- Totals Calculation ---
            if (amount.compareTo(BigDecimal.ZERO) < 0) {
                totalCredit = totalCredit.add(amount.abs());
            } else {
                totalDebit = totalDebit.add(amount.abs());
            }

            entities.add(entity);
            rowIndex++;
        }

        // 3. Save Details
        List<JournalRequest> savedEntities = journalRequestRepository.saveAll(entities);

        // 4. Save Summary
        String sumSql = "INSERT INTO JOURNAL_BATCH_MASTER (BATCH_ID, CREATOR_ID, REQ_DATE, BATCH_REMARKS, TOTAL_ROWS, TOTAL_DEBIT, TOTAL_CREDIT, BATCH_STATUS) VALUES (?, ?, ?, ?, ?, ?, ?, ?)";

        jdbcTemplate.update(sumSql,
                batchId,
                creatorId,
                ts,
                dto.getCommonBatchRemarks(),
                entities.size(),
                totalDebit,
                totalCredit,
                "PENDING"
        );

        // 5. Audit & Notification
        logAudit(creatorId, "CREATE_SUCCESS", "BATCH_MANUAL", "Created Manual Batch " + batchId);

        try {
            NotificationConfigDto config = permissionConfigService.getConfig("JOURNAL_AUTH");
            if (config != null) {
                String msg = String.format("New Manual Batch %s by %s requires approval.", batchId, creatorId);
                notificationWriterService.createNotification(
                        null,
                        config.getTargetRoles(),
                        msg,
                        "/journal-view/" + batchId,
                        batchId,
                        "JournalService"
                );
            }
        } catch (Exception e) {
            log.warn("Notification failed for manual batch {}", batchId, e);
        }

        return savedEntities;
    }


    @Override
    public String createBulkBatchRequest(BatchRequestDto d, String u, Integer r) throws JsonProcessingException {
        return "";
    }

    @Override
    public String createBatchFromCache(String r, String m, String u, Integer o) throws IOException {
        return createBatchFromCacheAsync(r, m, u, o);
    }

    @Override
    public List<JournalRequest> processBulkRequests(BulkProcessJournalRequestDto d, String u, Integer r) {
        processBulkRequestsAsync(d, u, r);
        return Collections.emptyList();
    }
}



















package com.fincore.JournalService.Service;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.fincore.JournalService.Dto.TaskProgressDto;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.data.redis.core.StringRedisTemplate;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;

import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicLong;

@Service
@RequiredArgsConstructor
@Slf4j
public class ProgressService {

    private final StringRedisTemplate stringRedisTemplate;
    private final ObjectMapper objectMapper;

    @Value("${notification.progress.topic:notifications:progress}")
    private String progressTopic;

    // Cache to prevent flooding Redis (Throttling)
    private final ConcurrentHashMap<String, AtomicLong> throttleMap = new ConcurrentHashMap<>();

    @Async("bulkExecutor")
    public void sendProgress(String taskId, String userId, int percent, String status, String message) {
        try {
            // 1. Throttling: Limit updates to 1 per 500ms (except 0%, 100%, or FAILED)
            if (percent > 0 && percent < 100 && !"FAILED".equals(status)) {
                AtomicLong lastTime = throttleMap.computeIfAbsent(taskId, k -> new AtomicLong(0));
                long now = System.currentTimeMillis();
                if (now - lastTime.get() < 500) return; // Skip
                lastTime.set(now);
            }

            // 2. Build Payload
            TaskProgressDto dto = TaskProgressDto.builder()
                    .taskId(taskId)
                    .userId(userId)
                    .percentage(percent)
                    .status(status)
                    .message(message)
                    .build();

            // 3. Publish
            stringRedisTemplate.convertAndSend(progressTopic, objectMapper.writeValueAsString(dto));

            // 4. Cleanup
            if (percent == 100 || "FAILED".equals(status)) {
                throttleMap.remove(taskId);
            }
        } catch (Exception e) {
            log.warn("Progress Notification Failed: {}", e.getMessage());
        }
    }
}



















package com.fincore.JournalService.Service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.stereotype.Service;

import java.util.HashSet;
import java.util.List;
import java.util.Set;

/**
 * Service to Fetch Master Data for Bulk Validation.
 * Optimizes performance by loading active codes into memory sets for O(1) lookups.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class ValidationMasterService {

    @Autowired
    @Qualifier("oracleJdbcTemplate")
    private JdbcTemplate jdbcTemplate;

    public Set<String> getAllActiveBranches() {
        long start = System.currentTimeMillis();
        // Uses fetchSize for faster network transfer
        String sql = "SELECT CODE FROM BRANCH_MASTER WHERE STATUS = 1";
        List<String> list = jdbcTemplate.query(con -> {
            var ps = con.prepareStatement(sql);
            ps.setFetchSize(5000);
            return ps;
        }, (rs, rowNum) -> rs.getString(1));

        log.info("Loaded {} Active Branches in {}ms", list.size(), System.currentTimeMillis() - start);
        return new HashSet<>(list);
    }

    public Set<String> getAllActiveCurrencies() {
        long start = System.currentTimeMillis();
        String sql = "SELECT CURRENCY_CODE FROM CURRENCY_MASTER WHERE FLAG = 1";
        List<String> list = jdbcTemplate.query(con -> {
            var ps = con.prepareStatement(sql);
            ps.setFetchSize(1000);
            return ps;
        }, (rs, rowNum) -> rs.getString(1));

        log.info("Loaded {} Active Currencies in {}ms", list.size(), System.currentTimeMillis() - start);
        return new HashSet<>(list);
    }

    public Set<String> getAllActiveCgls() {
        long start = System.currentTimeMillis();
        String sql = "SELECT CGL_NUMBER FROM CGL_MASTER WHERE STATUS = 1";
        List<String> list = jdbcTemplate.query(con -> {
            var ps = con.prepareStatement(sql);
            ps.setFetchSize(2000);
            return ps;
        }, (rs, rowNum) -> rs.getString(1));

        log.info("Loaded {} Active CGLs in {}ms", list.size(), System.currentTimeMillis() - start);
        return new HashSet<>(list);
    }
}






























package com.fincore.JournalService.Service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Propagation;
import org.springframework.transaction.annotation.Transactional;

import com.fincore.JournalService.Models.Notifications;
import com.fincore.JournalService.Repository.NotificationRepository;

@Service
@RequiredArgsConstructor
@Slf4j
public class NotificationWriterService {

    private final NotificationRepository notificationRepository;

    @Transactional(propagation = Propagation.MANDATORY) 
    public void createNotification(String userId, String targetRole, String message, String linkUrl, String aggregateId, String eventSource) {

        if (userId == null && targetRole == null) {
            log.warn("Skipping notification creation: Both userId and targetRole are null. AggregateID: {}", aggregateId);
            return;
        }

        if (message == null || message.isBlank()) {
            log.warn("Skipping notification creation: Message is null or blank. AggregateID: {}", aggregateId);
            throw new IllegalArgumentException("Notification message cannot be null or blank.");
        }

        Notifications notification = Notifications.builder()
                .userId(userId)
                .targetRole(targetRole)
                .message(message)
                .linkUrl(linkUrl)
                .aggregateId(aggregateId)
                .eventSource(eventSource)
                .build();

        notificationRepository.save(notification);

        if (userId != null) {
            log.info("Saved 1-to-1 notification event for user: {} (AggregateID: {})", userId, aggregateId);
        } else {
            log.info("Saved 1-to-many notification event for role: {} (AggregateID: {})", targetRole, aggregateId);
        }
    }
}












package com.fincore.JournalService.Service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.cache.annotation.Cacheable;
import org.springframework.stereotype.Service;

import com.fincore.JournalService.Dto.NotificationConfigDto;
import com.fincore.JournalService.Repository.PermissionRepository;

import java.util.List;
import java.util.stream.Collectors;

@Service
@RequiredArgsConstructor
@Slf4j
public class PermissionConfigService {

    private final PermissionRepository permissionRepository;

    @Cacheable(value = "notification_configs", key = "#requestType")
    public NotificationConfigDto getConfig(String requestType) {

        log.info("Cache Miss: Fetching DB permissions for MOC notification type: {}", requestType);

        List<Object[]> results = permissionRepository.findUrlAndRolesByRequestType(requestType);

        if (results.isEmpty()) {
            log.warn("No notification config found for request type {}. Using defaults.", requestType);
            return new NotificationConfigDto("/dashboard", null); // Default to no roles
        }

        String url = (String) results.get(0)[0];
        if (url == null) url = "/dashboard";

        String roles = results.stream()
                .map(row -> String.valueOf(row[1])) // Role ID
                .distinct()
                .collect(Collectors.joining(","));

        if (roles.isEmpty()) {
             log.warn("No roles found for request type {}. Notification will not be sent to a group.", requestType);
             return new NotificationConfigDto(url, null);
        }
        
        log.info("Fetched Notification Config for {}: URL=[{}], Roles=[{}]", requestType, url, roles);
        return new NotificationConfigDto(url, roles);
    }
}












package com.fincore.JournalService.Service;

import jakarta.persistence.EntityManager;
import jakarta.persistence.PersistenceContext;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import jakarta.transaction.Transactional;

@Service
@RequiredArgsConstructor
public class SequenceService {

    @PersistenceContext
    private EntityManager entityManager;

    /**
     * Pads the given number with leading zeros to a total length of 7.
     */
    private String padToSevenDigits(Long id) {
        if (id == null) {
            throw new IllegalStateException("Sequence query returned null");
        }
        return String.format("%07d", id);
    }

    /**
     * Gets the next BATCH_ID from the BATCH_SEQ sequence. 
     */
    @Transactional
    public String getNextBatchId() {
        Number nextId = (Number) entityManager
                .createNativeQuery("select BATCH_SEQ.nextval from dual")
                .getSingleResult();
        
        return padToSevenDigits(nextId.longValue());
    }

    /**
     * Gets the next JOURNAL_ID prefix from the JRNL_SEQ sequence.
     */
    @Transactional
    public String getNextJournalIdPrefix() {
        
      
        Number nextId = (Number) entityManager
                .createNativeQuery("select JRNL_SEQ.nextval from dual")
                .getSingleResult();
      

        return padToSevenDigits(nextId.longValue());
    }
}




***************************************************************************************************

package com.fincore.JournalService;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cache.annotation.EnableCaching;
import org.springframework.data.jpa.repository.config.EnableJpaRepositories;

@SpringBootApplication
@EnableCaching

    public class JournalServiceApplication {
	public static void main(String[] args) {
		SpringApplication.run(JournalServiceApplication.class, args);
	}

}

















*************************************************************************************************
spring.application.name=JournalService
spring.datasource.driver-class-name=oracle.jdbc.OracleDriver
spring.profiles.active=dev
server.port=9999

#todo Expose all actuator endpoints over HTTP.
management.endpoints.web.exposure.include=*

# Always show full details on the health endpoint (e.g., database connection status)
management.endpoint.health.show-details=always

# Add some custom info to the /info endpoint
info.app.name=JournalService
info.app.description=Service for managing journals.
info.app.version=1.0.0


# --- Redis Configuration ---
# Ensure timeouts are set so a Redis glitch doesn't hang the app
spring.data.redis.host=10.0.17.242
spring.data.redis.port=6379
spring.data.redis.timeout=2000
spring.data.redis.jedis.pool.max-active=50
spring.data.redis.jedis.pool.max-idle=10
spring.data.redis.jedis.pool.min-idle=5

# --- 3. FILE UPLOAD LIMITS ---
# 600k rows in Excel can be 50MB+. Ensure Spring accepts it.
spring.servlet.multipart.max-file-size=100MB
spring.servlet.multipart.max-request-size=100MB

# --- 4. ASYNC EXECUTOR TUNING (Matches our Java Config) ---
# Verify these match the AsyncConfig.java values
app.async.core-pool-size=5
app.async.max-pool-size=20
app.async.queue-capacity=500
# ----------------------------------


# LOGIN SERVICE KEY
jwt.secret=bWV0aGlvbnlsdGhyZW9ueWx0aHJlb255bGdsdXRhbWlueWxhbGFueWw=

spring.datasource.oracle.jdbc-url=jdbc:oracle:thin:@10.177.103.192:1523/fincorepdb1
spring.datasource.oracle.username=fincore
spring.datasource.oracle.password=Password#1234
spring.datasource.oracle.driver-class-name=oracle.jdbc.OracleDriver
# --- 1. ORACLE CONNECTION POOL (HikariCP) ---
# Critical: We are doing parallel inserts (8 threads).
# If pool is too small, threads will wait.
spring.datasource.hikari.maximum-pool-size=30
spring.datasource.hikari.minimum-idle=10
spring.datasource.hikari.connection-timeout=30000
spring.datasource.hikari.idle-timeout=600000
spring.datasource.hikari.max-lifetime=1800000

# Hibernate specific config for Oracle
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.OracleDialect
spring.jpa.show-sql=false




# --- 2. Secondary DataSource (Hive / Thrift / Delta Lake) ---
# Used for Data Lake connectivity
spring.datasource.hive.jdbc-url=jdbc:hive2://spark-thrift:10000/default
spring.datasource.hive.driver-class-name=org.apache.hive.jdbc.HiveDriver
# Validation query to ensure connection is alive (Hive specific)
spring.datasource.hive.test-on-borrow=true
spring.datasource.hive.validation-query=SELECT 1





















tables: 
***************************************************************************************************
journal_requests :

REQ_ID	NUMBER	No		1	
REQ_STATUS	VARCHAR2(10 CHAR)	No	'P' 	2	
CHANGE_TYPE	VARCHAR2(10 CHAR)	No		3	
REQ_DATE	DATE	No	SYSDATE 	4	
CREATOR_ID	VARCHAR2(12 CHAR)	No		5	
EXECUTOR_ID	VARCHAR2(12 CHAR)	Yes		6	
EXECUTION_DATE	DATE	Yes		7	
EXECUTOR_REMARKS	VARCHAR2(50 CHAR)	Yes		8	
PAYLOAD	CLOB	Yes		9	
BATCH_ID	VARCHAR2(50 CHAR)	Yes		10	
JOURNAL_ID	VARCHAR2(50 CHAR)	Yes		11	
COMMON_BATCH_REMARKS	VARCHAR2(50 CHAR)	Yes		12	
CREATOR_ROLE	NUMBER(10,0)	Yes		13	


data :

1404594	A	ADD	31-12-25	3333333	1021253	31-12-25	DAS	{"changeType":"ADD","masterJournalId":null,"pDate":"2025-12-19","branch":"32098","currency":"USD","cgl":"5051501701","amount":-9997.15,"productType":"12345678","remarks":"Balanced_Txn_326_Cr","arFlag":"A","acClassification":"A","batchId":"0001063","journalId":"0004025-006","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":6}	0001063	0004025-006	Bulk Upload - 2025-12-17	11
1408978	A	ADD	05-01-26	3333333	1021256	05-01-26	ASD	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-02","branch":"32121","currency":"USD","cgl":"1122505001","amount":-10000,"productType":"12345679","remarks":"Normal","arFlag":"A","acClassification":"A","batchId":"0001158","journalId":"0004707-002","commonBatchRemarks":"Bulk Upload - 2025-12-02","transactionCount":2}	0001158	0004707-002	Bulk Upload - 2025-12-02	11
1408977	A	ADD	05-01-26	3333333	1021256	05-01-26	ASD	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-02","branch":"32121","currency":"USD","cgl":"1208505003","amount":10000,"productType":"12345678","remarks":"ewreew","arFlag":"A","acClassification":"A","batchId":"0001158","journalId":"0004707-001","commonBatchRemarks":"Bulk Upload - 2025-12-02","transactionCount":1}	0001158	0004707-001	Bulk Upload - 2025-12-02	11
1408974	A	ADD	05-01-26	3333333	1021256	05-01-26	asd	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-02","branch":"32121","currency":"USD","cgl":"1122505001","amount":-10000,"productType":"12345679","remarks":"Normal","arFlag":"A","acClassification":"A","batchId":"0001156","journalId":"0004705-002","commonBatchRemarks":"Bulk Upload - 2025-12-02","transactionCount":2}	0001156	0004705-002	Bulk Upload - 2025-12-02	11
1408973	A	ADD	05-01-26	3333333	1021256	05-01-26	asd	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-02","branch":"32121","currency":"USD","cgl":"1208505003","amount":10000,"productType":"12345678","remarks":"ewreew","arFlag":"A","acClassification":"A","batchId":"0001156","journalId":"0004705-001","commonBatchRemarks":"Bulk Upload - 2025-12-02","transactionCount":1}	0001156	0004705-001	Bulk Upload - 2025-12-02	11
1408970	A	ADD	05-01-26	3333333	1021256	05-01-26	asd	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-02","branch":"32121","currency":"USD","cgl":"1122505001","amount":-10000,"productType":"12345679","remarks":"Normal","arFlag":"A","acClassification":"A","batchId":"0001154","journalId":"0004703-002","commonBatchRemarks":"Bulk Upload - 2025-12-02","transactionCount":2}	0001154	0004703-002	Bulk Upload - 2025-12-02	11
1408969	A	ADD	05-01-26	3333333	1021256	05-01-26	asd	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-02","branch":"32121","currency":"USD","cgl":"1208505003","amount":10000,"productType":"12345678","remarks":"ewreew","arFlag":"A","acClassification":"A","batchId":"0001154","journalId":"0004703-001","commonBatchRemarks":"Bulk Upload - 2025-12-02","transactionCount":1}	0001154	0004703-001	Bulk Upload - 2025-12-02	11
1406631	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"14443","currency":"USD","cgl":"5051220801","amount":9153.92,"productType":"12345678","remarks":"Balanced_Txn_555_Dr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004367-005","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":5}	0001149	0004367-005	Bulk Upload - 2025-12-17	11
1406630	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"14443","currency":"USD","cgl":"2051070601","amount":-3325.87,"productType":"12345678","remarks":"Balanced_Txn_490_Cr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004367-004","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":4}	0001149	0004367-004	Bulk Upload - 2025-12-17	11
1406629	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"14443","currency":"USD","cgl":"2051070601","amount":3325.87,"productType":"12345678","remarks":"Balanced_Txn_490_Dr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004367-003","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":3}	0001149	0004367-003	Bulk Upload - 2025-12-17	11
1406628	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"14443","currency":"USD","cgl":"5051080630","amount":-1428.23,"productType":"12345678","remarks":"Balanced_Txn_162_Cr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004367-002","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":2}	0001149	0004367-002	Bulk Upload - 2025-12-17	11
1406627	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"14443","currency":"USD","cgl":"5051080630","amount":1428.23,"productType":"12345678","remarks":"Balanced_Txn_162_Dr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004367-001","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":1}	0001149	0004367-001	Bulk Upload - 2025-12-17	11
1406626	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"31977","currency":"USD","cgl":"5051500130","amount":-9792.25,"productType":"12345678","remarks":"Balanced_Txn_488_Cr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004366-004","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":4}	0001149	0004366-004	Bulk Upload - 2025-12-17	11
1406625	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"31977","currency":"USD","cgl":"5051500130","amount":9792.25,"productType":"12345678","remarks":"Balanced_Txn_488_Dr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004366-003","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":3}	0001149	0004366-003	Bulk Upload - 2025-12-17	11
1406624	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"31977","currency":"USD","cgl":"7456505002","amount":-8177.29,"productType":"12345678","remarks":"Balanced_Txn_138_Cr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004366-002","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":2}	0001149	0004366-002	Bulk Upload - 2025-12-17	11
1406623	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"31977","currency":"USD","cgl":"7456505002","amount":8177.29,"productType":"12345678","remarks":"Balanced_Txn_138_Dr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004366-001","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":1}	0001149	0004366-001	Bulk Upload - 2025-12-17	11
1407047	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"14698","currency":"USD","cgl":"2080501103","amount":1590.84,"productType":"12345678","remarks":"Balanced_Txn_130_Dr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004429-001","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":1}	0001149	0004429-001	Bulk Upload - 2025-12-17	11


gl_balance : 

ID	NUMBER(19,0)	No	"FINCORE"."GL_BALANCE_SEQ"."NEXTVAL"	1	Id of the balance
BALANCE_DATE	DATE	No	NULL 	2	Date of the balance recorded
BRANCH_CODE	VARCHAR2(5 BYTE)	No		3	branchcodeis the foreign key of the branch_master
CURRENCY	VARCHAR2(3 BYTE)	No		4	currency is foreign key of currency_master
CGL	VARCHAR2(10 BYTE)	No		5	cgl is foreign key of cgl_master
BALANCE	NUMBER(25,4)	No		6	balance recorded for a date
INR_BALANCE	NUMBER(25,2)	Yes		7	Converted to INR Balance

data :
46232234	11-11-25	09298	USD	2248500101	-472.7385	
46232237	11-11-25	09601	USD	2248500101	-539.044	
46232238	11-11-25	09930	USD	2248500101	-13880.5559	
46232239	11-11-25	09995	USD	2248500101	6693.2951	





gl_transactions :
TRANSACTION_ID	NUMBER(20,0)	No	"FINCORE"."GL_TRANSACTIONS_SEQ"."NEXTVAL"	1	
BATCH_ID	VARCHAR2(50 CHAR)	Yes		2	
JOURNAL_ID	VARCHAR2(50 CHAR)	Yes		3	
TRANSACTION_DATE	DATE	Yes	NULL 	4	
POST_DATE	TIMESTAMP(6)	Yes	SYSDATE 	5	
BRANCH_CODE	VARCHAR2(50 CHAR)	Yes		6	
CURRENCY	VARCHAR2(3 BYTE)	Yes		7	
CGL	VARCHAR2(10 BYTE)	Yes		8	
NARRATION	VARCHAR2(40 BYTE)	Yes		9	
DEBIT_AMOUNT	NUMBER(25,4)	Yes		10	
CREDIT_AMOUNT	NUMBER(25,4)	Yes		11	
TRANSACTION_COUNT	NUMBER(10,0)	Yes		12	
SOURCE_FLAG	VARCHAR2(1 BYTE)	Yes		13	

sample data :

316290541	0001170	0004739-485	02-12-25	06-01-26 04:50:26.062668000 PM	32121	USD	1208505003	ewreew	1000	0	485	J
316290542	0001170	0004739-486	02-12-25	06-01-26 04:50:26.062668000 PM	32121	USD	1122505001	Normal	0	1000	486	J
316290543	0001170	0004739-487	02-12-25	06-01-26 04:50:26.062668000 PM	32121	USD	1208505003	ewreew	1000	0	487	J
316290544	0001170	0004739-488	02-12-25	06-01-26 04:50:26.062668000 PM	32121	USD	1122505001	Normal	0	1000	488	J



currency master :

CURRENCY_CODE	VARCHAR2(3 BYTE)	No		1	Currency Code of Currency
CURRENCY_NAME	VARCHAR2(50 BYTE)	No		2	Name of Currency
FLAG	NUMBER(1,0)	No	0	3	Currency Active or Inactive
CURRENCY_RATE	NUMBER(12,6)	Yes		4	Current Rate of Currency
RATE_DATE	DATE	Yes		5	Rate change date 
CREATED_AT	TIMESTAMP(6)	Yes	CURRENT_TIMESTAMP	6	Currency created date
UPDATED_AT	TIMESTAMP(6)	Yes	"CURRENT_TIMESTAMP
   "	7	Currency updated date


data :
VCY	drtvce	0	1.134546		15-12-25 12:21:10.611561000 PM	15-12-25 12:21:10.611561000 PM
RET	hfbdhfadbfhd	0	65.26565		15-12-25 12:52:50.394294000 PM	15-12-25 12:52:50.394294000 PM
MAT	PAMN	0	12.121321		19-12-25 06:34:35.235474000 AM	19-12-25 06:34:35.235477000 AM
GYK	gvkj 	0	1.1		15-12-25 11:39:22.736494000 AM	15-12-25 11:39:22.736494000 AM
GHF	iutyjgtkgjykjhjkgjkggjgjhhghj	0	465564.465454		15-12-25 11:39:42.892401000 AM	15-12-25 11:39:42.892401000 AM







// new introduced queries for fast performance check once for your reference if correc or not and also allign with thye application or not  :-




-- =====================================================================
-- 1. ADD COLUMNS TO JOURNAL_REQUEST
-- We keep 'PAYLOAD' for legacy UI support, but add these for speed.
-- =====================================================================
ALTER TABLE JOURNAL_REQUEST ADD (
    REQ_BRANCH_CODE VARCHAR2(50),
    REQ_CURRENCY    VARCHAR2(3),
    REQ_CGL         VARCHAR2(50),
    REQ_AMOUNT      NUMBER(25, 4), -- Will store Signed Amount (-ve for Credit)
    REQ_CSV_DATE    DATE,
    REQ_NARRATION   VARCHAR2(200),
    REQ_PRODUCT     VARCHAR2(50)
);

-- =====================================================================
-- 2. ADD INDEX FOR PERFORMANCE
-- Critical for the Stored Procedure to find batch rows instantly.
-- =====================================================================
CREATE INDEX IDX_JR_BATCH_OPT ON JOURNAL_REQUEST(BATCH_ID, REQ_STATUS);

-- High-Cardinality Index for specific transaction search
CREATE INDEX IDX_JR_JOURNAL_ID ON JOURNAL_REQUEST(JOURNAL_ID);

-- Branch filtering speed
CREATE INDEX IDX_JR_BRANCH_CODE ON JOURNAL_REQUEST(REQ_BRANCH_CODE);

commit;


-- =====================================================================
-- 3. CREATE/REPLACE THE PROCESSING PROCEDURE
-- This replaces the Java-side "Accept" loop.
-- =====================================================================

create or replace PROCEDURE PROCESS_JOURNAL_BATCH (
    p_batch_id      IN VARCHAR2,
    p_executor_id   IN VARCHAR2,
    p_remarks       IN VARCHAR2,
    p_status        IN VARCHAR2, 
    o_cursor        OUT SYS_REFCURSOR
) AS
    v_missing_rates NUMBER;
BEGIN
    -- Enable Parallel DML
    EXECUTE IMMEDIATE 'ALTER SESSION ENABLE PARALLEL DML';

    -- 0. PRE-CHECK: INTEGRITY GUARD
    -- Before processing, ensure ALL non-INR currencies in this batch have an active rate.
    -- If even one is missing, FAIL the whole batch.
    SELECT COUNT(*)
    INTO v_missing_rates
    FROM (
        SELECT DISTINCT REQ_CURRENCY 
        FROM JOURNAL_REQUEST 
        WHERE BATCH_ID = p_batch_id AND REQ_STATUS = 'P'
    ) req
    LEFT JOIN CURRENCY_MASTER cm 
        ON req.REQ_CURRENCY = cm.CURRENCY_CODE AND cm.FLAG = 1 -- Active Only
    WHERE req.REQ_CURRENCY <> 'INR' -- Ignore Base Currency
      AND cm.CURRENCY_RATE IS NULL;

    IF v_missing_rates > 0 THEN
        RAISE_APPLICATION_ERROR(-20001, 'CRITICAL: Active Exchange Rate missing for foreign currency in Batch. Operation Aborted.');
    END IF;

    -- BEGIN ATOMIC TRANSACTION
    BEGIN
        -- 1. Insert Transactions
        INSERT /*+ PARALLEL(GL_TRANSACTIONS, 8) */ INTO GL_TRANSACTIONS (
            TRANSACTION_ID, BATCH_ID, JOURNAL_ID, TRANSACTION_DATE, POST_DATE,
            BRANCH_CODE, CURRENCY, CGL, NARRATION, DEBIT_AMOUNT, CREDIT_AMOUNT, SOURCE_FLAG
        )
        SELECT /*+ PARALLEL(JOURNAL_REQUEST, 8) */
            GL_TRANSACTIONS_SEQ.nextval, BATCH_ID, JOURNAL_ID, NVL(REQ_CSV_DATE, TRUNC(SYSDATE)), SYSTIMESTAMP,
            REQ_BRANCH_CODE, REQ_CURRENCY, REQ_CGL, REQ_NARRATION,
            CASE WHEN REQ_AMOUNT > 0 THEN REQ_AMOUNT ELSE 0 END, 
            CASE WHEN REQ_AMOUNT < 0 THEN ABS(REQ_AMOUNT) ELSE 0 END, 
            'J'
        FROM JOURNAL_REQUEST
        WHERE BATCH_ID = p_batch_id AND REQ_STATUS = 'P';

        -- 2. Merge Balances (STRICT RATE LOGIC)
        MERGE /*+ PARALLEL(target, 8) */ INTO GL_BALANCE target
        USING (
            SELECT /*+ PARALLEL(j, 8) */
                j.REQ_BRANCH_CODE, j.REQ_CURRENCY, j.REQ_CGL, NVL(j.REQ_CSV_DATE, TRUNC(SYSDATE)) as BAL_DATE,
                SUM(j.REQ_AMOUNT) as TXN_AMOUNT,
                -- Logic: If INR, Rate is 1. If Foreign, Rate MUST exist (Checked in Step 0).
                CASE 
                    WHEN j.REQ_CURRENCY = 'INR' THEN 1 
                    ELSE MAX(c.CURRENCY_RATE) 
                END as EXCH_RATE 
            FROM JOURNAL_REQUEST j
            LEFT JOIN CURRENCY_MASTER c ON j.REQ_CURRENCY = c.CURRENCY_CODE AND c.FLAG = 1 
            WHERE j.BATCH_ID = p_batch_id AND j.REQ_STATUS = 'P'
            GROUP BY j.REQ_BRANCH_CODE, j.REQ_CURRENCY, j.REQ_CGL, j.REQ_CSV_DATE
        ) source
        ON (target.BRANCH_CODE = source.REQ_BRANCH_CODE AND target.CURRENCY = source.REQ_CURRENCY AND target.CGL = source.REQ_CGL AND target.BALANCE_DATE = source.BAL_DATE)
        WHEN MATCHED THEN
            UPDATE SET target.BALANCE = target.BALANCE + source.TXN_AMOUNT, target.INR_BALANCE = NVL(target.INR_BALANCE, 0) + (source.TXN_AMOUNT * source.EXCH_RATE)
        WHEN NOT MATCHED THEN
            INSERT (ID, BALANCE_DATE, BRANCH_CODE, CURRENCY, CGL, BALANCE, INR_BALANCE)
            VALUES (GL_BALANCE_SEQ.nextval, source.BAL_DATE, source.REQ_BRANCH_CODE, source.REQ_CURRENCY, source.REQ_CGL, source.TXN_AMOUNT, (source.TXN_AMOUNT * source.EXCH_RATE));

        -- 3. Update Status
        UPDATE /*+ PARALLEL(JOURNAL_REQUEST, 8) */ JOURNAL_REQUEST
        SET REQ_STATUS = p_status, EXECUTOR_ID = p_executor_id, EXECUTION_DATE = SYSDATE, EXECUTOR_REMARKS = p_remarks
        WHERE BATCH_ID = p_batch_id AND REQ_STATUS = 'P';

        COMMIT; 

        -- 4. Return Cursor
        OPEN o_cursor FOR
        SELECT 
            j.REQ_BRANCH_CODE AS BRANCH, j.REQ_CURRENCY AS CURRENCY, j.REQ_CGL AS CGL, j.REQ_CSV_DATE AS BAL_DATE,
            g.BALANCE AS NEW_BALANCE, g.INR_BALANCE AS NEW_INR_BALANCE
        FROM (SELECT DISTINCT REQ_BRANCH_CODE, REQ_CURRENCY, REQ_CGL, REQ_CSV_DATE FROM JOURNAL_REQUEST WHERE BATCH_ID = p_batch_id) j
        JOIN GL_BALANCE g ON g.BRANCH_CODE = j.REQ_BRANCH_CODE AND g.CURRENCY = j.REQ_CURRENCY AND g.CGL = j.REQ_CGL AND g.BALANCE_DATE = j.REQ_CSV_DATE;
    
    EXCEPTION
        WHEN OTHERS THEN
            ROLLBACK;
            RAISE; 
    END;
END;
/

commit;






---------------------------------------


-- Table to hold batches where HDFS Sync failed
CREATE TABLE HDFS_SYNC_RETRY_QUEUE (
    BATCH_ID      VARCHAR2(50) PRIMARY KEY,
    CREATED_AT    TIMESTAMP DEFAULT SYSTIMESTAMP,
    RETRY_COUNT   NUMBER DEFAULT 0,
    STATUS        VARCHAR2(20) DEFAULT 'PENDING' -- PENDING, FAILED, COMPLETED
);

-- Index for the Scheduler to pick up pending items quickly
CREATE INDEX IDX_HDFS_RETRY_STATUS ON HDFS_SYNC_RETRY_QUEUE(STATUS);
commit;




-----------------------------------------------------------


-- 1. Create the Lightweight Summary Table
CREATE TABLE JOURNAL_BATCH_MASTER (
    BATCH_ID          VARCHAR2(50) PRIMARY KEY,
    CREATOR_ID        VARCHAR2(50),
    REQ_DATE          TIMESTAMP,
    BATCH_REMARKS     VARCHAR2(200),
    TOTAL_ROWS        NUMBER DEFAULT 0,
    TOTAL_DEBIT       NUMBER(25, 4) DEFAULT 0,
    TOTAL_CREDIT      NUMBER(25, 4) DEFAULT 0,
    BATCH_STATUS      VARCHAR2(20), -- PENDING, ACCEPTED, REJECTED, DELETED
    EXECUTOR_ID       VARCHAR2(50),
    EXECUTION_DATE    TIMESTAMP,
    EXECUTOR_REMARKS  VARCHAR2(200)
);

-- 2. Create Index for super-fast dashboard filtering
CREATE INDEX IDX_JBM_STATUS_CREATOR ON JOURNAL_BATCH_MASTER(BATCH_STATUS, CREATOR_ID);

commit;


-- 3. MIGRATE EXISTING DATA (One-time fix for your 5.75 Lakh rows)
-- This aggregates your existing rows into the new master table.
INSERT INTO JOURNAL_BATCH_MASTER (
    BATCH_ID, CREATOR_ID, REQ_DATE, BATCH_REMARKS, 
    TOTAL_ROWS, TOTAL_DEBIT, TOTAL_CREDIT, BATCH_STATUS,
    EXECUTOR_ID, EXECUTION_DATE, EXECUTOR_REMARKS
)
SELECT 
    BATCH_ID,
    MAX(CREATOR_ID),
    MAX(REQ_DATE),
    MAX(COMMON_BATCH_REMARKS),
    COUNT(*),
    SUM(CASE WHEN REQ_AMOUNT > 0 THEN REQ_AMOUNT ELSE 0 END),
    SUM(CASE WHEN REQ_AMOUNT < 0 THEN ABS(REQ_AMOUNT) ELSE 0 END),
    CASE 
        WHEN MAX(REQ_STATUS) = 'P' THEN 'PENDING'
        WHEN MAX(REQ_STATUS) = 'A' THEN 'ACCEPTED'
        WHEN MAX(REQ_STATUS) = 'R' THEN 'REJECTED'
        ELSE MAX(REQ_STATUS)
    END,
    MAX(EXECUTOR_ID),
    MAX(EXECUTION_DATE),
    MAX(EXECUTOR_REMARKS)
FROM JOURNAL_REQUEST
GROUP BY BATCH_ID;

COMMIT;












//


CREATE OR REPLACE PROCEDURE PURGE_JOURNAL_BATCH(
    p_batch_id IN VARCHAR2,
    p_user_id IN VARCHAR2
) AS
    v_status VARCHAR2(20);
BEGIN
    -- 1. Security Check
    SELECT BATCH_STATUS INTO v_status
    FROM JOURNAL_BATCH_MASTER
    WHERE BATCH_ID = p_batch_id AND CREATOR_ID = p_user_id;

    -- ALLOW 'PENDING' OR 'QUEUED' (Locked by Java)
    IF v_status NOT IN ('PENDING', 'QUEUED', 'ERROR') THEN
        RAISE_APPLICATION_ERROR(-20002, 'Cannot delete batch. Status is ' || v_status);
    END IF;

    -- 2. Fast Chunked Delete
    LOOP
        DELETE FROM JOURNAL_REQUEST
        WHERE BATCH_ID = p_batch_id
          AND REQ_STATUS IN ('P', 'R') -- Clean PENDING or REJECTED
          AND ROWNUM <= 50000;

        EXIT WHEN SQL%ROWCOUNT = 0;
        COMMIT; 
    END LOOP;

    -- 3. Delete Master
    DELETE FROM JOURNAL_BATCH_MASTER WHERE BATCH_ID = p_batch_id;
    COMMIT;

EXCEPTION
    WHEN NO_DATA_FOUND THEN
        RAISE_APPLICATION_ERROR(-20003, 'Batch not found or access denied.');
END;
/

commit;









//--------------------------------------------------------------------------------



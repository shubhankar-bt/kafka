package com.fincore.JournalService.Aspect;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.fincore.JournalService.Dto.BatchRequestDto;
import com.fincore.JournalService.Dto.BulkProcessJournalRequestDto;
import com.fincore.JournalService.Dto.ProcessJournalRequestDto;
import com.fincore.JournalService.Models.JournalLog;
import com.fincore.JournalService.Models.JournalRequest;
import com.fincore.JournalService.Repository.JournalLogRepository;
import jakarta.servlet.http.HttpServletRequest;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.aspectj.lang.JoinPoint;
import org.aspectj.lang.annotation.AfterReturning;
import org.aspectj.lang.annotation.Aspect;
import org.springframework.stereotype.Component;
import org.springframework.web.context.request.RequestContextHolder;
import org.springframework.web.context.request.ServletRequestAttributes;

import java.time.LocalDateTime;

@Aspect
@Component
@Slf4j
@RequiredArgsConstructor
public class JournalActivityLogger {

    private final JournalLogRepository journalLogRepository;
    private final ObjectMapper objectMapper;

    // --- 1. Log Batch Creation (Standard) ---
    @AfterReturning(
            pointcut = "execution(* com.fincore.JournalService.Service.JournalRequestService.createBatchRequest(..)) && args(batchDto, creatorId, creatorRole)",
            returning = "result"
    )
    public void logCreateBatch(JoinPoint joinPoint, BatchRequestDto batchDto, String creatorId, Integer creatorRole, Object result) {
        String summary = "Created Batch with " + (batchDto.getRows() != null ? batchDto.getRows().size() : 0) + " rows.";
        saveLog(creatorId, "CREATE", "BATCH_CREATION", summary, null, null);
    }

    // --- 2. Log Bulk Batch Creation ---
    @AfterReturning(
            pointcut = "execution(* com.fincore.JournalService.Service.JournalRequestService.createBulkBatchRequest(..)) && args(batchDto, creatorId, creatorRole)",
            returning = "batchId"
    )
    public void logBulkCreate(JoinPoint joinPoint, BatchRequestDto batchDto, String creatorId, Integer creatorRole, Object batchId) {
        String summary = "Bulk Upload Created. Batch ID: " + batchId + ". Rows: " + (batchDto.getRows() != null ? batchDto.getRows().size() : 0);
        saveLog(creatorId, "CREATE", "BULK_UPLOAD", summary, null, null);
    }

    // --- 3. Log Status Update (Approve/Reject) ---
    @AfterReturning(
            pointcut = "execution(* com.fincore.JournalService.Service.JournalRequestService.updateRequestStatus(..)) && args(dto, executorId, executorRole)",
            returning = "result"
    )
    public void logStatusUpdate(JoinPoint joinPoint, ProcessJournalRequestDto dto, String executorId, Integer executorRole, Object result) {
        if (result instanceof JournalRequest) {
            JournalRequest req = (JournalRequest) result;
            String newValue = "Status: " + dto.getStatus() + ", Remarks: " + dto.getRemarks();
            saveLog(executorId, dto.getStatus().toString(), "STATUS_UPDATE", newValue, null, req.getId());
        }
    }

    // --- 4. Log Bulk Status Update ---
    @AfterReturning(
            pointcut = "execution(* com.fincore.JournalService.Service.JournalRequestService.processBulkRequests(..)) && args(dto, executorId, executorRole)",
            returning = "result"
    )
    public void logBulkStatusUpdate(JoinPoint joinPoint, BulkProcessJournalRequestDto dto, String executorId, Integer executorRole, Object result) {
        String target = dto.getBatchId() != null ? "Batch ID: " + dto.getBatchId() : "Journals: " + dto.getJournalIdPrefixes();
        String newValue = "Bulk " + dto.getStatus() + " for " + target + ". Remarks: " + dto.getRemarks();
        saveLog(executorId, "BULK_" + dto.getStatus(), "BULK_PROCESS", newValue, null, null);
    }

    // --- 5. Log Cancellations ---
    @AfterReturning(
            pointcut = "execution(* com.fincore.JournalService.Service.JournalRequestService.cancelMyRequest(..)) && args(requestId, userId)"
    )
    public void logCancelRequest(JoinPoint joinPoint, Long requestId, String userId) {
        saveLog(userId, "DELETE", "CANCEL_REQUEST", "User canceled request ID: " + requestId, null, requestId);
    }

    @AfterReturning(
            pointcut = "execution(* com.fincore.JournalService.Service.JournalRequestService.cancelMyRequestsByBatchId(..)) && args(batchId, userId)"
    )
    public void logCancelBatch(JoinPoint joinPoint, String batchId, String userId) {
        saveLog(userId, "DELETE", "CANCEL_BATCH", "User canceled all requests in Batch: " + batchId, null, null);
    }

    // --- Helper ---
    private void saveLog(String userId, String actionType, String changeType, String newValue, String oldValue, Long requestId) {
        try {
            JournalLog logEntry = new JournalLog();
            logEntry.setUserId(userId);
            logEntry.setActionType(actionType);
            logEntry.setChangeType(changeType);
            logEntry.setActionTime(LocalDateTime.now());
            logEntry.setRequestId(requestId);
            logEntry.setIpAddress(getClientIp());

            if (newValue != null && !newValue.getClass().equals(String.class)) {
                newValue = objectMapper.writeValueAsString(newValue);
            }

            logEntry.setNewValue(truncate(newValue, 3900)); // Adjusted to fit DB column
            logEntry.setOldValue(truncate(oldValue, 3900));

            journalLogRepository.save(logEntry);
            log.info("Audit Log Saved: {} performed {} on {}", userId, actionType, (requestId != null ? requestId : "Batch"));

        } catch (Exception e) {
            log.error("Failed to save audit log: {}", e.getMessage());
        }
    }

    private String getClientIp() {
        try {
            HttpServletRequest request = ((ServletRequestAttributes) RequestContextHolder.currentRequestAttributes()).getRequest();
            String xForwardedFor = request.getHeader("X-Forwarded-For");
            if (xForwardedFor != null && !xForwardedFor.isEmpty()) {
                return xForwardedFor.split(",")[0].trim();
            }
            return request.getRemoteAddr();
        } catch (Exception e) {
            return "UNKNOWN";
        }
    }

    private String truncate(String value, int length) {
        if (value != null && value.length() > length) {
            return value.substring(0, length - 3) + "...";
        }
        return value;
    }
}










package com.fincore.JournalService.config;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.fasterxml.jackson.databind.SerializationFeature;
import com.fasterxml.jackson.datatype.jsr310.JavaTimeModule;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;

@Configuration
public class AppConfig {

	@Bean
	@Primary
	public ObjectMapper objectMapper() {
		ObjectMapper mapper = new ObjectMapper();

		mapper.registerModule(new JavaTimeModule());

		mapper.disable(SerializationFeature.WRITE_DATES_AS_TIMESTAMPS);

		return mapper;
	}
}





package com.fincore.JournalService.config;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.scheduling.annotation.EnableAsync;
import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;
import java.util.concurrent.Executor;

@Configuration
@EnableAsync
public class AsyncConfig {

    @Bean(name = "bulkExecutor")
    public Executor bulkExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(5);   // Steady state
        executor.setMaxPoolSize(20);   // Burst capability
        executor.setQueueCapacity(500); // Holds 500 batches in memory if DB is slow
        executor.setThreadNamePrefix("JournalAsync-");
        executor.initialize();
        return executor;
    }
}







package com.fincore.JournalService.config;


import org.springframework.cache.annotation.EnableCaching;
import org.springframework.cache.concurrent.ConcurrentMapCacheManager;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
@EnableCaching
public class CacheConfig {

    @Bean
    public ConcurrentMapCacheManager cacheManager() {
      
        return new ConcurrentMapCacheManager("notification_configs");
    }
}






package com.fincore.JournalService.config;

import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.boot.jdbc.DataSourceBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.jdbc.core.JdbcTemplate;

import javax.sql.DataSource;

/**
 * SECONDARY DATASOURCE CONFIGURATION (Hive/Thrift)
 * * Configures the connection to the Data Lake (Delta Lake).
 * Uses JdbcTemplate for direct SQL execution, suitable for analytics queries.
 */
@Configuration
public class HiveDbConfig {

    @Bean(name = "hiveDataSource")
    @ConfigurationProperties(prefix = "spring.datasource.hive")
    public DataSource hiveDataSource() {
        return DataSourceBuilder.create().build();
    }

    @Bean(name = "hiveJdbcTemplate")
    public JdbcTemplate hiveJdbcTemplate(@Qualifier("hiveDataSource") DataSource dataSource) {
        return new JdbcTemplate(dataSource);
    }
}





package com.fincore.JournalService.config;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.redis.connection.RedisConnectionFactory;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.data.redis.serializer.StringRedisSerializer;

/**
 * Configuration for Redis to handle compressed binary data.
 * This prevents Out-Of-Memory errors by offloading large datasets to Redis.
 */
@Configuration
public class RedisConfig {

    @Bean(name = "byteArrayRedisTemplate")
    public RedisTemplate<String, byte[]> byteArrayRedisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, byte[]> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);

        // Keys are Strings (e.g., "JRNL_DATA_UUID") - Readable in Redis CLI
        template.setKeySerializer(new StringRedisSerializer());

        // Values are raw Byte Arrays (GZIP Compressed Data) - No serialization overhead
        template.setEnableDefaultSerializer(false);

        return template;
    }
}






package com.fincore.JournalService.config;

import jakarta.persistence.EntityManagerFactory;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.boot.jdbc.DataSourceBuilder;
import org.springframework.boot.orm.jpa.EntityManagerFactoryBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import org.springframework.data.jpa.repository.config.EnableJpaRepositories;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.orm.jpa.JpaTransactionManager;
import org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean;
import org.springframework.transaction.PlatformTransactionManager;
import org.springframework.transaction.annotation.EnableTransactionManagement;
import javax.sql.DataSource;
import java.util.HashMap;
import java.util.Map;

/**
 * PRIMARY DATASOURCE CONFIGURATION (Oracle)
 * * This class ensures that all existing JPA Repositories and Entities
 * continue to work using the Oracle database.
 * * We mark beans as @Primary so Spring injects this datasource by default
 * into existing services.
 */
@Configuration
@EnableTransactionManagement
@EnableJpaRepositories(basePackages = "com.fincore.JournalService.Repository", // Location of existing Repos
        entityManagerFactoryRef = "oracleEntityManagerFactory", transactionManagerRef = "oracleTransactionManager")
public class OracleDbConfig {
    @Primary
    @Bean(name = "oracleDataSource")
    @ConfigurationProperties(prefix = "spring.datasource.oracle")
    public DataSource oracleDataSource() {
        return DataSourceBuilder.create().build();
    }

    @Primary
    @Bean(name = "oracleEntityManagerFactory")
    public LocalContainerEntityManagerFactoryBean oracleEntityManagerFactory(
            EntityManagerFactoryBuilder builder,
            @Qualifier("oracleDataSource") DataSource dataSource) {

        Map<String, Object> properties = new HashMap<>();
        properties.put("hibernate.dialect", "org.hibernate.dialect.OracleDialect");
        properties.put("hibernate.hbm2ddl.auto", "update"); // Or 'validate' for prod

        return builder
                .dataSource(dataSource)
                .packages("com.fincore.JournalService.Models") // Location of existing Entities
                .persistenceUnit("oracle")
                .properties(properties)
                .build();
    }

    @Primary
    @Bean(name = "oracleTransactionManager")
    public PlatformTransactionManager oracleTransactionManager(
            @Qualifier("oracleEntityManagerFactory") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }
    @Primary
    @Bean(name = "oracleJdbcTemplate")
    public JdbcTemplate oracleJdbcTemplate(@Qualifier("oracleDataSource") DataSource dataSource) {
        return new JdbcTemplate(dataSource);
    }

}






package com.fincore.JournalService.config;

import jakarta.persistence.AttributeConverter;
import jakarta.persistence.Converter;
import com.fincore.JournalService.Models.enums.RequestStatus;
import java.util.stream.Stream;

@Converter(autoApply = true)
public class RequestStatusConverter implements AttributeConverter<RequestStatus, String> {

    @Override
    public String convertToDatabaseColumn(RequestStatus status) {
        if (status == null) {
            return null;
        }
        return status.getCode(); // Writes "P", "A", "R" to DB
    }

    @Override
    public RequestStatus convertToEntityAttribute(String code) {
        if (code == null) {
            return null;
        }
        return Stream.of(RequestStatus.values())
                .filter(c -> c.getCode().equals(code))
                .findFirst()
                .orElseThrow(() -> new IllegalArgumentException("Unknown status code: " + code));
    }
}









package com.fincore.JournalService.config;

import com.fincore.commonutilities.config.CommonSecurityConfig;
import com.fincore.commonutilities.config.RedisConfig;
import com.fincore.commonutilities.jwt.JwtUtil;
import com.fincore.commonutilities.security.ContextRbacFilter;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Import;
import org.springframework.data.redis.core.StringRedisTemplate;
import org.springframework.security.config.annotation.web.builders.HttpSecurity;
import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;
import org.springframework.security.config.http.SessionCreationPolicy;
import org.springframework.security.web.SecurityFilterChain;
import org.springframework.security.web.authentication.UsernamePasswordAuthenticationFilter;

/**
 * Common Security Configuration.
 * * Aligned with the "Distributed Gateway" architecture.
 * It uses the ContextRbacFilter from Common Utilities to enforce:
 * 1. Token Validity
 * 2. Single Session (Redis check)
 * 3. RBAC Permissions
 */
@Configuration
@EnableWebSecurity
@Import({RedisConfig.class, JwtUtil.class, CommonSecurityConfig.class }) // Import logic from JAR
public class SecurityConfig {

    @Autowired
    private StringRedisTemplate redisTemplate;

    @Autowired
    private JwtUtil jwtUtil;

    @Autowired
    private CommonSecurityConfig commonSecurityConfig; // Wire in the CORS config

    @Bean
    public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
        http
                // 1. Disable CSRF (Stateless API)
                .csrf(csrf -> csrf.disable())

                // 2. Apply Centralized CORS Policy
                .cors(cors -> cors.configurationSource(commonSecurityConfig.corsConfigurationSource()))

                // 3. Stateless Session (No JSESSIONID)
                .sessionManagement(s -> s.sessionCreationPolicy(SessionCreationPolicy.STATELESS))

                // 4. Authorization Rules
                .authorizeHttpRequests(authz -> authz
                        // Public Endpoints
                        .requestMatchers("/actuator/**", "/auth/**", "/error").permitAll()
                        // All other endpoints require Authentication (and RBAC filter check)
                        .anyRequest().authenticated()
                )

                // 5. Add the "Distributed Gateway" Filter
                .addFilterBefore(new ContextRbacFilter(redisTemplate, jwtUtil), UsernamePasswordAuthenticationFilter.class);

        return http.build();
    }
}










--***********************************************************************************************
controllers: 
package com.fincore.JournalService.Controllers;

import lombok.RequiredArgsConstructor;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

import com.fincore.JournalService.Dto.BranchDto;
import com.fincore.JournalService.Service.BranchService;

import java.util.Collections; 
import java.util.List;
import java.util.Map;

@RestController
@RequestMapping("/api/branches-journal")
@RequiredArgsConstructor
public class BranchController {

	private final BranchService branchService;

	@GetMapping("/map")
	public Map<String, String> getBranchMap() {
		return branchService.getBranchCodeAndNameMap();
	}

	@GetMapping("/search")
	public List<BranchDto> searchBranches(@RequestParam String query) {
       
        if (query == null || query.trim().isEmpty()) {
            return Collections.emptyList();
        }
     
		return branchService.searchBranches(query);
	}
}













package com.fincore.JournalService.Controllers;


import com.fincore.JournalService.Dto.CglDto;
import com.fincore.JournalService.Service.CglService;
import lombok.RequiredArgsConstructor;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;
import org.springframework.web.bind.annotation.RestController;

import java.util.List;


@RestController
@RequestMapping("/api/cgl-journal")
@RequiredArgsConstructor
public class CglController {

    private final CglService cglService;

    @GetMapping("/search")
    public ResponseEntity<List<CglDto>> searchCgl(@RequestParam(value = "query", required = false, defaultValue = "") String query) {
        return ResponseEntity.ok(cglService.searchCgls(query));
    }
}















package com.fincore.JournalService.Controllers;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fincore.JournalService.Dto.*;
import com.fincore.JournalService.Models.JournalRequest;
import com.fincore.JournalService.Service.JournalBulkValidationService;
import com.fincore.JournalService.Service.JournalRequestService;
import com.fincore.commonutilities.jwt.JwtUtil;
import jakarta.servlet.http.HttpServletRequest;
import jakarta.validation.Valid;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.core.io.ByteArrayResource;
import org.springframework.core.io.Resource;
import org.springframework.data.domain.PageRequest;
import org.springframework.http.HttpHeaders;
import org.springframework.http.HttpStatus;
import org.springframework.http.MediaType;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;
import org.springframework.web.multipart.MultipartFile;

import java.io.IOException;
import java.time.LocalDate;
import java.time.format.DateTimeFormatter;
import java.util.List;
import java.util.Map;

/**
 * Controller for Journal Request Management.
 * Implements High-Performance Async Endpoints for Bulk Operations.
 */
@RestController
@RequestMapping("/api/journals")
@RequiredArgsConstructor
@Slf4j
public class JournalRequestController {

    private final JournalRequestService journalRequestService;
    private final JournalBulkValidationService journalBulkValidationService;
    private final JwtUtil jwtUtil;

    // ==================================================================================
    // 1. ASYNC BULK OPERATIONS (Fire-and-Forget)
    // ==================================================================================

    /**
     * Create Bulk Batch from Cached Validation Data.
     * Response: Immediate (HTTP 202) with Batch ID. Processing happens in background.
     */
    @PostMapping("/create-batch-from-cache")
    public ResponseEntity<Map<String, Object>> createBatchFromCache(
            @RequestBody Map<String, String> payload,
            @RequestHeader("Authorization") String token) {

        try {
            String userId = jwtUtil.getUserIdFromToken(token);
            Integer userRole = jwtUtil.getUserRoleFromToken(token);

            String batchId = journalRequestService.createBatchFromCacheAsync(
                    payload.get("requestId"),
                    payload.get("commonBatchRemarks"),
                    userId,
                    userRole
            );

            return ResponseEntity.status(HttpStatus.ACCEPTED).body(Map.of(
                    "status", "PROCESSING",
                    "message", "Batch creation initiated in background.",
                    "batchId", batchId
            ));

        } catch (IllegalStateException e) {
            // Handles Redis Lock collisions (Duplicate clicks)
            return ResponseEntity.status(HttpStatus.CONFLICT).body(Map.of("status", "ERROR", "message", e.getMessage()));
        } catch (Exception e) {
            log.error("Batch Init Error", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(Map.of("status", "ERROR", "message", "System error: " + e.getMessage()));
        }
    }

    /**
     * Process (Approve/Reject) Bulk Batch.
     * Response: Immediate (HTTP 202). Uses Oracle Parallel execution in background.
     */
    @PostMapping("/process-bulk")
    public ResponseEntity<?> processBulkRequests(
            @RequestHeader("Authorization") String token,
            @Valid @RequestBody BulkProcessJournalRequestDto dto) {
        try {
            String userId = jwtUtil.getUserIdFromToken(token);
            Integer userRole = jwtUtil.getUserRoleFromToken(token);

            journalRequestService.processBulkRequestsAsync(dto, userId, userRole);

            return ResponseEntity.status(HttpStatus.ACCEPTED).body(Map.of(
                    "status", "PROCESSING",
                    "message", "Approval process started. Notifications will be sent upon completion."
            ));
        } catch (Exception e) {
            log.error("Process Error", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(Map.of("status", "ERROR", "message", "Processing failed."));
        }
    }

    /**
     * Delete/Cancel Batch Asynchronously.
     * Prevents DB Timeouts on large batches by deleting in chunks.
     */
    @DeleteMapping("/my-requests/by-batch/{batchId}")
    public ResponseEntity<?> cancelMyRequestsByBatch(
            @RequestHeader("Authorization") String token,
            @PathVariable String batchId) {
        try {
            String userId = jwtUtil.getUserIdFromToken(token);
            journalRequestService.cancelMyRequestsByBatchIdAsync(batchId, userId);

            return ResponseEntity.status(HttpStatus.ACCEPTED).body(Map.of(
                    "status", "DELETING",
                    "message", "Batch deletion queued. Status will update shortly."
            ));
        } catch (Exception e) {
            log.error("Delete Batch Error", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).body(Map.of("status", "ERROR", "message", "Cancel failed: " + e.getMessage()));
        }
    }

    // ==================================================================================
    // 2. SAFEGUARDED FETCH API
    // ==================================================================================

    /**
     * Get All Requests for a Batch.
     * SAFEGUARD: If batch > 2000 rows, forces Frontend to use Pagination to prevent Server OOM.
     */
    @GetMapping("/by-batch/{batchId}")
    public ResponseEntity<?> getRequestsByBatchId(@PathVariable String batchId) {
        long count = journalRequestService.getRequestCountByBatchId(batchId);

        if (count > 2000) {
            return ResponseEntity.status(HttpStatus.PAYLOAD_TOO_LARGE).body(Map.of(
                    "error", "Batch too large (" + count + " rows). Please use Paginated API.",
                    "suggestion", "/api/journals/by-batch-paginated/" + batchId
            ));
        }
        return ResponseEntity.ok(journalRequestService.getRequestsByBatchId(batchId));
    }

    // ==================================================================================
    // 3. VALIDATION & STATUS
    // ==================================================================================

    @PostMapping(value = "/bulk-validate-init", consumes = MediaType.MULTIPART_FORM_DATA_VALUE)
    public ResponseEntity<?> initiateValidation(
            @RequestParam("file") MultipartFile file,
            @RequestParam("postingDate") String date,
            HttpServletRequest request) {
        try {
            if (file == null || file.isEmpty())
                return ResponseEntity.badRequest().body(Map.of("error", "File is missing"));

            String reqId = journalBulkValidationService.initiateValidation(
                    file.getBytes(),
                    file.getOriginalFilename(),
                    LocalDate.parse(date)
            );
            return ResponseEntity.ok(Map.of("status", "QUEUED", "requestId", reqId));
        } catch (Exception e) {
            return ResponseEntity.badRequest().body(Map.of("error", e.getMessage()));
        }
    }

    @GetMapping("/bulk-status/{requestId}")
    public ResponseEntity<BulkUploadStateDto> checkStatus(@PathVariable String requestId) {
        BulkUploadStateDto state = journalBulkValidationService.getState(requestId);
        return state != null ? ResponseEntity.ok(state) : ResponseEntity.notFound().build();
    }

    // ==================================================================================
    // 4. UTILS & LEGACY SUPPORT
    // ==================================================================================

    @GetMapping("/current-posting-date")
    public String getCurrentPostingDate() {
        return journalRequestService.getCurrentPostingDate().format(DateTimeFormatter.ISO_LOCAL_DATE);
    }

    @GetMapping("/pending-requests-summary")
    public ResponseEntity<?> getPendingBatchSummaries() {
        return ResponseEntity.ok(journalRequestService.getPendingBatchSummaries());
    }

    @GetMapping("/all-requests-summary")
    public ResponseEntity<?> getAllBatchSummaries() {
        return ResponseEntity.ok(journalRequestService.getAllBatchSummaries());
    }

    @GetMapping("/by-batch-paginated/{batchId}")
    public ResponseEntity<?> getRequestsByBatchIdPaginated(
            @PathVariable String batchId,
            @RequestParam(defaultValue = "0") int page,
            @RequestParam(defaultValue = "10") int size) {
        return ResponseEntity.ok(journalRequestService.getRequestsByBatchIdPaginated(batchId, PageRequest.of(page, size)));
    }

    @GetMapping("/download-bulk-file/{requestId}")
    public ResponseEntity<Resource> downloadFile(@PathVariable String requestId, @RequestParam String type) {
        byte[] data = journalBulkValidationService.getFileBytes(requestId, type);
        if (data == null) return ResponseEntity.notFound().build();

        String filename = type.equalsIgnoreCase("ERROR") ? "Error_Report.xlsx" : "Success.csv";
        MediaType mediaType = type.equalsIgnoreCase("ERROR")
                ? MediaType.parseMediaType("application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
                : MediaType.TEXT_PLAIN;

        return ResponseEntity.ok()
                .header(HttpHeaders.CONTENT_DISPOSITION, "attachment; filename=\"" + filename + "\"")
                .contentType(mediaType)
                .body(new ByteArrayResource(data));
    }

    @GetMapping("/download-template")
    public ResponseEntity<Resource> downloadTemplate() throws IOException {
        return ResponseEntity.ok()
                .header(HttpHeaders.CONTENT_DISPOSITION, "attachment; filename=\"Journal_Template.xlsx\"")
                .body(new ByteArrayResource(journalBulkValidationService.generateTemplateBytes()));
    }

    // --- Manual/Small Batch Endpoints ---
    @PostMapping("/create-batch")
    public ResponseEntity<?> createBatchRequest(@Valid @RequestBody BatchRequestDto batchDto, @RequestHeader("Authorization") String token) throws JsonProcessingException {
        return ResponseEntity.status(HttpStatus.CREATED).body(journalRequestService.createBatchRequest(batchDto, jwtUtil.getUserIdFromToken(token), jwtUtil.getUserRoleFromToken(token)));
    }

    @GetMapping("/my-requests")
    public List<JournalRequest> getMyRequests(@RequestHeader("Authorization") String token) {
        return journalRequestService.getMyRequests(jwtUtil.getUserIdFromToken(token));
    }

    @GetMapping("/pending-requests")
    public List<JournalRequest> getPendingRequests(@RequestHeader("Authorization") String token) {
        return journalRequestService.getPendingRequests(jwtUtil.getUserIdFromToken(token), jwtUtil.getUserRoleFromToken(token));
    }

    @PatchMapping("/update-request")
    public JournalRequest updateRequestStatus(@RequestHeader("Authorization") String token, @RequestBody ProcessJournalRequestDto dto) throws JsonProcessingException {
        return journalRequestService.updateRequestStatus(dto, jwtUtil.getUserIdFromToken(token), jwtUtil.getUserRoleFromToken(token)).get();
    }

    @DeleteMapping("/my-request/{requestId}")
    public ResponseEntity<Void> cancelMyRequest(@RequestHeader("Authorization") String token, @PathVariable Long requestId) {
        journalRequestService.cancelMyRequest(requestId, jwtUtil.getUserIdFromToken(token));
        return ResponseEntity.noContent().build();
    }

    @DeleteMapping("/my-requests/by-journal-list")
    public ResponseEntity<?> cancelMyRequestsByJournalPrefixes(@RequestHeader("Authorization") String token, @RequestBody List<String> list) {
        journalRequestService.cancelMyRequestsByJournalPrefixes(list, jwtUtil.getUserIdFromToken(token));
        return ResponseEntity.ok(Map.of("status", "SUCCESS"));
    }

    @GetMapping("/status")
    public ResponseEntity<List<JournalRequestStatusDto>> getJournalStatusList() {
        return ResponseEntity.ok(journalRequestService.getJournalRequestStatusList());
    }
}


















// ***************************************************************************************************
dtos :




package com.fincore.JournalService.Dto;

import com.fincore.JournalService.Models.enums.ChangeType;
import jakarta.validation.Valid;
import jakarta.validation.constraints.NotNull;
import jakarta.validation.constraints.Size;
import java.math.BigDecimal;
import java.time.LocalDate;
import lombok.Data;
import java.util.List;

@Data
public class BatchRequestDto {

    @Data
    public static class JournalRequestRow {
        @NotNull
        private ChangeType changeType = ChangeType.ADD;

        private Long masterJournalId;

        private LocalDate csvDate;

        @NotNull @Size(min = 1, max = 50)
        private String branch;

        @NotNull @Size(min = 3, max = 3)
        private String currency;

        @NotNull @Size(min = 1, max = 50)
        private String cgl;

        @NotNull
        private BigDecimal amount;

        @Size(max = 20)
        private String productType;

        @Size(max = 200)
        private String remarks;

        @Size(min = 1, max = 1)
        private String arFlag = "A";

        @Size(min = 1, max = 1)
        private String acClassification;
    }

    @Size(max = 200)
    private String commonBatchRemarks;

    @Valid
    @NotNull
    private List<JournalRequestRow> rows;
}










package com.fincore.JournalService.Dto;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;


@Data
@NoArgsConstructor
@AllArgsConstructor
public class BranchDto {
    private String code;
    private String name;
    
}












package com.fincore.JournalService.Dto;

import java.util.List;

import com.fincore.JournalService.Models.enums.RequestStatus;

import jakarta.validation.constraints.NotNull;
import jakarta.validation.constraints.Size;
import lombok.Data;

@Data
public class BulkProcessJournalRequestDto {
     
    private String batchId;
    
    private List<String> journalIdPrefixes;

    @NotNull
    private RequestStatus status; 
    
    @Size(max = 50)
    private String remarks;
}










package com.fincore.JournalService.Dto;

import lombok.Data;
import java.io.Serializable;

@Data
public class BulkUploadStateDto implements Serializable {
    private String requestId;
    private String status; // PROCESSING, SUCCESS, ERROR
    private Integer currentStage; // 1..4
    private Integer totalRows;
    private Long errorCount;
    private String message;
    private String previewDataJson;

    private boolean hasErrorFile;
    private boolean hasSuccessFile;

    private transient String errorFilePath;
    private transient String successFilePath;
}













package com.fincore.JournalService.Dto;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;


@Data
@NoArgsConstructor
@AllArgsConstructor
public class CglDto {
    private String cglNumber;
    private String description;
}






package com.fincore.JournalService.Dto;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;
import java.math.BigDecimal;
import java.time.LocalDate;

@Data
@AllArgsConstructor
@NoArgsConstructor
public class HdfsSyncDto {
    private String branch;
    private String currency;
    private String cgl;
    private LocalDate balanceDate;
    private BigDecimal newBalance;
    private BigDecimal newInrBalance;
}






package com.fincore.JournalService.Dto;

import com.fasterxml.jackson.annotation.JsonIgnoreProperties;
import com.fasterxml.jackson.annotation.JsonProperty;
import lombok.Data;
import lombok.Getter;
import lombok.Setter;
import jakarta.validation.constraints.NotNull;
import jakarta.validation.constraints.Size;
import java.math.BigDecimal;
import java.time.LocalDate;

import com.fincore.JournalService.Models.enums.ChangeType;

@Data
@Getter
@Setter
@JsonIgnoreProperties(ignoreUnknown = true)
public class CreateJournalRequestDto {

    @NotNull
    private ChangeType changeType;

    private Long masterJournalId;

    @NotNull
    @JsonProperty("pDate")
    private LocalDate pDate;

    @NotNull @Size(min = 1, max = 50)
    private String branch;

    @NotNull @Size(min = 3, max = 3)
    private String currency;

    @NotNull @Size(min = 1, max = 50)
    private String cgl;

    @NotNull
    private BigDecimal amount;

    @Size(max = 20)
    private String productType;

    @Size(max = 50)
    private String remarks;

    @Size(min = 1, max = 1)
    private String arFlag;

    @Size(min = 1, max = 1)
    private String acClassification;

    @NotNull @Size(min = 1, max = 50)
    private String batchId;

    @NotNull @Size(min = 1, max = 50)
    private String journalId;

    @Size(max = 50)
    private String commonBatchRemarks;

    private Integer transactionCount;

    private String transactionType;
}














package com.fincore.JournalService.Dto;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;

import java.math.BigDecimal;
import java.time.LocalDate;

@Data
@AllArgsConstructor
@NoArgsConstructor
public class GlAggregatedDataDto {
    private Long id; // Add this field
    private String branch;
    private String currency;
    private String cgl;
    private LocalDate txnDate;
    private BigDecimal rawAmount;
    private BigDecimal convertedAmount;

    // Existing Constructor used in map (id will be null initially)
    public GlAggregatedDataDto(String branch, String currency, String cgl, LocalDate txnDate, BigDecimal rawAmount, BigDecimal convertedAmount) {
        this.branch = branch;
        this.currency = currency;
        this.cgl = cgl;
        this.txnDate = txnDate;
        this.rawAmount = rawAmount;
        this.convertedAmount = convertedAmount;
    }
}












package com.fincore.JournalService.Dto;

import lombok.Data;
import java.time.LocalDateTime;

import com.fincore.JournalService.Models.JournalRequest;
import com.fincore.JournalService.Models.enums.RequestStatus;

@Data
public class JournalRequestStatusDto {

    private RequestStatus requestStatus;
    private LocalDateTime requestDate;
    private String creatorId;
    private String executorId;
    private String executorRemarks;
    private String batchId;
    private String journalId;

    // A helper constructor 
    public JournalRequestStatusDto(JournalRequest request) {
        this.requestStatus = request.getRequestStatus();
        this.requestDate = request.getRequestDate();
        this.creatorId = request.getCreatorId();
        this.executorId = request.getExecutorId();
        this.executorRemarks = request.getExecutorRemarks();
        this.batchId = request.getBatchId();
        this.journalId = request.getJournalId();
    }
}














package com.fincore.JournalService.Dto;

import lombok.AllArgsConstructor;
import lombok.Data;
import lombok.NoArgsConstructor;

@Data
@AllArgsConstructor
@NoArgsConstructor
public class NotificationConfigDto {
    private String targetUrl;
    private String targetRoles; // Comma separated string: "51,52,55"
}











package com.fincore.JournalService.Dto;

import com.fincore.JournalService.Models.enums.RequestStatus;

import jakarta.persistence.Column;
import jakarta.validation.constraints.Size;
import lombok.Data;
import lombok.Getter;
import lombok.Setter;

@Data
@Getter
@Setter
public class ProcessJournalRequestDto {
	@Column(nullable = false)
    private Long requestId;
    
    @Column(nullable = false)
    private RequestStatus status; 
    
    @Size(max = 50)
    private String remarks;

}






********************************************************************************************




package com.fincore.JournalService.Exception;


import org.springframework.http.HttpStatus;
import org.springframework.web.bind.annotation.ResponseStatus;

@ResponseStatus(HttpStatus.NOT_FOUND)
public class ResourceNotFoundException extends RuntimeException {
    public ResourceNotFoundException(String message) {
        super(message);
    }
}




********************************************************************************************

models:



package com.fincore.JournalService.Models.enums;

import lombok.Getter;

import java.util.Arrays;
import java.util.Map;
import java.util.stream.Collectors;

@Getter
public enum AcClassification {
	  ASSET('A', "Asset"),
	    LIABILITY('L', "Liability"),
	    INCOME('I', "Income"),
	    EXPENSE('E', "Expense"),
	    MEMO('M', "Memo A/c");

	    private final char code;
	    private final String description;

	    AcClassification(char code, String description) {
	        this.code = code;
	        this.description = description;
	    }

	  
	    public static Map<String, String> getClassificationMap() {
	        return Arrays.stream(AcClassification.values())
	                .collect(Collectors.toMap(
	                        ac -> String.valueOf(ac.getCode()), 
	                        AcClassification::getDescription
	                ));
	    }
}







package com.fincore.JournalService.Models.enums;

import lombok.Getter;

@Getter
public enum ChangeType {
	    ADD("A"),
	    UPDATE("U"),
	    DELETE("D");

	    private final String code;

	    ChangeType(String code) {
	        this.code = code;
	    }
}





package com.fincore.JournalService.Models.enums;


import lombok.Getter;

@Getter
public enum RequestStatus {
    PENDING("P"),
    ACCEPTED("A"),
    REJECTED("R");

    private final String code;

    RequestStatus(String code) {
        this.code = code;
    }
}





package com.fincore.JournalService.Models;

import jakarta.persistence.Column;
import jakarta.persistence.Entity;
import jakarta.persistence.Id;
import jakarta.persistence.Table;
import lombok.Data;
import java.time.LocalDate;


@Entity
@Table(name = "BRANCH_MASTER")
@Data
public class BranchMaster {

    @Id
    @Column(name = "CODE" )
    private String code;

    @Column(name = "NAME")
    private String name;

    @Column(name = "CIRCLE_CODE")
    private String circleCode;

    @Column(name = "STATE")
    private String state;

    @Column(name = "CITY")
    private String city;

    @Column(name = "ADDRESS")
    private String address;

    @Column(name = "PINCODE")
    private String pincode;

    @Column(name = "PHONE_NUMBER")
    private String phoneNumber;

    @Column(name = "EMAIL_ID")
    private String emailId;

    @Column(name = "NMR_CODE" )
    private String nmrCode;

    @Column(name = "STATUS")
    private Integer status;

    @Column(name = "OPEN_DATE")
    private LocalDate openDate;

    @Column(name = "CLOSE_DATE")
    private LocalDate closeDate;

    @Column(name = "MERGE_DATE")
    private LocalDate mergeDate;

    @Column(name = "MERGED_WITH_BRANCH" )
    private String mergedWithBranch;

    @Column(name = "LAST_CHANGE_DATE")
    private LocalDate lastChangeDate;

    @Column(name = "CPC_FLAG")
    private Boolean cpcFlag;

    @Column(name = "FOOD_CREDIT_FLAG")
    private Boolean foodCreditFlag;

    @Column(name = "CURR_CHEST_FLAG")
    private Boolean currChestFlag;

    @Column(name = "BRANCH_TYPE")
    private String branchType;
}












package com.fincore.JournalService.Models;

import jakarta.persistence.Column;
import jakarta.persistence.Entity;
import jakarta.persistence.Id;
import jakarta.persistence.Table;
import lombok.Data;
import java.time.LocalDate;


@Entity
@Table(name = "CGL_MASTER")
@Data
public class CglMaster {

    @Id
    @Column(name = "CGL_NUMBER", length = 10, updatable = false, insertable = false)
    private String cglNumber;

    @Column(name = "COMP_1", length = 4, nullable = false)
    private String comp1;

    @Column(name = "SEGMENT_CODE", length = 4, nullable = false)
    private String segmentCode;

    @Column(name = "COMP_2", length = 2, nullable = false)
    private String comp2;

    @Column(name = "DESCRIPTION", length = 100, nullable = false)
    private String description;

    @Column(name = "AC_CLASSIFICATION", length = 1, nullable = false)
    private String acClassification;

    @Column(name = "BAL_FWD", nullable = false)
    private Boolean balFwd = false;

    @Column(name = "DEF_BAL_TYPE", length = 1, nullable = false)
    private String defBalType;

    @Column(name = "STATUS", nullable = false)
    private Boolean status = true;

    @Column(name = "OPEN_DATE", nullable = false)
    private LocalDate openDate;

    @Column(name = "CLOSE_DATE")
    private LocalDate closeDate;

    @Column(name = "BAL_COMPARE", nullable = false)
    private Boolean balCompare = true;

    @Column(name = "MANUAL_POSTING", nullable = false)
    private Boolean manualPosting = true;
}












package com.fincore.JournalService.Models;

import jakarta.persistence.Column;
import jakarta.persistence.Entity;
import jakarta.persistence.Id;
import jakarta.persistence.Table;
import lombok.Data;

@Entity
@Table(name = "CURRENCY_MASTER")
@Data
public class CurrencyMaster {
    @Id
    @Column(name = "CURRENCY_CODE", length = 3)
    private String currencyCode;

    @Column(name = "CURRENCY_NAME", length = 50)
    private String currencyName;
    
    // 1 = Active, 0 = Inactive
    @Column(name = "FLAG")
    private Integer flag; 
}









package com.fincore.JournalService.Models;
import jakarta.persistence.Column;
import jakarta.persistence.Entity;
import jakarta.persistence.Id;
import jakarta.persistence.Table;
import lombok.Data;
import java.time.LocalDate;

@Entity
@Table(name = "FINCORE_DATE")
@Data
public class FincoreDate {

    @Id
    @Column(name = "USERS_DATE")
    private LocalDate usersDate;

    @Column(name = "ETL_DATE")
    private LocalDate etlDate;
}









package com.fincore.JournalService.Models;

import jakarta.persistence.*;
import lombok.Data;
import java.math.BigDecimal;
import java.time.LocalDate;

@Entity
@Table(name = "GL_BALANCE")
@Data
public class GlBalance {

    @Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "GL_BALANCE_SEQ")
    @SequenceGenerator(name = "GL_BALANCE_SEQ", sequenceName = "GL_BALANCE_SEQ", allocationSize = 1)
    @Column(name = "ID")
    private Long id;

    @Column(name = "BALANCE_DATE")
    private LocalDate balanceDate;

    @Column(name = "BRANCH_CODE", length = 5, nullable = false)
    private String branchCode;

    @Column(name = "CURRENCY", length = 3, nullable = false)
    private String currency;

    @Column(name = "CGL", length = 10, nullable = false)
    private String cgl;

    @Column(name = "BALANCE", precision = 25, scale = 4)
    private BigDecimal balance;
}











package com.fincore.JournalService.Models;

import jakarta.persistence.*;
import lombok.Data;
import java.math.BigDecimal;
import java.time.LocalDate;
import java.time.LocalDateTime;

@Entity
@Table(name = "GL_TRANSACTIONS")
@Data

public class GlTransaction {

    @Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "GL_TRANSACTIONS_SEQ")
    @SequenceGenerator(name = "GL_TRANSACTIONS_SEQ", sequenceName = "GL_TRANSACTIONS_SEQ", allocationSize = 1)
    @Column(name = "TRANSACTION_ID")
    private Long transactionId;

    @Column(name = "BATCH_ID", length = 50) 
    private String batchId;

    @Column(name = "JOURNAL_ID", length = 50)
    private String journalId;
   
    @Column(name = "TRANSACTION_DATE")
    @Temporal(TemporalType.DATE)
    private LocalDate transactionDate;

    @Column(name = "POST_DATE")
    @Temporal(TemporalType.TIMESTAMP)
    private LocalDateTime postDate;

    @Column(name = "BRANCH_CODE", length = 50)
    private String branchCode;

    @Column(name = "CURRENCY", length = 3)
    private String currency;

    @Column(name = "CGL", length = 10)
    private String cgl;

    @Column(name = "NARRATION", length = 40)
    private String narration;

    @Column(name = "DEBIT_AMOUNT", precision = 25, scale = 4)
    private BigDecimal debitAmount;

    @Column(name = "CREDIT_AMOUNT", precision = 25, scale = 4)
    private BigDecimal creditAmount;

    @Column(name = "TRANSACTION_COUNT")
    private Integer transactionCount;

    @Column(name = "SOURCE_FLAG", length = 1)
    private String sourceFlag;
}







package com.fincore.JournalService.Models;

import jakarta.persistence.*;
import lombok.Data;
import java.time.LocalDateTime;

@Entity
@Table(name = "AUDIT_LOG")
@Data
public class JournalLog {


    
	@Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "AUDIT_LOG_SEQ")
    @SequenceGenerator(
        name = "AUDIT_LOG_SEQ", 
        sequenceName = "AUDIT_LOG_SEQ", 
        allocationSize = 1
    )
    @Column(name = "LOG_ID")
    private Long id;

    @Column(name = "ACTION_TIME")
    private LocalDateTime actionTime;

    @Column(name = "ACTION_TYPE", length = 255)
    private String actionType; // e.g., CREATE, APPROVE, REJECT

    @Column(name = "CHANGE_TYPE", length = 255)
    private String changeType; // e.g., BATCH_UPLOAD, STATUS_CHANGE

    @Column(name = "IP_ADDRESS", length = 255)
    private String ipAddress;

    @Column(name = "NEW_VALUE", length = 4000) // Adjusted length for safety
    private String newValue;

    @Column(name = "OLD_VALUE", length = 4000)
    private String oldValue;

    @Column(name = "REQUEST_ID")
    private Long requestId; // Links to specific Journal Request ID (PK)

    @Column(name = "USER_ID", length = 255)
    private String userId;
}








package com.fincore.JournalService.Models;

import jakarta.persistence.*;
import lombok.Data;
import java.math.BigDecimal;
import java.time.LocalDate;
import java.time.LocalDateTime;

import com.fincore.JournalService.Models.enums.ChangeType;
import com.fincore.JournalService.Models.enums.RequestStatus;
import com.fincore.JournalService.config.RequestStatusConverter;

@Entity
@Table(name = "JOURNAL_REQUEST")
@Data
public class JournalRequest {
    @Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "JOURNAL_REQUEST_SEQ")
    @SequenceGenerator(name = "JOURNAL_REQUEST_SEQ", sequenceName = "JOURNAL_REQUEST_SEQ", allocationSize = 1)
    @Column(name = "REQ_ID")
    private Long id;

    @Convert(converter = RequestStatusConverter.class)
    @Column(name = "REQ_STATUS", length = 10)
    private RequestStatus requestStatus;

    @Enumerated(EnumType.STRING)
    @Column(name = "CHANGE_TYPE", length = 10)
    private ChangeType changeType;

    @Column(name = "REQ_DATE", updatable = false)
    private LocalDateTime requestDate = LocalDateTime.now();

    @Column(name = "CREATOR_ID", length = 12, updatable = false)
    private String creatorId;

    @Column(name = "CREATOR_ROLE", updatable = false)
    private Integer creatorRole;

    @Column(name = "EXECUTOR_ID", length = 12)
    private String executorId;

    @Column(name = "EXECUTION_DATE")
    private LocalDateTime executionDate;

    @Column(name = "EXECUTOR_REMARKS", length = 50)
    private String executorRemarks;

    // Legacy JSON Payload (Kept for Frontend Display)
    @Lob
    @Column(name = "PAYLOAD")
    private String payload;

    @Column(name = "BATCH_ID", length = 50)
    private String batchId;

    @Column(name = "JOURNAL_ID", length = 50)
    private String journalId;

    @Column(name = "COMMON_BATCH_REMARKS" , length = 50)
    private String commonBatchRemarks;

    // --- NEW OPTIMIZED COLUMNS ---
    @Column(name = "REQ_BRANCH_CODE", length = 50)
    private String branchCode;

    @Column(name = "REQ_CURRENCY", length = 3)
    private String currency;

    @Column(name = "REQ_CGL", length = 50)
    private String cgl;

    @Column(name = "REQ_AMOUNT", precision = 25, scale = 4)
    private BigDecimal amount;

    @Column(name = "REQ_CSV_DATE")
    private LocalDate csvDate;

    @Column(name = "REQ_NARRATION", length = 200)
    private String narration;

    @Column(name = "REQ_PRODUCT", length = 50)
    private String productCode;
}













package com.fincore.JournalService.Models;

import java.math.BigDecimal;
import java.time.LocalDate;
import jakarta.persistence.*;
import lombok.Data;

@Entity
@Table(name = "MASTER_JOURNAL")
@Data
public class MasterJournal {
      @Id
      @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "MASTER_JOURNAL_SEQ")
      @SequenceGenerator(name = "MASTER_JOURNAL_SEQ", sequenceName = "MASTER_JOURNAL_SEQ", allocationSize = 1)
      @Column(name = "JOURNAL_ID")
      private Long id;

      @Column(name = "JOURNAL_PDATE")
      private LocalDate pDate;

      @Column(name = "JOURNAL_BRANCH", length = 50)
      private String branch;

      @Column(name = "CURRENCY", length = 3)
      private String currency;

      @Column(name = "JOURNAL_CGL", length = 50)
      private String cgl;

      @Column(name = "JOURNAL_AMT")
      private BigDecimal amount;

      @Column(name = "TXN_TYPE", length = 20)
      private String transactionType;

      @Column(name = "PRODUCT_TYPE", length = 20)
      private String productType;

      @Column(name = "REMARKS", length = 50)
      private String remarks;

      @Column(name = "AR_FLAG", length = 1)
      private String arFlag;
      
      @Column(name= "AC_CLASSIFICATION")
      private String acClassification;

      @Column(name = "BATCH_ID", length = 50)
      private String batchId;

      @Column(name = "JOURNAL_ID_REF", length = 50) // Renamed to avoid conflict with PK if needed, or just JOURNAL_ID
      private String journalId;
      
      @Column(name = "COMMON_BATCH_REMARKS" , length = 50)
      private String commonBatchRemarks;
}










package com.fincore.JournalService.Models;

import jakarta.persistence.*;
import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import org.hibernate.annotations.CreationTimestamp;
import org.springframework.data.jpa.domain.support.AuditingEntityListener;

import java.io.Serializable;
import java.time.Instant;
import java.util.UUID;

@Entity
@Table(name = "NOTIFICATIONS")
@Builder
@Data
@NoArgsConstructor
@AllArgsConstructor
@EntityListeners(AuditingEntityListener.class)
public class Notifications implements Serializable {

    @Id
    @GeneratedValue(strategy = GenerationType.UUID)
    @Column(name = "EVENT_ID", nullable = false, updatable = false)
    private UUID eventId; // Maps to RAW(16)

    @Column(name = "USER_ID", length = 255)
    private String userId;

    @Column(name = "MESSAGE", length = 1024, nullable = false)
    private String message;

    @Column(name = "LINK_URL", length = 1024)
    private String linkUrl;

    @Column(name = "EVENT_SOURCE", length = 100)
    private String eventSource;

    @Column(name = "AGGREGATE_ID", length = 255)
    private String aggregateId;

    @CreationTimestamp
    @Column(name = "EVENT_TIMESTAMP", nullable = false, updatable = false)
    private Instant eventTimestamp; // Maps to TIMESTAMP(6) WITH TIME ZONE

    @Column(name = "TARGET_ROLE", length = 100)
    private String targetRole;
}

















package com.fincore.JournalService.Models;

import jakarta.persistence.*;
import lombok.*;

@Entity
@Table(name = "MENU_ITEMS") // Mapped to your existing table
@Getter
@Setter
@NoArgsConstructor
@AllArgsConstructor
@ToString
public class Permissions {

    @Id
    @GeneratedValue(strategy = GenerationType.SEQUENCE, generator = "menu_item_seq_gen")
    @SequenceGenerator(name = "menu_item_seq_gen", sequenceName = "FINCORE.MENU_ITEMS_SEQ", allocationSize = 1)
    @Column(name = "MENU_ID", nullable = false)
    private int menuId;

    @Column(name = "MENU_TITLE", length = 100)
    private String menuTitle;

    @Column(name = "MENU_ICON", length = 100)
    private String menuIcon;

    @Column(name = "MENU_SUBMENU", length = 100)
    private String menuSubmenu;

    @Column(name = "MENU_ACTION", length = 200)
    private String menuAction;

    @Column(name = "MENU_URL", length = 200)
    private String menuUrl;

    @Column(name = "MENU_COMPONENT_PATH", length = 200)
    private String menuComponentPath;

    @Column(name = "MENU_DESCRIPTION", length = 255)
    private String menuDescription;

    @Column(name = "MENU_DEPENDANT")
    private Integer menuDependant;

    // This is the key field used for Journal Authorization logic
    @Column(name = "MAPPED_REQUEST_TYPE", length = 50)
    private String mappedRequestType; 
}












******************************************************************************

repository:

package com.fincore.JournalService.Repository;

import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import com.fincore.JournalService.Models.BranchMaster;

import java.util.List;

@Repository
public interface BranchMasterRepository extends JpaRepository<BranchMaster, String> {

    List<BranchMaster> findByNameContainingIgnoreCaseOrCodeContainingIgnoreCase(String name, String code, Pageable pageable);
}





package com.fincore.JournalService.Repository;

import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import com.fincore.JournalService.Models.CglMaster;

import java.util.List;

@Repository
public interface CglMasterRepository extends JpaRepository<CglMaster, String> {

    List<CglMaster> findByCglNumberContainingOrDescriptionContainingIgnoreCase(String cglNumber, String description, Pageable pageable);
}







package com.fincore.JournalService.Repository;

import com.fincore.JournalService.Models.CurrencyMaster;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface CurrencyMasterRepository extends JpaRepository<CurrencyMaster, String> {
}










package com.fincore.JournalService.Repository;


import com.fincore.JournalService.Models.FincoreDate;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;
import org.springframework.stereotype.Repository;

import java.time.LocalDate;

@Repository
public interface FincoreDateRepository extends JpaRepository<FincoreDate, LocalDate> {

    @Query(value = "SELECT USERS_DATE FROM FINCORE_DATE FETCH FIRST 1 ROWS ONLY", nativeQuery = true)
    LocalDate findCurrentPostingDate();
}










package com.fincore.JournalService.Repository;

import com.fincore.JournalService.Models.GlBalance;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

@Repository
public interface GlBalanceRepository extends JpaRepository<GlBalance, Long> {
}











package com.fincore.JournalService.Repository;

import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import com.fincore.JournalService.Models.GlTransaction;

@Repository
public interface GlTransactionRepository extends JpaRepository<GlTransaction, Long> {
}









package com.fincore.JournalService.Repository;

import com.fincore.JournalService.Models.JournalLog;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;


@Repository
public interface JournalLogRepository extends JpaRepository<JournalLog, Long> {
}







package com.fincore.JournalService.Repository;

import java.util.List;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Modifying;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.jpa.repository.JpaSpecificationExecutor;
import org.springframework.stereotype.Repository;
import org.springframework.data.repository.query.Param;

import com.fincore.JournalService.Models.JournalRequest;
import com.fincore.JournalService.Models.enums.RequestStatus;

@Repository
public interface JournalRequestRepository extends JpaRepository<JournalRequest, Long>, JpaSpecificationExecutor<JournalRequest> {

    // --- 1. APPROVAL SCREEN (Pending Batches) ---
    // Logic: Debit = Amount > 0. Credit = Amount < 0 (Taken as Absolute for display)
    @Query(value = """
        SELECT 
            BATCH_ID, 
            MAX(CREATOR_ID) as CREATOR, 
            MAX(REQ_DATE) as RDATE, 
            MAX(COMMON_BATCH_REMARKS) as REM,
            COUNT(*) as CNT,
            SUM(CASE WHEN TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount')) > 0 THEN TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount')) ELSE 0 END) as TOT_DR,
            SUM(CASE WHEN TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount')) < 0 THEN ABS(TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount'))) ELSE 0 END) as TOT_CR
        FROM JOURNAL_REQUEST 
        WHERE TRIM(REQ_STATUS) IN ('P', 'PENDING')
        GROUP BY BATCH_ID
        ORDER BY MAX(REQ_DATE) DESC
    """, nativeQuery = true)
    List<Object[]> findPendingBatchSummariesNative();

    // --- 2. STATUS SCREEN (All History) ---
    @Query(value = """
        SELECT 
            BATCH_ID, 
            MAX(CREATOR_ID) as CREATOR, 
            MAX(REQ_DATE) as RDATE, 
            MAX(COMMON_BATCH_REMARKS) as REM,
            COUNT(*) as CNT,
            SUM(CASE WHEN TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount')) > 0 THEN TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount')) ELSE 0 END) as TOT_DR,
            SUM(CASE WHEN TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount')) < 0 THEN ABS(TO_NUMBER(JSON_VALUE(PAYLOAD, '$.amount'))) ELSE 0 END) as TOT_CR,
            MAX(REQ_STATUS) as STATUS,
            MAX(EXECUTOR_ID) as EXEC_ID,
            MAX(EXECUTOR_REMARKS) as EXEC_REM
        FROM JOURNAL_REQUEST 
        GROUP BY BATCH_ID
        ORDER BY MAX(REQ_DATE) DESC
    """, nativeQuery = true)
    List<Object[]> findAllBatchSummariesNative();

    // --- 3. PAGINATED DETAIL FETCH ---
    @Query(value = "SELECT * FROM JOURNAL_REQUEST WHERE BATCH_ID = :batchId ORDER BY JOURNAL_ID ASC",
            countQuery = "SELECT COUNT(*) FROM JOURNAL_REQUEST WHERE BATCH_ID = :batchId",
            nativeQuery = true)
    Page<JournalRequest> findByBatchIdPaginated(@Param("batchId") String batchId, Pageable pageable);

    // --- 4. FAST DELETE ---
    @Modifying
    @Query(value = "DELETE FROM JOURNAL_REQUEST WHERE BATCH_ID = :batchId AND UPPER(TRIM(CREATOR_ID)) = UPPER(TRIM(:userId)) AND TRIM(REQ_STATUS) IN ('P', 'PENDING')", nativeQuery = true)
    void deleteBatchNative(@Param("batchId") String batchId, @Param("userId") String userId);

    @Modifying
    @Query(value = "DELETE FROM JOURNAL_REQUEST WHERE JOURNAL_ID IN (:journalIds) AND UPPER(TRIM(CREATOR_ID)) = UPPER(TRIM(:userId)) AND TRIM(REQ_STATUS) IN ('P', 'PENDING')", nativeQuery = true)
    void deleteJournalsNative(@Param("journalIds") List<String> journalIds, @Param("userId") String userId);

    // --- 5. STANDARD METHODS ---
    @Query(value = "SELECT * FROM JOURNAL_REQUEST WHERE TRIM(REQ_STATUS) IN ('P', 'PENDING')", nativeQuery = true)
    List<JournalRequest> findAllPendingNative();

    @Query(value = "SELECT * FROM JOURNAL_REQUEST WHERE UPPER(TRIM(CREATOR_ID)) = UPPER(TRIM(:creatorId))", nativeQuery = true)
    List<JournalRequest> findAllByCreatorIdNative(@Param("creatorId") String creatorId);

    // --- 6. RESTORED MISSING METHODS ---
    List<JournalRequest> findByBatchId(String batchId);
    List<JournalRequest> findByCreatorId(String creatorId);
    List<JournalRequest> findByRequestStatus(RequestStatus status);

    // This was the specific one causing your error:
    List<JournalRequest> findByJournalIdStartingWithAndCreatorIdAndRequestStatus(String journalIdPrefix, String creatorId, RequestStatus status);

    // Keeping this just in case:
    List<JournalRequest> findByJournalIdStartingWithAndRequestStatus(String journalIdPrefix, RequestStatus status);
}








package com.fincore.JournalService.Repository;

import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;

import com.fincore.JournalService.Models.MasterJournal;

@Repository
public interface MasterJournalRepository extends JpaRepository<MasterJournal, Long>{

}











package com.fincore.JournalService.Repository;

import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.stereotype.Repository;
import com.fincore.JournalService.Models.Notifications;
import java.util.UUID;

@Repository
public interface NotificationRepository extends JpaRepository<Notifications, UUID> {
}





package com.fincore.JournalService.Repository;

import org.springframework.data.jpa.repository.JpaRepository;
import org.springframework.data.jpa.repository.Query;
import org.springframework.data.repository.query.Param;
import org.springframework.stereotype.Repository;

import com.fincore.JournalService.Models.Permissions;

import java.util.List;

@Repository
public interface PermissionRepository extends JpaRepository<Permissions, Integer> {

    @Query(value = """
       SELECT p.MENU_URL, rp.ROLE_ID
       FROM PERMISSIONS p
       JOIN ROLE_PERMISSIONS rp ON p.MENU_ID = rp.PERMISSION_ID
       JOIN ROLES r ON rp.ROLE_ID = r.ROLE_ID
       WHERE p.MAPPED_REQUEST_TYPE = :requestType
         AND (p.MENU_ACTION LIKE '%approve%' OR p.MENU_ACTION LIKE '%reject%')
         AND r.ROLE_STATUS = 'ACTIVE'
   """, nativeQuery = true)
    List<Object[]> findUrlAndRolesByRequestType(@Param("requestType") String requestType);
}














***********************************************************************************************************

service :



package com.fincore.JournalService.Service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.stereotype.Service;

import com.fincore.JournalService.Dto.BranchDto;
import com.fincore.JournalService.Models.BranchMaster;
import com.fincore.JournalService.Repository.BranchMasterRepository;

import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

@Service
@RequiredArgsConstructor
@Slf4j
public class BranchService {

    private final BranchMasterRepository branchMasterRepository;

    public Map<String, String> getBranchCodeAndNameMap() {
        // This method is likely cached or used rarely, leaving as is
        List<BranchMaster> branches = branchMasterRepository.findAll();
        return branches.stream()
                .collect(Collectors.toMap(BranchMaster::getCode, BranchMaster::getName));
    }

    public List<BranchDto> searchBranches(String query) {
        log.info("Searching branches for: '{}'", query);

        Pageable limit = PageRequest.of(0, 50);


        List<BranchMaster> results = branchMasterRepository
                .findByNameContainingIgnoreCaseOrCodeContainingIgnoreCase(query, query, limit);

        return results.stream()
                .map(branch -> new BranchDto(branch.getCode(), branch.getName()))
                .collect(Collectors.toList());
    }
}







package com.fincore.JournalService.Service;

import com.fincore.JournalService.Dto.HdfsSyncDto;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.jdbc.core.BatchPreparedStatementSetter;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;

import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.List;

@Service
@RequiredArgsConstructor
@Slf4j
public class HdfsSyncService {

    @Autowired
    @Qualifier("hiveJdbcTemplate")
    private JdbcTemplate hiveJdbcTemplate;

    @Async("bulkExecutor") // Runs in background thread, UI returns immediately
    public void syncToDataLake(List<HdfsSyncDto> data) {
        if (data == null || data.isEmpty()) return;

        log.info("HDFS SYNC: Starting background update for {} aggregated records...", data.size());
        long start = System.currentTimeMillis();

        try {
            // Updating the Data Lake.
            // Note: Ensure your Hive/Thrift table supports updates.
            // If not (e.g. raw HDFS), change this to an INSERT statement.
            String sql = "UPDATE GL_BALANCE_HDFS SET BALANCE = ?, INR_BALANCE = ? " +
                    "WHERE BRANCH_CODE = ? AND CURRENCY = ? AND CGL = ? AND BALANCE_DATE = ?";

            hiveJdbcTemplate.batchUpdate(sql, new BatchPreparedStatementSetter() {
                @Override
                public void setValues(PreparedStatement ps, int i) throws SQLException {
                    HdfsSyncDto dto = data.get(i);
                    ps.setBigDecimal(1, dto.getNewBalance());
                    ps.setBigDecimal(2, dto.getNewInrBalance());
                    ps.setString(3, dto.getBranch());
                    ps.setString(4, dto.getCurrency());
                    ps.setString(5, dto.getCgl());
                    ps.setDate(6, java.sql.Date.valueOf(dto.getBalanceDate()));
                }

                @Override
                public int getBatchSize() {
                    return data.size();
                }
            });

            log.info("HDFS SYNC: Completed successfully in {} ms.", System.currentTimeMillis() - start);

        } catch (Exception e) {
            log.error("HDFS SYNC FAILED: {}. Data might be out of sync.", e.getMessage());
        }
    }
}










package com.fincore.JournalService.Service;

import com.fincore.JournalService.Dto.HdfsSyncDto;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;

import java.util.List;

@Service
@RequiredArgsConstructor
@Slf4j
public class HdfsRecoveryService {

    private final HdfsSyncService hdfsSyncService;

    @Autowired
    @Qualifier("oracleJdbcTemplate")
    private JdbcTemplate oracleJdbcTemplate;

    // Run every 5 minutes (300,000 ms)
    @Scheduled(fixedDelay = 300000)
    public void retryFailedHdfsSyncs() {
        // 1. Fetch PENDING batches
        String fetchSql = "SELECT BATCH_ID FROM HDFS_SYNC_RETRY_QUEUE WHERE STATUS = 'PENDING' AND RETRY_COUNT < 5";
        List<String> failedBatches = oracleJdbcTemplate.query(fetchSql, (rs, rowNum) -> rs.getString("BATCH_ID"));

        if (failedBatches.isEmpty()) return;  // 99% of the time, code stops here. Instant return.

        log.info("HDFS RECOVERY: Found {} failed batches. Starting self-healing...", failedBatches.size());

        for (String batchId : failedBatches) {
            try {
                processRetry(batchId);
            } catch (Exception e) {
                log.error("HDFS RECOVERY: Failed to recover batch {}", batchId, e);
                // Increment retry count
                oracleJdbcTemplate.update("UPDATE HDFS_SYNC_RETRY_QUEUE SET RETRY_COUNT = RETRY_COUNT + 1 WHERE BATCH_ID = ?", batchId);
            }
        }
    }

    private void processRetry(String batchId) {
        // 2. SELF-HEALING QUERY
        // Instead of using old data, we join JOURNAL_REQUEST with GL_BALANCE
        // to get the CURRENT REAL-TIME balance from Oracle.
        // This fixes the "Race Condition" issue.
        String freshDataSql = """
            SELECT 
                j.REQ_BRANCH_CODE AS BRANCH, 
                j.REQ_CURRENCY AS CURRENCY, 
                j.REQ_CGL AS CGL, 
                j.REQ_CSV_DATE AS BAL_DATE,
                g.BALANCE AS NEW_BALANCE,
                g.INR_BALANCE AS NEW_INR_BALANCE
            FROM (
                SELECT DISTINCT REQ_BRANCH_CODE, REQ_CURRENCY, REQ_CGL, REQ_CSV_DATE 
                FROM JOURNAL_REQUEST 
                WHERE BATCH_ID = ?
            ) j
            JOIN GL_BALANCE g ON 
                g.BRANCH_CODE = j.REQ_BRANCH_CODE AND 
                g.CURRENCY = j.REQ_CURRENCY AND 
                g.CGL = j.REQ_CGL AND 
                g.BALANCE_DATE = j.REQ_CSV_DATE
        """;

        List<HdfsSyncDto> freshSyncData = oracleJdbcTemplate.query(freshDataSql,
                (rs, rowNum) -> new HdfsSyncDto(
                        rs.getString("BRANCH"),
                        rs.getString("CURRENCY"),
                        rs.getString("CGL"),
                        rs.getDate("BAL_DATE").toLocalDate(),
                        rs.getBigDecimal("NEW_BALANCE"),
                        rs.getBigDecimal("NEW_INR_BALANCE")
                ), batchId);

        if (!freshSyncData.isEmpty()) {
            // 3. Push the LATEST/CURRENT balance to HDFS
            hdfsSyncService.syncToDataLake(freshSyncData);
            log.info("HDFS RECOVERY: Successfully synced batch {}. Data is now consistent.", batchId);
        }

        // 4. Mark as Completed
        oracleJdbcTemplate.update("UPDATE HDFS_SYNC_RETRY_QUEUE SET STATUS = 'COMPLETED' WHERE BATCH_ID = ?", batchId);
    }
}













package com.fincore.JournalService.Service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.data.domain.PageRequest;
import org.springframework.data.domain.Pageable;
import org.springframework.stereotype.Service;
import org.springframework.util.StringUtils;

import com.fincore.JournalService.Dto.CglDto;
import com.fincore.JournalService.Models.CglMaster;
import com.fincore.JournalService.Repository.CglMasterRepository;

import java.util.List;
import java.util.stream.Collectors;

@Service
@RequiredArgsConstructor
@Slf4j
public class CglService {

    private final CglMasterRepository cglMasterRepository;

    private CglDto convertToDto(CglMaster cgl) {
        return new CglDto(cgl.getCglNumber(), cgl.getDescription());
    }

    public List<CglDto> searchCgls(String query) {



        if (!StringUtils.hasText(query)) {
            return List.of();
        }

        log.info("Searching CGLs for: '{}'", query);

        Pageable limit = PageRequest.of(0, 50);

        List<CglMaster> results = cglMasterRepository
                .findByCglNumberContainingOrDescriptionContainingIgnoreCase(query, query, limit);

        return results.stream()
                .map(this::convertToDto)
                .collect(Collectors.toList());
    }
}

















//-----

package com.fincore.JournalService.Service;

import com.fincore.JournalService.Dto.BulkUploadStateDto;
import com.fincore.JournalService.Models.BranchMaster;
import com.fincore.JournalService.Models.CglMaster;
import com.fincore.JournalService.Models.CurrencyMaster;
import com.fincore.JournalService.Repository.BranchMasterRepository;
import com.fincore.JournalService.Repository.CglMasterRepository;
import com.fincore.JournalService.Repository.CurrencyMasterRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.poi.ss.usermodel.*;
import org.apache.poi.xssf.streaming.SXSSFWorkbook;
import org.apache.poi.xssf.usermodel.XSSFWorkbook;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.context.annotation.Lazy;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;

import java.io.*;
import java.math.BigDecimal;
import java.math.RoundingMode;
import java.nio.charset.StandardCharsets;
import java.time.LocalDate;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.LongAdder;
import java.util.regex.Pattern;
import java.util.stream.Collectors;

@Service
@RequiredArgsConstructor
@Slf4j
public class JournalBulkValidationService {

    // --- REGEX PATTERNS ---
    private static final Pattern CLEAN_AMOUNT_REGEX = Pattern.compile("[^0-9.]");
    private static final Pattern PRODUCT_CODE_REGEX = Pattern.compile("^\\d{8}$");
    private static final Pattern CGL_FORMAT_REGEX = Pattern.compile("^\\d{10}$");
    private static final DateTimeFormatter SYSTEM_DATE_FMT = DateTimeFormatter.ofPattern("ddMMyyyy");
    private final BranchMasterRepository branchRepo;
    private final CglMasterRepository cglRepo;
    private final CurrencyMasterRepository currencyRepo;
    private final Map<String, BulkUploadStateDto> statusCache = new ConcurrentHashMap<>();
    private final Map<String, List<ExcelRowData>> dataCache = new ConcurrentHashMap<>();
    private final Map<String, byte[]> fileCache = new ConcurrentHashMap<>();
    @Autowired
    @Lazy
    private JournalBulkValidationService self;

    public BulkUploadStateDto getState(String reqId) {
        return statusCache.get(reqId);
    }

    public byte[] getFileBytes(String reqId, String type) {
        return fileCache.get(reqId + "_" + type);
    }

    public List<ExcelRowData> getValidRowsFromCache(String requestId) {
        return dataCache.get(requestId);
    }

    public String initiateValidation(byte[] fileBytes, String filename, LocalDate postingDate) throws IOException {
        String requestId = UUID.randomUUID().toString();
        BulkUploadStateDto state = new BulkUploadStateDto();
        state.setRequestId(requestId);
        state.setStatus("PROCESSING");
        state.setCurrentStage(1);
        state.setMessage("Initializing Upload...");
        state.setTotalRows(0);

        statusCache.put(requestId, state);
        self.processAsync(requestId, fileBytes, filename, postingDate);
        return requestId;
    }

    @Async("bulkExecutor")
    public void processAsync(String requestId, byte[] fileBytes, String filename, LocalDate postingDate) {
        log.info("Starting Async Validation for ReqID: {}", requestId);
        try {
            updateState(requestId, s -> s.setMessage("Parsing File..."));
            List<ExcelRowData> parsedRows;
            boolean isCsv = filename != null && (filename.toLowerCase().endsWith(".csv") || filename.toLowerCase().endsWith(".txt"));

            if (isCsv) parsedRows = parseCsvBytes(fileBytes, postingDate);
            else parsedRows = parseExcelBytes(fileBytes, postingDate);

            updateState(requestId, s -> {
                s.setTotalRows(parsedRows.size());
                s.setMessage("Validating Formats...");
            });

            if (runFormatCheck(parsedRows)) {
                failRequest(requestId, parsedRows, "Format Validation Failed", 1);
                return;
            }

            updateState(requestId, s -> {
                s.setCurrentStage(2);
                s.setMessage("Checking Database...");
            });
            if (runDbCheck(parsedRows)) {
                failRequest(requestId, parsedRows, "Database Validation Failed", 2);
                return;
            }

            updateState(requestId, s -> {
                s.setCurrentStage(3);
                s.setMessage("Checking Balances...");
            });
            if (runBalanceCheck(parsedRows)) {
                failRequest(requestId, parsedRows, "Debit/Credit Balance Mismatch", 3);
                return;
            }

            completeRequest(requestId, parsedRows, postingDate, isCsv);
        } catch (Exception e) {
            log.error("Async Validation Error", e);
            updateState(requestId, s -> {
                s.setStatus("ERROR");
                s.setMessage("System Error: " + e.getMessage());
                s.setHasErrorFile(false);
            });
        }
    }

    private List<ExcelRowData> parseCsvBytes(byte[] bytes, LocalDate postingDate) throws IOException {
        List<ExcelRowData> list = new ArrayList<>();
        try (BufferedReader br = new BufferedReader(new InputStreamReader(new ByteArrayInputStream(bytes), StandardCharsets.UTF_8))) {
            String line;
            int i = 0;
            while ((line = br.readLine()) != null) {
                if (line.trim().isEmpty()) continue;
                String[] c = line.split(",", -1);

                if (i == 0) {
                    String h = c.length > 0 ? c[0].trim().toLowerCase() : "";
                    if ((h.contains("branch") || h.contains("batch")) && !h.equals("01")) {
                        i++;
                        continue;
                    }
                }
                ExcelRowData d = new ExcelRowData();
                d.rowIndex = i++;

                if (c.length > 0) d.sysSite = c[0].trim();
                if (c.length > 1) d.sysDate = c[1].trim();
                if (c.length > 2) d.sysYear = c[2].trim();
                if (c.length > 3) d.sysPeriod = c[3].trim();
                if (c.length > 4) d.branch = c[4].trim();
                if (c.length > 5) d.currency = c[5].trim().toUpperCase();
                if (c.length > 6) d.cgl = c[6].trim();

                String amtRaw = (c.length > 7) ? c[7].trim() : "";
                String txnRaw = (c.length > 8) ? c[8].trim() : "";
                parseAmount(d, amtRaw, txnRaw);

                if (c.length > 9) d.remarks = c[9].trim();
                String rawProd = (c.length > 10) ? c[10].trim() : "";
                d.productCode = rawProd.isEmpty() ? "A" : rawProd;

                if (c.length != 14) {
                    d.formatErrors.add("Invalid CSV Format: Row has " + c.length + " columns. Expected 14.");
                } else {
                    d.isSystemFormat = true;
                    if (d.sysSite.isEmpty()) d.formatErrors.add("Batch ID is Mandatory");
                    if (d.branch.isEmpty()) d.formatErrors.add("Branch is Mandatory");
                    if (d.currency.isEmpty()) d.formatErrors.add("Currency is Mandatory");
                    if (d.cgl.isEmpty()) d.formatErrors.add("CGL is Mandatory");
                    if (amtRaw.isEmpty()) d.formatErrors.add("Amount is Mandatory");
                    validateSystemColumns(d, d.sysSite, d.sysDate, d.sysYear, d.sysPeriod, c[11], c[12], c[13], postingDate);
                }
                list.add(d);
            }
        }
        return list;
    }

    private List<ExcelRowData> parseExcelBytes(byte[] bytes, LocalDate postingDate) throws IOException {
        try (Workbook wb = new XSSFWorkbook(new ByteArrayInputStream(bytes))) {
            return parseSheet(wb.getSheetAt(0), postingDate);
        }
    }

    private List<ExcelRowData> parseSheet(Sheet sheet, LocalDate postingDate) {
        List<ExcelRowData> list = new ArrayList<>();
        DataFormatter fmt = new DataFormatter();
        for (Row r : sheet) {
            if (isRowEmpty(r)) continue;
            if (r.getRowNum() == 0) {
                String h = fmt.formatCellValue(r.getCell(0)).toLowerCase();
                if ((h.contains("branch") || h.contains("batch")) && !h.equals("01")) continue;
            }
            ExcelRowData d = new ExcelRowData();
            d.rowIndex = r.getRowNum();

            String col0 = parseCode(r.getCell(0), fmt);

            if (col0.equals("01")) {
                d.isSystemFormat = true;
                d.sysSite = col0;
                d.sysDate = parseCode(r.getCell(1), fmt);
                d.sysYear = parseCode(r.getCell(2), fmt);
                d.sysPeriod = parseCode(r.getCell(3), fmt);
                d.branch = parseCode(r.getCell(4), fmt);
                d.currency = fmt.formatCellValue(r.getCell(5)).trim().toUpperCase();
                d.cgl = parseCode(r.getCell(6), fmt);
                parseAmount(d, fmt.formatCellValue(r.getCell(7)), fmt.formatCellValue(r.getCell(8)));
                d.remarks = fmt.formatCellValue(r.getCell(9));
                String rawProd = parseCode(r.getCell(10), fmt);
                d.productCode = rawProd.isEmpty() ? "A" : rawProd;

                if (isCellEmpty(r.getCell(0))) d.formatErrors.add("Batch ID is Mandatory");
                validateSystemColumns(d, col0, d.sysDate, d.sysYear, d.sysPeriod,
                        parseCode(r.getCell(11), fmt), parseCode(r.getCell(12), fmt), parseCode(r.getCell(13), fmt), postingDate);
            } else {
                d.branch = col0;
                d.currency = fmt.formatCellValue(r.getCell(1)).trim().toUpperCase();
                d.cgl = parseCode(r.getCell(2), fmt);
                parseAmount(d, fmt.formatCellValue(r.getCell(3)), fmt.formatCellValue(r.getCell(4)));
                d.remarks = fmt.formatCellValue(r.getCell(5));
                String rawProd = parseCode(r.getCell(6), fmt);
                d.productCode = rawProd.isEmpty() ? "A" : rawProd;
            }
            list.add(d);
        }
        return list;
    }

    private boolean runFormatCheck(List<ExcelRowData> rows) {
        rows.parallelStream().forEach(d -> {
            if (d.amount == null || d.amount.compareTo(BigDecimal.ZERO) == 0) {
                d.formatErrors.add("Amount cannot be Zero or Null");
            } else {
                if (d.amount.signum() < 0) d.formatErrors.add("Amount cannot be negative");
                if (d.amount.precision() > 20 || d.amount.scale() > 4)
                    d.formatErrors.add("Amount exceeds format (Max 16.4)");
                if ("INR".equalsIgnoreCase(d.currency)) {
                    if (d.amount.stripTrailingZeros().scale() > 2) {
                        d.formatErrors.add("INR Amount cannot have more than 2 decimal places");
                    }
                }
            }
            if (d.productCode != null && !d.productCode.isEmpty()) {
                if (!d.productCode.equals("A") && !PRODUCT_CODE_REGEX.matcher(d.productCode).matches()) {
                    d.formatErrors.add("Product Code must be 'A' or 8 digits");
                }
            }
            if (d.remarks == null || d.remarks.trim().isEmpty()) d.formatErrors.add("Remarks is Mandatory");
            else if (d.remarks.length() > 30) d.formatErrors.add("Remarks length must be <= 30 chars");

            if (d.currency == null || d.currency.length() != 3)
                d.formatErrors.add("Currency must be exactly 3 characters");
            if (d.cgl == null || d.cgl.isEmpty()) d.formatErrors.add("CGL is Mandatory");
            else if (!CGL_FORMAT_REGEX.matcher(d.cgl).matches()) d.formatErrors.add("CGL must be exactly 10 digits");
            if (d.branch == null || d.branch.trim().isEmpty()) d.formatErrors.add("Branch is Mandatory");
        });
        return rows.stream().anyMatch(ExcelRowData::hasErrors);
    }

    private void validateSystemColumns(ExcelRowData d, String c0, String cDate, String cYear, String cMonth, String c11, String c12, String c13, LocalDate postingDate) {
        if (!"01".equals(c0)) d.formatErrors.add("Batch ID must be '01'");
        LocalDate parsedDate = null;
        if (cDate == null || cDate.trim().length() != 8) d.formatErrors.add("Invalid Date Format (ddMMyyyy)");
        else {
            try {
                parsedDate = LocalDate.parse(cDate.trim(), SYSTEM_DATE_FMT);
                if (!parsedDate.equals(postingDate))
                    d.formatErrors.add("Date Mismatch with Posting Date (" + postingDate + ")");
            } catch (Exception e) {
                d.formatErrors.add("Invalid Calendar Date");
            }
        }
        if (parsedDate != null) {
            String expectedYear = (parsedDate.getMonthValue() >= 4) ? String.valueOf(parsedDate.getYear()) : String.valueOf(parsedDate.getYear() - 1);
            String expectedMonth = String.format("%02d", (parsedDate.getMonthValue() >= 4) ? (parsedDate.getMonthValue() - 3) : (parsedDate.getMonthValue() + 9));
            if (cYear == null || !cYear.trim().equals(expectedYear))
                d.formatErrors.add("Invalid Fin Year. Expected: " + expectedYear);
            if (cMonth == null || !cMonth.trim().equals(expectedMonth))
                d.formatErrors.add("Invalid Fin Month. Expected: " + expectedMonth);
        }
        if (c11 == null || !"B".equalsIgnoreCase(c11.trim())) d.formatErrors.add("Col 12 must be 'B'");
        if (c12 == null || !"C".equalsIgnoreCase(c12.trim())) d.formatErrors.add("Col 13 must be 'C'");
        if (c13 == null || !"D".equalsIgnoreCase(c13.trim())) d.formatErrors.add("Col 14 must be 'D'");
    }

    private void parseAmount(ExcelRowData d, String amountRaw, String typeRaw) {
        try {
            if (amountRaw == null) amountRaw = "";
            if (typeRaw == null) typeRaw = "";
            if (amountRaw.contains("-") || amountRaw.contains("(") || amountRaw.contains(")"))
                d.formatErrors.add("Amount cannot be negative");
            String clean = CLEAN_AMOUNT_REGEX.matcher(amountRaw).replaceAll("");
            if (clean.isEmpty()) {
                d.amount = BigDecimal.ZERO;
                return;
            }
            BigDecimal v = new BigDecimal(clean);
            if (v.signum() < 0) d.formatErrors.add("Amount cannot be negative");
            boolean isCredit = typeRaw.toUpperCase().contains("C") || typeRaw.toUpperCase().contains("CR");
            d.txnType = isCredit ? "Credit" : "Debit";
            d.amount = v; // Store absolute, type determines sign later
        } catch (Exception e) {
            d.amount = BigDecimal.ZERO;
        }
    }

    private boolean runBalanceCheck(List<ExcelRowData> rows) {
        ConcurrentHashMap<String, LongAdder> balanceMap = new ConcurrentHashMap<>();
        rows.parallelStream().forEach(d -> {
            if (d.amount != null) {
                BigDecimal val = d.amount;
                if ("Credit".equals(d.txnType)) val = val.negate();
                String type = (d.cgl != null && d.cgl.startsWith("5")) ? "MEMO" : "NORMAL";
                String key = d.branch + "_" + d.currency + "_" + type;
                balanceMap.computeIfAbsent(key, k -> new LongAdder()).add(val.multiply(new BigDecimal("10000")).longValue());
            }
        });
        rows.parallelStream().forEach(d -> {
            String type = (d.cgl != null && d.cgl.startsWith("5")) ? "MEMO" : "NORMAL";
            String key = d.branch + "_" + d.currency + "_" + type;
            LongAdder adder = balanceMap.get(key);
            if (adder != null && adder.sum() != 0) {
                BigDecimal diff = BigDecimal.valueOf(adder.sum()).divide(new BigDecimal("10000"));
                synchronized (d.balErrors) {
                    if (d.balErrors.isEmpty())
                        d.balErrors.add(type + " Balance Mismatch: Total " + diff.toPlainString());
                }
            }
        });
        return rows.stream().anyMatch(ExcelRowData::hasErrors);
    }

    private boolean runDbCheck(List<ExcelRowData> rows) {
        Set<String> branches = rows.parallelStream().map(r -> r.branch).collect(Collectors.toSet());
        Set<String> cgls = rows.parallelStream().map(r -> r.cgl).collect(Collectors.toSet());
        Set<String> currs = rows.parallelStream().map(r -> r.currency).collect(Collectors.toSet());
        Set<String> validBranches = branchRepo.findAllById(branches).stream().map(BranchMaster::getCode).collect(Collectors.toSet());
        Set<String> validCgls = cglRepo.findAllById(cgls).stream().map(CglMaster::getCglNumber).collect(Collectors.toSet());
        Set<String> validCurrs = currencyRepo.findAllById(currs).stream().map(CurrencyMaster::getCurrencyCode).collect(Collectors.toSet());
        rows.parallelStream().forEach(d -> {
            if (!validBranches.contains(d.branch)) d.dbErrors.add("Branch Not Found: " + d.branch);
            if (!validCurrs.contains(d.currency)) d.dbErrors.add("Currency Not Found: " + d.currency);
            if (!validCgls.contains(d.cgl)) d.dbErrors.add("CGL Not Found: " + d.cgl);
        });
        return rows.stream().anyMatch(ExcelRowData::hasErrors);
    }

    private void failRequest(String reqId, List<ExcelRowData> rows, String msg, int stage) throws IOException {
        byte[] excel = generateErrorExcelFast(rows);
        fileCache.put(reqId + "_ERROR", excel);
        updateState(reqId, s -> {
            s.setStatus("ERROR");
            s.setMessage(msg);
            s.setCurrentStage(stage);
            s.setErrorCount(rows.stream().filter(ExcelRowData::hasErrors).count());
            s.setHasErrorFile(true);
        });
    }

    private void completeRequest(String reqId, List<ExcelRowData> rows, LocalDate pDate, boolean isCsv) throws IOException {
        dataCache.put(reqId, rows);
        if (!isCsv) {
            byte[] csv = generateSuccessCsv(rows, pDate);
            fileCache.put(reqId + "_SUCCESS", csv);
        }
        List<Map<String, Object>> preview = rows.stream().limit(2000).map(this::mapToPreview).collect(Collectors.toList());
        updateState(reqId, s -> {
            s.setCurrentStage(4);
            s.setStatus("SUCCESS");
            s.setMessage("Validation Successful");
            s.setHasSuccessFile(!isCsv);
            try {
                s.setPreviewDataJson(new com.fasterxml.jackson.databind.ObjectMapper().writeValueAsString(preview));
            } catch (Exception e) {
            }
        });
    }

    private byte[] generateSuccessCsv(List<ExcelRowData> rows, LocalDate postingDate) {
        StringBuilder csv = new StringBuilder(rows.size() * 100);
        DateTimeFormatter ddMMyyyy = DateTimeFormatter.ofPattern("ddMMyyyy");
        String col2 = postingDate.format(ddMMyyyy);
        String year = (postingDate.getMonthValue() >= 4) ? String.valueOf(postingDate.getYear()) : String.valueOf(postingDate.getYear() - 1);
        String month = String.format("%02d", (postingDate.getMonthValue() >= 4) ? (postingDate.getMonthValue() - 3) : (postingDate.getMonthValue() + 9));
        for (ExcelRowData row : rows) {
            String cDate = row.isSystemFormat ? row.sysDate : col2;
            String cYear = row.isSystemFormat ? row.sysYear : year;
            String cMonth = row.isSystemFormat ? row.sysPeriod : month;
            String col5 = String.format("%5s", row.branch).replace(' ', '0');
            String col6 = (row.currency == null || row.currency.isEmpty()) ? "INR" : row.currency;
            String plainAmt = row.amount.abs().setScale(4, RoundingMode.HALF_UP).toPlainString();
            int dotIndex = plainAmt.indexOf('.');
            String intPart = (dotIndex == -1) ? plainAmt : plainAmt.substring(0, dotIndex);
            String decPart = (dotIndex == -1) ? "0000" : plainAmt.substring(dotIndex + 1);
            String col8 = String.format("%16s", intPart).replace(' ', '0') + "." + decPart;
            String col9 = "Credit".equalsIgnoreCase(row.txnType) ? "Cr" : "Dr";
            String col10 = (row.remarks != null) ? row.remarks.replace(",", " ") : "";
            String col11 = (row.productCode != null && !row.productCode.isEmpty()) ? row.productCode : "A";
            csv.append("01,").append(cDate).append(",").append(cYear).append(",").append(cMonth).append(",")
                    .append(col5).append(",").append(col6).append(",").append(row.cgl).append(",").append(col8).append(",")
                    .append(col9).append(",").append(col10).append(",").append(col11).append(",B,C,D\n");
        }
        return csv.toString().getBytes(StandardCharsets.UTF_8);
    }

    private byte[] generateErrorExcelFast(List<ExcelRowData> rows) throws IOException {
        try (SXSSFWorkbook workbook = new SXSSFWorkbook(100)) {
            Sheet sheet = workbook.createSheet("Error Report");
            CellStyle errStyle = workbook.createCellStyle();
            Font f = workbook.createFont();
            f.setColor(IndexedColors.RED.getIndex());
            errStyle.setFont(f);
            Row h = sheet.createRow(0);
            List<String> headerList = new ArrayList<>();
            boolean isSystem = rows.stream().anyMatch(r -> r.isSystemFormat);
            if (isSystem) {
                headerList.add("BatchID");
                headerList.add("Date");
                headerList.add("Year");
                headerList.add("Month");
            }
            headerList.addAll(Arrays.asList("Branch", "Currency", "CGL", "Amount", "TxnType", "Remarks", "Product", "ERRORS"));
            for (int i = 0; i < headerList.size(); i++) h.createCell(i).setCellValue(headerList.get(i));
            int idx = 1;
            for (ExcelRowData d : rows) {
                Row r = sheet.createRow(idx++);
                int col = 0;
                if (isSystem) {
                    r.createCell(col++).setCellValue(d.sysSite);
                    r.createCell(col++).setCellValue(d.sysDate);
                    r.createCell(col++).setCellValue(d.sysYear);
                    r.createCell(col++).setCellValue(d.sysPeriod);
                }
                r.createCell(col++).setCellValue(d.branch);
                r.createCell(col++).setCellValue(d.currency);
                r.createCell(col++).setCellValue(d.cgl);
                r.createCell(col++).setCellValue(d.amount != null ? d.amount.toString() : "");
                r.createCell(col++).setCellValue(d.txnType);
                r.createCell(col++).setCellValue(d.remarks);
                r.createCell(col++).setCellValue(d.productCode);
                Cell c = r.createCell(col);
                if (d.hasErrors()) {
                    c.setCellValue(d.getAllErrors());
                    c.setCellStyle(errStyle);
                }
            }
            ByteArrayOutputStream out = new ByteArrayOutputStream();
            workbook.write(out);
            return out.toByteArray();
        }
    }

    public byte[] generateTemplateBytes() throws IOException {
        try (SXSSFWorkbook wb = new SXSSFWorkbook()) {
            Sheet sheet = wb.createSheet("Journal Template");
            Row row = sheet.createRow(0);
            String[] headers = {"Branch", "Currency", "CGL", "Amount", "TxnType", "Remarks", "Product"};
            for (int i = 0; i < headers.length; i++) {
                Cell cell = row.createCell(i);
                cell.setCellValue(headers[i]);
                sheet.setColumnWidth(i, 4000);
            }
            ByteArrayOutputStream out = new ByteArrayOutputStream();
            wb.write(out);
            return out.toByteArray();
        }
    }

    private String parseCode(Cell c, DataFormatter f) {
        if (c == null) return "";
        if (c.getCellType() == CellType.NUMERIC)
            return BigDecimal.valueOf(c.getNumericCellValue()).toPlainString().split("\\.")[0];
        return f.formatCellValue(c).trim();
    }

    private boolean isRowEmpty(Row r) {
        if (r == null) return true;
        for (int c = r.getFirstCellNum(); c < r.getLastCellNum(); c++)
            if (r.getCell(c) != null && r.getCell(c).getCellType() != CellType.BLANK && !r.getCell(c).toString().trim().isEmpty())
                return false;
        return true;
    }

    private boolean isCellEmpty(Cell c) {
        return c == null || c.getCellType() == CellType.BLANK || c.toString().trim().isEmpty();
    }

    private Map<String, Object> mapToPreview(ExcelRowData r) {
        Map<String, Object> m = new HashMap<>();
        m.put("id", r.rowIndex);
        m.put("branch", r.branch);
        m.put("currency", r.currency);
        m.put("cgl", r.cgl);
        m.put("amount", r.amount != null ? r.amount.toString() : "");
        m.put("txnType", r.txnType);
        m.put("remarks", r.remarks);
        m.put("productCode", r.productCode);
        return m;
    }

    private void updateState(String requestId, java.util.function.Consumer<BulkUploadStateDto> updater) {
        BulkUploadStateDto state = statusCache.getOrDefault(requestId, new BulkUploadStateDto());
        updater.accept(state);
        statusCache.put(requestId, state);
    }

    public static class ExcelRowData {
        public int rowIndex;
        public String branch = "", currency = "", cgl = "", txnType = "", remarks = "", productCode = "";
        public BigDecimal amount;

        public boolean isSystemFormat = false;
        public String sysSite = "", sysDate = "", sysYear = "", sysPeriod = "";

        public List<String> formatErrors = Collections.synchronizedList(new ArrayList<>());
        public List<String> dbErrors = Collections.synchronizedList(new ArrayList<>());
        public List<String> balErrors = Collections.synchronizedList(new ArrayList<>());

        public boolean hasErrors() {
            return !formatErrors.isEmpty() || !dbErrors.isEmpty() || !balErrors.isEmpty();
        }

        public String getAllErrors() {
            List<String> all = new ArrayList<>();
            all.addAll(formatErrors);
            all.addAll(dbErrors);
            all.addAll(balErrors);
            return String.join("; ", all);
        }
    }
}




// ---------------


























package com.fincore.JournalService.Service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fincore.JournalService.Dto.*;
import com.fincore.JournalService.Models.JournalRequest;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;

import java.io.IOException;
import java.time.LocalDate;
import java.util.List;
import java.util.Map;
import java.util.Optional;

/**
 * Interface for Journal Request Service.
 * Includes both Legacy (Sync) and Optimized (Async) methods.
 */
public interface JournalRequestService {

    // --- 1. ASYNC & OPTIMIZED METHODS ---
    /**
     * Starts background batch creation from cached Excel data.
     * @return batchId (String) immediately.
     */
    String createBatchFromCacheAsync(String requestId, String commonRemarks, String creatorId, Integer creatorRole) throws IOException;

    /**
     * Starts background approval/rejection logic.
     */
    void processBulkRequestsAsync(BulkProcessJournalRequestDto dto, String executorId, Integer executorRole);

    /**
     * Starts background chunked deletion of a batch.
     */
    void cancelMyRequestsByBatchIdAsync(String batchId, String userId);

    /**
     * Internal method: Deletes one chunk (10k rows) in a new transaction.
     */
    int deleteBatchChunk(String batchId, String userId);

    /**
     * Fast count using Summary Table or Index.
     */
    long getRequestCountByBatchId(String batchId);

    // --- 2. INTERNAL ASYNC EXECUTORS (Public for Spring Proxy) ---
    void executeAsyncBatchCreation(String batchId, String requestId, String commonRemarks, String creatorId, Integer creatorRole);
    void executeAsyncBatchProcessing(BulkProcessJournalRequestDto dto, String executorId);
    void executeAsyncBatchCancellation(String batchId, String userId);

    // --- 3. LEGACY / STANDARD METHODS ---
    LocalDate getCurrentPostingDate();
    List<Map<String, Object>> getPendingBatchSummaries();
    List<Map<String, Object>> getAllBatchSummaries();

    // Manual Creation (Sync)
    List<JournalRequest> createBatchRequest(BatchRequestDto dto, String creatorId, Integer creatorRole) throws JsonProcessingException;
    String createBulkBatchRequest(BatchRequestDto dto, String creatorId, Integer creatorRole) throws JsonProcessingException;
    String createBatchFromCache(String requestId, String commonRemarks, String creatorId, Integer creatorRole) throws IOException; // Deprecated by Async version

    List<JournalRequest> processBulkRequests(BulkProcessJournalRequestDto dto, String executorId, Integer executorRole);
    Optional<JournalRequest> updateRequestStatus(ProcessJournalRequestDto dto, String executorId, Integer executorRole) throws JsonProcessingException;

    List<JournalRequest> getMyRequests(String userId);
    List<JournalRequest> getPendingRequests(String userId, Integer userRole);
    List<JournalRequest> getRequestsByBatchId(String batchId);
    Page<JournalRequest> getRequestsByBatchIdPaginated(String batchId, Pageable pageable);
    List<JournalRequestStatusDto> getJournalRequestStatusList();

    void cancelMyRequest(Long requestId, String userId);
    void cancelMyRequestsByBatchId(String batchId, String userId); // Deprecated by Async version
    void cancelMyRequestsByJournalPrefixes(List<String> journalIdPrefixes, String userId);
    void cancelMyRequestsByJournalPrefix(String journalIdPrefix, String userId);
}






















package com.fincore.JournalService.Service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fincore.JournalService.Dto.*;
import com.fincore.JournalService.Models.JournalLog;
import com.fincore.JournalService.Models.JournalRequest;
import com.fincore.JournalService.Models.enums.RequestStatus;
import com.fincore.JournalService.Repository.JournalLogRepository;
import com.fincore.JournalService.Repository.JournalRequestRepository;
import com.fincore.JournalService.Service.JournalBulkValidationService.ExcelRowData;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.context.annotation.Lazy;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.jdbc.core.BatchPreparedStatementSetter;
import org.springframework.jdbc.core.CallableStatementCallback;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Propagation;
import org.springframework.transaction.annotation.Transactional;

import java.io.IOException;
import java.math.BigDecimal;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Timestamp;
import java.time.LocalDate;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.TimeUnit;

/**
 * High-Performance Implementation of Journal Service.
 * Handles 600k+ row batches using Async execution, JDBC Batching, and Oracle Parallelism.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class JournalRequestServiceImpl implements JournalRequestService {

    private final JournalRequestRepository journalRequestRepository;
    private final JournalLogRepository journalLogRepository;
    private final SequenceService sequenceService;
    private final NotificationWriterService notificationWriterService;
    private final PermissionConfigService permissionConfigService;
    private final JournalBulkValidationService journalBulkValidationService;
    private final HdfsSyncService hdfsSyncService;

    // Lazy injection for self-invocation (Spring AOP proxy support for @Async)
    @Autowired
    @Lazy
    private JournalRequestService self;

    @Autowired
    @Qualifier("oracleJdbcTemplate")
    private JdbcTemplate jdbcTemplate;

    @Autowired
    @Qualifier("byteArrayRedisTemplate")
    private RedisTemplate<String, byte[]> redisTemplate;

    // ==================================================================================
    // 1. ASYNC BATCH CREATION (Safe & Fast)
    // ==================================================================================

    @Override
    @Transactional
    public String createBatchFromCacheAsync(String requestId, String commonRemarks, String creatorId, Integer creatorRole) throws IOException {
        // A. REDIS LOCK: Prevent "Double-Click" Duplicate Batches
        String lockKey = "LOCK_REQ_" + requestId;
        Boolean acquired = redisTemplate.opsForValue().setIfAbsent(lockKey, new byte[0], 5, TimeUnit.MINUTES);

        if (Boolean.FALSE.equals(acquired)) {
            log.warn("Duplicate creation attempt blocked for ReqID: {}", requestId);
            throw new IllegalStateException("Batch creation is already in progress.");
        }

        // B. Generate ID & Fire Async Process
        try {
            String batchId = sequenceService.getNextBatchId();
            self.executeAsyncBatchCreation(batchId, requestId, commonRemarks, creatorId, creatorRole);
            return batchId;
        } catch (Exception e) {
            redisTemplate.delete(lockKey); // Release lock if firing failed
            throw e;
        }
    }

    @Override
    @Async("bulkExecutor")
    @Transactional
    public void executeAsyncBatchCreation(String batchId, String requestId, String commonRemarks, String creatorId, Integer creatorRole) {
        log.info("ASYNC CREATE: Starting Batch {} (Source: {})", batchId, requestId);
        long start = System.currentTimeMillis();

        try {
            // 1. Fetch Validated Rows from Redis (Decompressed)
            List<ExcelRowData> cachedRows = journalBulkValidationService.getValidRowsFromCache(requestId);
            if (cachedRows == null || cachedRows.isEmpty()) {
                logAudit(creatorId, "CREATE_FAIL", "BATCH_ASYNC", "Cache Expired for " + batchId);
                return;
            }

            // 2. In-Memory Aggregation for Summary Table
            final BigDecimal[] totals = {BigDecimal.ZERO, BigDecimal.ZERO}; // [0]=Debit, [1]=Credit

            // 3. JDBC Batch Insert (Optimized for 10k/chunk)
            String sql = "INSERT INTO JOURNAL_REQUEST (REQ_ID, REQ_STATUS, CHANGE_TYPE, REQ_DATE, CREATOR_ID, CREATOR_ROLE, BATCH_ID, JOURNAL_ID, COMMON_BATCH_REMARKS, PAYLOAD, REQ_BRANCH_CODE, REQ_CURRENCY, REQ_CGL, REQ_AMOUNT, REQ_CSV_DATE, REQ_NARRATION, REQ_PRODUCT) VALUES (JOURNAL_REQUEST_SEQ.nextval, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)";
            Timestamp ts = Timestamp.valueOf(LocalDateTime.now());
            final DateTimeFormatter jsonFmt = DateTimeFormatter.ISO_DATE;

            jdbcTemplate.batchUpdate(sql, new BatchPreparedStatementSetter() {
                public void setValues(PreparedStatement ps, int i) throws SQLException {
                    ExcelRowData r = cachedRows.get(i);
                    String jId = batchId + "-" + (i + 1);

                    // Date Parsing
                    LocalDate rDate = LocalDate.now();
                    if (r.isSystemFormat && r.sysDate != null && r.sysDate.length() == 8) {
                        try {
                            rDate = LocalDate.parse(r.sysDate, DateTimeFormatter.ofPattern("ddMMyyyy"));
                        } catch (Exception e) {
                        }
                    }

                    // CRITICAL: Logic for Credit = Negative
                    BigDecimal absAmt = (r.amount != null) ? r.amount.abs() : BigDecimal.ZERO;
                    boolean isCredit = "Credit".equalsIgnoreCase(r.txnType) || "Cr".equalsIgnoreCase(r.txnType);
                    BigDecimal signedAmt = isCredit ? absAmt.negate() : absAmt;

                    // Accumulate for Summary
                    if (isCredit) totals[1] = totals[1].add(absAmt);
                    else totals[0] = totals[0].add(absAmt);

                    // Set Parameters
                    ps.setString(1, "P");
                    ps.setString(2, "ADD");
                    ps.setTimestamp(3, ts);
                    ps.setString(4, creatorId);
                    ps.setInt(5, creatorRole != null ? creatorRole : 0);
                    ps.setString(6, batchId);
                    ps.setString(7, jId);
                    ps.setString(8, commonRemarks);

                    // Build JSON Payload for Legacy UI
                    ps.setString(9, buildJsonPayloadFast(r, signedAmt, rDate, batchId, jId, commonRemarks, i + 1, jsonFmt));

                    // Structured Columns for Oracle PL/SQL
                    ps.setString(10, r.branch);
                    ps.setString(11, r.currency);
                    ps.setString(12, r.cgl);
                    ps.setBigDecimal(13, signedAmt);
                    ps.setDate(14, java.sql.Date.valueOf(rDate));
                    ps.setString(15, r.remarks);
                    ps.setString(16, r.productCode);
                }

                public int getBatchSize() {
                    return cachedRows.size();
                }
            });

            // 4. Update Summary Table (JOURNAL_BATCH_MASTER)
            String sumSql = "INSERT INTO JOURNAL_BATCH_MASTER (BATCH_ID, CREATOR_ID, REQ_DATE, BATCH_REMARKS, TOTAL_ROWS, TOTAL_DEBIT, TOTAL_CREDIT, BATCH_STATUS) VALUES (?, ?, ?, ?, ?, ?, ?, ?)";
            jdbcTemplate.update(sumSql, batchId, creatorId, ts, commonRemarks, cachedRows.size(), totals[0], totals[1], "PENDING");

            // 5. Cleanup Redis
//            journalBulkValidationService.clearCache(requestId);

            // 6. Notifications
            createNotification(batchId, creatorId, cachedRows.size());
            logAudit(creatorId, "CREATE_SUCCESS", "BATCH_ASYNC", "Created Batch " + batchId);
            log.info("Batch {} Created. Time: {}ms", batchId, System.currentTimeMillis() - start);

        } catch (Exception e) {
            log.error("Async Create Failed", e);
            logAudit(creatorId, "CREATE_FAIL", "BATCH_ASYNC", e.getMessage());
            // Lock auto-expires, but we can delete it here to be safe
            redisTemplate.delete("LOCK_REQ_" + requestId);
            throw e; // Triggers Rollback
        }
    }

    // ==================================================================================
    // 2. ASYNC APPROVAL (With Parallel DB & HDFS Retry)
    // ==================================================================================

    @Override
    public void processBulkRequestsAsync(BulkProcessJournalRequestDto dto, String executorId, Integer executorRole) {
        log.info("Initiating Async Process for Batch: {}", dto.getBatchId());
        self.executeAsyncBatchProcessing(dto, executorId);
    }

    @Async("bulkExecutor")
    @Transactional
    public void executeAsyncBatchProcessing(BulkProcessJournalRequestDto dto, String executorId) {
        String batchId = dto.getBatchId();
        try {
            if (RequestStatus.ACCEPTED.equals(dto.getStatus())) {

                // 1. Call Oracle PL/SQL (Parallel Mode)
                log.info("Calling Oracle Procedure for Batch {}", batchId);
                List<HdfsSyncDto> syncData = jdbcTemplate.execute(
                        "{call PROCESS_JOURNAL_BATCH(?, ?, ?, ?, ?)}",
                        (CallableStatementCallback<List<HdfsSyncDto>>) cs -> {
                            cs.setString(1, batchId);
                            cs.setString(2, executorId);
                            cs.setString(3, dto.getRemarks());
                            cs.setString(4, "A");
                            cs.registerOutParameter(5, -10); // Cursor
                            cs.execute();

                            List<HdfsSyncDto> list = new ArrayList<>();
                            try (ResultSet rs = (ResultSet) cs.getObject(5)) {
                                while (rs.next()) {
                                    list.add(new HdfsSyncDto(
                                            rs.getString("BRANCH"), rs.getString("CURRENCY"), rs.getString("CGL"),
                                            rs.getDate("BAL_DATE").toLocalDate(),
                                            rs.getBigDecimal("NEW_BALANCE"), rs.getBigDecimal("NEW_INR_BALANCE")
                                    ));
                                }
                            }
                            return list;
                        }
                );

                // 2. Update Summary Table
                jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'ACCEPTED', EXECUTOR_ID = ?, EXECUTION_DATE = SYSTIMESTAMP, EXECUTOR_REMARKS = ? WHERE BATCH_ID = ?",
                        executorId, dto.getRemarks(), batchId);

                // 3. Sync HDFS (With Retry Queue Safety)
                try {
                    hdfsSyncService.syncToDataLake(syncData);
                } catch (Exception e) {
                    log.error("HDFS Sync Failed for Batch {}. Queuing for Retry.", batchId, e);
                    // Insert into Retry Queue for Recovery Service
                    jdbcTemplate.update("INSERT INTO HDFS_SYNC_RETRY_QUEUE (BATCH_ID, STATUS) VALUES (?, 'PENDING')", batchId);
                }

                logAudit(executorId, "APPROVE_SUCCESS", "BATCH_ASYNC", "Approved Batch " + batchId);

            } else if (RequestStatus.REJECTED.equals(dto.getStatus())) {
                // Reject Rows
                jdbcTemplate.update("UPDATE JOURNAL_REQUEST SET REQ_STATUS = 'R', EXECUTOR_ID = ?, EXECUTOR_REMARKS = ?, EXECUTION_DATE = SYSDATE WHERE BATCH_ID = ? AND REQ_STATUS = 'P'",
                        executorId, dto.getRemarks(), batchId);

                // Reject Summary
                jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'REJECTED', EXECUTOR_ID = ?, EXECUTION_DATE = SYSTIMESTAMP, EXECUTOR_REMARKS = ? WHERE BATCH_ID = ?",
                        executorId, dto.getRemarks(), batchId);

                logAudit(executorId, "REJECT_OK", "BATCH_ASYNC", "Rejected Batch " + batchId);
            }
        } catch (Exception e) {
            log.error("Async Process Failed", e);
            logAudit(executorId, "PROCESS_FAIL", "BATCH_ASYNC", e.getMessage());
            // No rollback here for Java exceptions if PL/SQL succeeded,
            // but PL/SQL handles its own rollback on error.
        }
    }

    // ==================================================================================
    // 3. ASYNC DELETION (Chunked & Safe)
    // ==================================================================================

    @Override
    public void cancelMyRequestsByBatchIdAsync(String batchId, String userId) {
        log.info("Initiating Async Cancel for Batch: {}", batchId);
        self.executeAsyncBatchCancellation(batchId, userId);
    }

    @Override
    @Async("bulkExecutor")
    public void executeAsyncBatchCancellation(String batchId, String userId) {
        try {
            // Check PENDING status using Summary Table (Fast)
            String status = null;
            try {
                status = jdbcTemplate.queryForObject("SELECT BATCH_STATUS FROM JOURNAL_BATCH_MASTER WHERE BATCH_ID = ? AND CREATOR_ID = ?", String.class, batchId, userId);
            } catch (Exception e) {
                log.warn("Batch/User mismatch for delete");
                return;
            }

            if (!"PENDING".equals(status)) {
                log.warn("Cannot delete batch {} in status {}", batchId, status);
                return;
            }

            // Chunked Delete Loop (10k rows/tx)
            boolean hasMore = true;
            while (hasMore) {
                int deleted = self.deleteBatchChunk(batchId, userId);
                if (deleted == 0) hasMore = false;
                else Thread.sleep(50); // Yield CPU
            }

            // Delete Summary
            jdbcTemplate.update("DELETE FROM JOURNAL_BATCH_MASTER WHERE BATCH_ID = ?", batchId);

            logAudit(userId, "CANCEL_SUCCESS", "BATCH_ASYNC", "Deleted Batch " + batchId);
        } catch (Exception e) {
            log.error("Async Cancel Failed", e);
        }
    }

    @Override
    @Transactional(propagation = Propagation.REQUIRES_NEW)
    public int deleteBatchChunk(String batchId, String userId) {
        return jdbcTemplate.update("DELETE FROM JOURNAL_REQUEST WHERE BATCH_ID = ? AND CREATOR_ID = ? AND REQ_STATUS = 'P' AND ROWNUM <= 10000", batchId, userId);
    }

    // ==================================================================================
    // 4. OPTIMIZED SUMMARIES (Using Master Table)
    // ==================================================================================

    @Override
    public List<Map<String, Object>> getPendingBatchSummaries() {
        String sql = """
            SELECT BATCH_ID, CREATOR_ID, REQ_DATE, BATCH_REMARKS, TOTAL_ROWS, TOTAL_DEBIT, TOTAL_CREDIT, BATCH_STATUS 
            FROM JOURNAL_BATCH_MASTER 
            WHERE BATCH_STATUS = 'PENDING' 
            ORDER BY REQ_DATE DESC
        """;

        return jdbcTemplate.query(sql, (rs, rowNum) -> {
            Map<String, Object> map = new HashMap<>();
            // Mapped to match EXACTLY what Frontend expects (Original DTO keys)
            map.put("batchId", rs.getString("BATCH_ID"));
            map.put("creatorId", rs.getString("CREATOR_ID"));
            map.put("requestDate", rs.getTimestamp("REQ_DATE"));
            map.put("commonBatchRemarks", rs.getString("BATCH_REMARKS"));
            map.put("requestCount", rs.getLong("TOTAL_ROWS"));
            map.put("totalDebit", rs.getBigDecimal("TOTAL_DEBIT"));
            map.put("totalCredit", rs.getBigDecimal("TOTAL_CREDIT"));
            map.put("requestStatus", rs.getString("BATCH_STATUS")); // Master has 'PENDING'
            return map;
        });
    }

    @Override
    public List<Map<String, Object>> getAllBatchSummaries() {
        String sql = """
            SELECT BATCH_ID, CREATOR_ID, REQ_DATE, BATCH_REMARKS, TOTAL_ROWS, TOTAL_DEBIT, TOTAL_CREDIT, 
                   BATCH_STATUS, EXECUTOR_ID, EXECUTOR_REMARKS 
            FROM JOURNAL_BATCH_MASTER 
            ORDER BY REQ_DATE DESC 
            FETCH FIRST 100 ROWS ONLY
        """;

        return jdbcTemplate.query(sql, (rs, rowNum) -> {
            Map<String, Object> map = new HashMap<>();
            map.put("batchId", rs.getString("BATCH_ID"));
            map.put("creatorId", rs.getString("CREATOR_ID"));
            map.put("requestDate", rs.getTimestamp("REQ_DATE"));
            map.put("commonBatchRemarks", rs.getString("BATCH_REMARKS"));
            map.put("requestCount", rs.getLong("TOTAL_ROWS"));
            map.put("totalDebit", rs.getBigDecimal("TOTAL_DEBIT"));
            map.put("totalCredit", rs.getBigDecimal("TOTAL_CREDIT"));
            map.put("requestStatus", rs.getString("BATCH_STATUS"));
            map.put("executorId", rs.getString("EXECUTOR_ID"));
            map.put("executorRemarks", rs.getString("EXECUTOR_REMARKS"));
            return map;
        });
    }

    @Override
    public long getRequestCountByBatchId(String batchId) {
        try {
            return jdbcTemplate.queryForObject("SELECT TOTAL_ROWS FROM JOURNAL_BATCH_MASTER WHERE BATCH_ID = ?", Long.class, batchId);
        } catch (Exception e) {
            return 0; // Or fallback to count(*)
        }
    }

    // ==================================================================================
    // 5. HELPERS
    // ==================================================================================

    private String buildJsonPayloadFast(ExcelRowData row, BigDecimal amount, LocalDate pDate, String batchId, String jId, String rem, int count, DateTimeFormatter fmt) {
        return "{\"changeType\":\"ADD\",\"masterJournalId\":null,\"csvDate\":\"" + pDate.format(fmt) + "\"," +
                "\"branch\":\"" + row.branch + "\",\"currency\":\"" + row.currency + "\"," +
                "\"cgl\":\"" + row.cgl + "\",\"amount\":" + amount + "," +
                "\"productType\":\"" + (row.productCode == null ? "" : row.productCode) + "\"," +
                "\"remarks\":\"" + (row.remarks == null ? "" : escapeJson(row.remarks)) + "\"," +
                "\"arFlag\":\"A\",\"acClassification\":\"A\",\"batchId\":\"" + batchId + "\"," +
                "\"journalId\":\"" + jId + "\",\"commonBatchRemarks\":\"" + escapeJson(rem) + "\"," +
                "\"transactionCount\":" + count + "}";
    }

    private String escapeJson(String s) {
        return s == null ? "" : s.replace("\"", "\\\"").replace("\\", "\\\\");
    }

    private void createNotification(String batchId, String creatorId, int size) {
        try {
            NotificationConfigDto config = permissionConfigService.getConfig("JOURNAL_AUTH");
            String message = String.format("Batch %s (%d rows) by %s pending.", batchId, size, creatorId);
            notificationWriterService.createNotification(null, config.getTargetRoles(), message, config.getTargetUrl(), batchId, "JournalService");
        } catch (Exception e) {
            log.error("Notification Failed", e);
        }
    }

    private void logAudit(String user, String action, String type, String val) {
        try {
            JournalLog l = new JournalLog();
            l.setUserId(user);
            l.setActionType(action);
            l.setChangeType(type);
            l.setNewValue(val.length() > 3900 ? val.substring(0, 3900) : val);
            l.setActionTime(LocalDateTime.now());
            journalLogRepository.save(l);
        } catch (Exception e) {
            log.error("Audit Failed", e);
        }
    }

    // --- Legacy / Unchanged Implementations ---
    @Override
    public LocalDate getCurrentPostingDate() {
        try {
            String sql = "SELECT USERS_DATE FROM FINCORE_DATE FETCH FIRST 1 ROWS ONLY";
            return jdbcTemplate.queryForObject(sql, (rs, rowNum) -> {
                Timestamp ts = rs.getTimestamp("USERS_DATE");
                if (ts != null) return ts.toLocalDateTime().toLocalDate();
                return LocalDate.now();
            });
        } catch (Exception e) {
            log.error("Error fetching USERS_DATE. Using System Date.", e);
            return LocalDate.now();
        }
    }

    @Override
    public Page<JournalRequest> getRequestsByBatchIdPaginated(String b, Pageable p) {
        return journalRequestRepository.findByBatchIdPaginated(b, p);
    }

    @Override
    public List<JournalRequest> getRequestsByBatchId(String b) {
        return journalRequestRepository.findByBatchId(b);
    }

    @Override
    public List<JournalRequest> getMyRequests(String u) {
        return new ArrayList<>(); /* Optimized: Don't load 600k rows */
    }

    @Override
    public List<JournalRequest> getPendingRequests(String u, Integer r) {
        return new ArrayList<>();
    }

    @Override
    public List<JournalRequestStatusDto> getJournalRequestStatusList() {
        return new ArrayList<>();
    }

    @Override
    @Transactional
    public void cancelMyRequest(Long r, String u) {
        journalRequestRepository.deleteById(r);
    }

    @Override
    @Transactional
    public void cancelMyRequestsByBatchId(String b, String u) {
        cancelMyRequestsByBatchIdAsync(b, u);
    }

    @Override
    @Transactional
    public void cancelMyRequestsByJournalPrefixes(List<String> l, String u) {
        journalRequestRepository.deleteJournalsNative(l, u);
    }

    @Override
    @Transactional
    public void cancelMyRequestsByJournalPrefix(String p, String u) {
    }

    @Override
    public Optional<JournalRequest> updateRequestStatus(ProcessJournalRequestDto d, String u, Integer r) {
        return Optional.empty();
    }

    @Override
    public List<JournalRequest> createBatchRequest(BatchRequestDto d, String u, Integer r) throws JsonProcessingException {
        return new ArrayList<>();
    }

    @Override
    public String createBulkBatchRequest(BatchRequestDto d, String u, Integer r) throws JsonProcessingException {
        return "";
    }

    @Override
    public String createBatchFromCache(String r, String m, String u, Integer o) throws IOException {
        return createBatchFromCacheAsync(r, m, u, o);
    }

    @Override
    public List<JournalRequest> processBulkRequests(BulkProcessJournalRequestDto d, String u, Integer r) {
        processBulkRequestsAsync(d, u, r);
        return Collections.emptyList();
    }
}






















package com.fincore.JournalService.Service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.stereotype.Service;

import java.util.HashSet;
import java.util.List;
import java.util.Set;

/**
 * Helper service to fetch Master Data for bulk validation.
 * Fetches entire active sets to allow O(1) in-memory validation.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class ValidationMasterService {

    @Autowired
    @Qualifier("oracleJdbcTemplate")
    private JdbcTemplate jdbcTemplate;

    public Set<String> getAllActiveBranches() {
        long start = System.currentTimeMillis();
        // Optimized query with FetchSize
        String sql = "SELECT CODE FROM BRANCH_MASTER WHERE STATUS = 1";
        List<String> list = jdbcTemplate.query(con -> {
            var ps = con.prepareStatement(sql);
            ps.setFetchSize(5000); // Critical for performance
            return ps;
        }, (rs, rowNum) -> rs.getString(1));

        log.info("Loaded {} Active Branches in {}ms", list.size(), System.currentTimeMillis() - start);
        return new HashSet<>(list);
    }

    public Set<String> getAllActiveCurrencies() {
        long start = System.currentTimeMillis();
        String sql = "SELECT CURRENCY_CODE FROM CURRENCY_MASTER WHERE FLAG = 1";
        List<String> list = jdbcTemplate.query(con -> {
            var ps = con.prepareStatement(sql);
            ps.setFetchSize(1000);
            return ps;
        }, (rs, rowNum) -> rs.getString(1));

        log.info("Loaded {} Active Currencies in {}ms", list.size(), System.currentTimeMillis() - start);
        return new HashSet<>(list);
    }

    public Set<String> getAllActiveCgls() {
        long start = System.currentTimeMillis();
        String sql = "SELECT CGL_NUMBER FROM CGL_MASTER WHERE STATUS = 1";
        List<String> list = jdbcTemplate.query(con -> {
            var ps = con.prepareStatement(sql);
            ps.setFetchSize(2000);
            return ps;
        }, (rs, rowNum) -> rs.getString(1));

        log.info("Loaded {} Active CGLs in {}ms", list.size(), System.currentTimeMillis() - start);
        return new HashSet<>(list);
    }
}




































package com.fincore.JournalService.Service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Propagation;
import org.springframework.transaction.annotation.Transactional;

import com.fincore.JournalService.Models.Notifications;
import com.fincore.JournalService.Repository.NotificationRepository;

@Service
@RequiredArgsConstructor
@Slf4j
public class NotificationWriterService {

    private final NotificationRepository notificationRepository;

    @Transactional(propagation = Propagation.MANDATORY) 
    public void createNotification(String userId, String targetRole, String message, String linkUrl, String aggregateId, String eventSource) {

        if (userId == null && targetRole == null) {
            log.warn("Skipping notification creation: Both userId and targetRole are null. AggregateID: {}", aggregateId);
            return;
        }

        if (message == null || message.isBlank()) {
            log.warn("Skipping notification creation: Message is null or blank. AggregateID: {}", aggregateId);
            throw new IllegalArgumentException("Notification message cannot be null or blank.");
        }

        Notifications notification = Notifications.builder()
                .userId(userId)
                .targetRole(targetRole)
                .message(message)
                .linkUrl(linkUrl)
                .aggregateId(aggregateId)
                .eventSource(eventSource)
                .build();

        notificationRepository.save(notification);

        if (userId != null) {
            log.info("Saved 1-to-1 notification event for user: {} (AggregateID: {})", userId, aggregateId);
        } else {
            log.info("Saved 1-to-many notification event for role: {} (AggregateID: {})", targetRole, aggregateId);
        }
    }
}












package com.fincore.JournalService.Service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.cache.annotation.Cacheable;
import org.springframework.stereotype.Service;

import com.fincore.JournalService.Dto.NotificationConfigDto;
import com.fincore.JournalService.Repository.PermissionRepository;

import java.util.List;
import java.util.stream.Collectors;

@Service
@RequiredArgsConstructor
@Slf4j
public class PermissionConfigService {

    private final PermissionRepository permissionRepository;

    @Cacheable(value = "notification_configs", key = "#requestType")
    public NotificationConfigDto getConfig(String requestType) {

        log.info("Cache Miss: Fetching DB permissions for MOC notification type: {}", requestType);

        List<Object[]> results = permissionRepository.findUrlAndRolesByRequestType(requestType);

        if (results.isEmpty()) {
            log.warn("No notification config found for request type {}. Using defaults.", requestType);
            return new NotificationConfigDto("/dashboard", null); // Default to no roles
        }

        String url = (String) results.get(0)[0];
        if (url == null) url = "/dashboard";

        String roles = results.stream()
                .map(row -> String.valueOf(row[1])) // Role ID
                .distinct()
                .collect(Collectors.joining(","));

        if (roles.isEmpty()) {
             log.warn("No roles found for request type {}. Notification will not be sent to a group.", requestType);
             return new NotificationConfigDto(url, null);
        }
        
        log.info("Fetched Notification Config for {}: URL=[{}], Roles=[{}]", requestType, url, roles);
        return new NotificationConfigDto(url, roles);
    }
}












package com.fincore.JournalService.Service;

import jakarta.persistence.EntityManager;
import jakarta.persistence.PersistenceContext;
import lombok.RequiredArgsConstructor;
import org.springframework.stereotype.Service;
import jakarta.transaction.Transactional;

@Service
@RequiredArgsConstructor
public class SequenceService {

    @PersistenceContext
    private EntityManager entityManager;

    /**
     * Pads the given number with leading zeros to a total length of 7.
     */
    private String padToSevenDigits(Long id) {
        if (id == null) {
            throw new IllegalStateException("Sequence query returned null");
        }
        return String.format("%07d", id);
    }

    /**
     * Gets the next BATCH_ID from the BATCH_SEQ sequence. 
     */
    @Transactional
    public String getNextBatchId() {
        Number nextId = (Number) entityManager
                .createNativeQuery("select BATCH_SEQ.nextval from dual")
                .getSingleResult();
        
        return padToSevenDigits(nextId.longValue());
    }

    /**
     * Gets the next JOURNAL_ID prefix from the JRNL_SEQ sequence.
     */
    @Transactional
    public String getNextJournalIdPrefix() {
        
      
        Number nextId = (Number) entityManager
                .createNativeQuery("select JRNL_SEQ.nextval from dual")
                .getSingleResult();
      

        return padToSevenDigits(nextId.longValue());
    }
}




***************************************************************************************************

package com.fincore.JournalService;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.cache.annotation.EnableCaching;
import org.springframework.data.jpa.repository.config.EnableJpaRepositories;

@SpringBootApplication
@EnableCaching

    public class JournalServiceApplication {
	public static void main(String[] args) {
		SpringApplication.run(JournalServiceApplication.class, args);
	}

}

















*************************************************************************************************
spring.application.name=JournalService
spring.datasource.driver-class-name=oracle.jdbc.OracleDriver
spring.profiles.active=dev
server.port=9999

#todo Expose all actuator endpoints over HTTP.
management.endpoints.web.exposure.include=*

# Always show full details on the health endpoint (e.g., database connection status)
management.endpoint.health.show-details=always

# Add some custom info to the /info endpoint
info.app.name=JournalService
info.app.description=Service for managing journals.
info.app.version=1.0.0


# --- Redis Configuration ---
# Ensure timeouts are set so a Redis glitch doesn't hang the app
spring.data.redis.host=10.0.17.242
spring.data.redis.port=6379
spring.data.redis.timeout=2000
spring.data.redis.jedis.pool.max-active=50
spring.data.redis.jedis.pool.max-idle=10
spring.data.redis.jedis.pool.min-idle=5

# --- 3. FILE UPLOAD LIMITS ---
# 600k rows in Excel can be 50MB+. Ensure Spring accepts it.
spring.servlet.multipart.max-file-size=100MB
spring.servlet.multipart.max-request-size=100MB

# --- 4. ASYNC EXECUTOR TUNING (Matches our Java Config) ---
# Verify these match the AsyncConfig.java values
app.async.core-pool-size=5
app.async.max-pool-size=20
app.async.queue-capacity=500
# ----------------------------------


# LOGIN SERVICE KEY
jwt.secret=bWV0aGlvbnlsdGhyZW9ueWx0aHJlb255bGdsdXRhbWlueWxhbGFueWw=

spring.datasource.oracle.jdbc-url=jdbc:oracle:thin:@10.177.103.192:1523/fincorepdb1
spring.datasource.oracle.username=fincore
spring.datasource.oracle.password=Password#1234
spring.datasource.oracle.driver-class-name=oracle.jdbc.OracleDriver
# --- 1. ORACLE CONNECTION POOL (HikariCP) ---
# Critical: We are doing parallel inserts (8 threads).
# If pool is too small, threads will wait.
spring.datasource.hikari.maximum-pool-size=30
spring.datasource.hikari.minimum-idle=10
spring.datasource.hikari.connection-timeout=30000
spring.datasource.hikari.idle-timeout=600000
spring.datasource.hikari.max-lifetime=1800000

# Hibernate specific config for Oracle
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.OracleDialect
spring.jpa.show-sql=false




# --- 2. Secondary DataSource (Hive / Thrift / Delta Lake) ---
# Used for Data Lake connectivity
spring.datasource.hive.jdbc-url=jdbc:hive2://spark-thrift:10000/default
spring.datasource.hive.driver-class-name=org.apache.hive.jdbc.HiveDriver
# Validation query to ensure connection is alive (Hive specific)
spring.datasource.hive.test-on-borrow=true
spring.datasource.hive.validation-query=SELECT 1





















tables: 
***************************************************************************************************
journal_requests :

REQ_ID	NUMBER	No		1	
REQ_STATUS	VARCHAR2(10 CHAR)	No	'P' 	2	
CHANGE_TYPE	VARCHAR2(10 CHAR)	No		3	
REQ_DATE	DATE	No	SYSDATE 	4	
CREATOR_ID	VARCHAR2(12 CHAR)	No		5	
EXECUTOR_ID	VARCHAR2(12 CHAR)	Yes		6	
EXECUTION_DATE	DATE	Yes		7	
EXECUTOR_REMARKS	VARCHAR2(50 CHAR)	Yes		8	
PAYLOAD	CLOB	Yes		9	
BATCH_ID	VARCHAR2(50 CHAR)	Yes		10	
JOURNAL_ID	VARCHAR2(50 CHAR)	Yes		11	
COMMON_BATCH_REMARKS	VARCHAR2(50 CHAR)	Yes		12	
CREATOR_ROLE	NUMBER(10,0)	Yes		13	


data :

1404594	A	ADD	31-12-25	3333333	1021253	31-12-25	DAS	{"changeType":"ADD","masterJournalId":null,"pDate":"2025-12-19","branch":"32098","currency":"USD","cgl":"5051501701","amount":-9997.15,"productType":"12345678","remarks":"Balanced_Txn_326_Cr","arFlag":"A","acClassification":"A","batchId":"0001063","journalId":"0004025-006","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":6}	0001063	0004025-006	Bulk Upload - 2025-12-17	11
1408978	A	ADD	05-01-26	3333333	1021256	05-01-26	ASD	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-02","branch":"32121","currency":"USD","cgl":"1122505001","amount":-10000,"productType":"12345679","remarks":"Normal","arFlag":"A","acClassification":"A","batchId":"0001158","journalId":"0004707-002","commonBatchRemarks":"Bulk Upload - 2025-12-02","transactionCount":2}	0001158	0004707-002	Bulk Upload - 2025-12-02	11
1408977	A	ADD	05-01-26	3333333	1021256	05-01-26	ASD	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-02","branch":"32121","currency":"USD","cgl":"1208505003","amount":10000,"productType":"12345678","remarks":"ewreew","arFlag":"A","acClassification":"A","batchId":"0001158","journalId":"0004707-001","commonBatchRemarks":"Bulk Upload - 2025-12-02","transactionCount":1}	0001158	0004707-001	Bulk Upload - 2025-12-02	11
1408974	A	ADD	05-01-26	3333333	1021256	05-01-26	asd	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-02","branch":"32121","currency":"USD","cgl":"1122505001","amount":-10000,"productType":"12345679","remarks":"Normal","arFlag":"A","acClassification":"A","batchId":"0001156","journalId":"0004705-002","commonBatchRemarks":"Bulk Upload - 2025-12-02","transactionCount":2}	0001156	0004705-002	Bulk Upload - 2025-12-02	11
1408973	A	ADD	05-01-26	3333333	1021256	05-01-26	asd	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-02","branch":"32121","currency":"USD","cgl":"1208505003","amount":10000,"productType":"12345678","remarks":"ewreew","arFlag":"A","acClassification":"A","batchId":"0001156","journalId":"0004705-001","commonBatchRemarks":"Bulk Upload - 2025-12-02","transactionCount":1}	0001156	0004705-001	Bulk Upload - 2025-12-02	11
1408970	A	ADD	05-01-26	3333333	1021256	05-01-26	asd	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-02","branch":"32121","currency":"USD","cgl":"1122505001","amount":-10000,"productType":"12345679","remarks":"Normal","arFlag":"A","acClassification":"A","batchId":"0001154","journalId":"0004703-002","commonBatchRemarks":"Bulk Upload - 2025-12-02","transactionCount":2}	0001154	0004703-002	Bulk Upload - 2025-12-02	11
1408969	A	ADD	05-01-26	3333333	1021256	05-01-26	asd	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-02","branch":"32121","currency":"USD","cgl":"1208505003","amount":10000,"productType":"12345678","remarks":"ewreew","arFlag":"A","acClassification":"A","batchId":"0001154","journalId":"0004703-001","commonBatchRemarks":"Bulk Upload - 2025-12-02","transactionCount":1}	0001154	0004703-001	Bulk Upload - 2025-12-02	11
1406631	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"14443","currency":"USD","cgl":"5051220801","amount":9153.92,"productType":"12345678","remarks":"Balanced_Txn_555_Dr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004367-005","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":5}	0001149	0004367-005	Bulk Upload - 2025-12-17	11
1406630	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"14443","currency":"USD","cgl":"2051070601","amount":-3325.87,"productType":"12345678","remarks":"Balanced_Txn_490_Cr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004367-004","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":4}	0001149	0004367-004	Bulk Upload - 2025-12-17	11
1406629	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"14443","currency":"USD","cgl":"2051070601","amount":3325.87,"productType":"12345678","remarks":"Balanced_Txn_490_Dr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004367-003","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":3}	0001149	0004367-003	Bulk Upload - 2025-12-17	11
1406628	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"14443","currency":"USD","cgl":"5051080630","amount":-1428.23,"productType":"12345678","remarks":"Balanced_Txn_162_Cr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004367-002","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":2}	0001149	0004367-002	Bulk Upload - 2025-12-17	11
1406627	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"14443","currency":"USD","cgl":"5051080630","amount":1428.23,"productType":"12345678","remarks":"Balanced_Txn_162_Dr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004367-001","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":1}	0001149	0004367-001	Bulk Upload - 2025-12-17	11
1406626	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"31977","currency":"USD","cgl":"5051500130","amount":-9792.25,"productType":"12345678","remarks":"Balanced_Txn_488_Cr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004366-004","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":4}	0001149	0004366-004	Bulk Upload - 2025-12-17	11
1406625	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"31977","currency":"USD","cgl":"5051500130","amount":9792.25,"productType":"12345678","remarks":"Balanced_Txn_488_Dr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004366-003","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":3}	0001149	0004366-003	Bulk Upload - 2025-12-17	11
1406624	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"31977","currency":"USD","cgl":"7456505002","amount":-8177.29,"productType":"12345678","remarks":"Balanced_Txn_138_Cr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004366-002","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":2}	0001149	0004366-002	Bulk Upload - 2025-12-17	11
1406623	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"31977","currency":"USD","cgl":"7456505002","amount":8177.29,"productType":"12345678","remarks":"Balanced_Txn_138_Dr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004366-001","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":1}	0001149	0004366-001	Bulk Upload - 2025-12-17	11
1407047	A	ADD	05-01-26	3333333	1021256	05-01-26	qwe	{"changeType":"ADD","masterJournalId":null,"csvDate":"2025-12-17","branch":"14698","currency":"USD","cgl":"2080501103","amount":1590.84,"productType":"12345678","remarks":"Balanced_Txn_130_Dr","arFlag":"A","acClassification":"A","batchId":"0001149","journalId":"0004429-001","commonBatchRemarks":"Bulk Upload - 2025-12-17","transactionCount":1}	0001149	0004429-001	Bulk Upload - 2025-12-17	11


gl_balance : 

ID	NUMBER(19,0)	No	"FINCORE"."GL_BALANCE_SEQ"."NEXTVAL"	1	Id of the balance
BALANCE_DATE	DATE	No	NULL 	2	Date of the balance recorded
BRANCH_CODE	VARCHAR2(5 BYTE)	No		3	branchcodeis the foreign key of the branch_master
CURRENCY	VARCHAR2(3 BYTE)	No		4	currency is foreign key of currency_master
CGL	VARCHAR2(10 BYTE)	No		5	cgl is foreign key of cgl_master
BALANCE	NUMBER(25,4)	No		6	balance recorded for a date
INR_BALANCE	NUMBER(25,2)	Yes		7	Converted to INR Balance

data :
46232234	11-11-25	09298	USD	2248500101	-472.7385	
46232237	11-11-25	09601	USD	2248500101	-539.044	
46232238	11-11-25	09930	USD	2248500101	-13880.5559	
46232239	11-11-25	09995	USD	2248500101	6693.2951	





gl_transactions :
TRANSACTION_ID	NUMBER(20,0)	No	"FINCORE"."GL_TRANSACTIONS_SEQ"."NEXTVAL"	1	
BATCH_ID	VARCHAR2(50 CHAR)	Yes		2	
JOURNAL_ID	VARCHAR2(50 CHAR)	Yes		3	
TRANSACTION_DATE	DATE	Yes	NULL 	4	
POST_DATE	TIMESTAMP(6)	Yes	SYSDATE 	5	
BRANCH_CODE	VARCHAR2(50 CHAR)	Yes		6	
CURRENCY	VARCHAR2(3 BYTE)	Yes		7	
CGL	VARCHAR2(10 BYTE)	Yes		8	
NARRATION	VARCHAR2(40 BYTE)	Yes		9	
DEBIT_AMOUNT	NUMBER(25,4)	Yes		10	
CREDIT_AMOUNT	NUMBER(25,4)	Yes		11	
TRANSACTION_COUNT	NUMBER(10,0)	Yes		12	
SOURCE_FLAG	VARCHAR2(1 BYTE)	Yes		13	

sample data :

316290541	0001170	0004739-485	02-12-25	06-01-26 04:50:26.062668000 PM	32121	USD	1208505003	ewreew	1000	0	485	J
316290542	0001170	0004739-486	02-12-25	06-01-26 04:50:26.062668000 PM	32121	USD	1122505001	Normal	0	1000	486	J
316290543	0001170	0004739-487	02-12-25	06-01-26 04:50:26.062668000 PM	32121	USD	1208505003	ewreew	1000	0	487	J
316290544	0001170	0004739-488	02-12-25	06-01-26 04:50:26.062668000 PM	32121	USD	1122505001	Normal	0	1000	488	J



currency master :

CURRENCY_CODE	VARCHAR2(3 BYTE)	No		1	Currency Code of Currency
CURRENCY_NAME	VARCHAR2(50 BYTE)	No		2	Name of Currency
FLAG	NUMBER(1,0)	No	0	3	Currency Active or Inactive
CURRENCY_RATE	NUMBER(12,6)	Yes		4	Current Rate of Currency
RATE_DATE	DATE	Yes		5	Rate change date 
CREATED_AT	TIMESTAMP(6)	Yes	CURRENT_TIMESTAMP	6	Currency created date
UPDATED_AT	TIMESTAMP(6)	Yes	"CURRENT_TIMESTAMP
   "	7	Currency updated date


data :
VCY	drtvce	0	1.134546		15-12-25 12:21:10.611561000 PM	15-12-25 12:21:10.611561000 PM
RET	hfbdhfadbfhd	0	65.26565		15-12-25 12:52:50.394294000 PM	15-12-25 12:52:50.394294000 PM
MAT	PAMN	0	12.121321		19-12-25 06:34:35.235474000 AM	19-12-25 06:34:35.235477000 AM
GYK	gvkj 	0	1.1		15-12-25 11:39:22.736494000 AM	15-12-25 11:39:22.736494000 AM
GHF	iutyjgtkgjykjhjkgjkggjgjhhghj	0	465564.465454		15-12-25 11:39:42.892401000 AM	15-12-25 11:39:42.892401000 AM







// new introduced queries for fast performance check once for your reference if correc or not and also allign with thye application or not  :-


-- =====================================================================
-- 1. ADD COLUMNS TO JOURNAL_REQUEST
-- We keep 'PAYLOAD' for legacy UI support, but add these for speed.
-- =====================================================================
ALTER TABLE JOURNAL_REQUEST ADD (
    REQ_BRANCH_CODE VARCHAR2(50),
    REQ_CURRENCY    VARCHAR2(3),
    REQ_CGL         VARCHAR2(50),
    REQ_AMOUNT      NUMBER(25, 4), -- Will store Signed Amount (-ve for Credit)
    REQ_CSV_DATE    DATE,
    REQ_NARRATION   VARCHAR2(200),
    REQ_PRODUCT     VARCHAR2(50)
);

-- =====================================================================
-- 2. ADD INDEX FOR PERFORMANCE
-- Critical for the Stored Procedure to find batch rows instantly.
-- =====================================================================
CREATE INDEX IDX_JR_BATCH_OPT ON JOURNAL_REQUEST(BATCH_ID, REQ_STATUS);

-- High-Cardinality Index for specific transaction search
CREATE INDEX IDX_JR_JOURNAL_ID ON JOURNAL_REQUEST(JOURNAL_ID);

-- Branch filtering speed
CREATE INDEX IDX_JR_BRANCH_CODE ON JOURNAL_REQUEST(REQ_BRANCH_CODE);

commit;


-- =====================================================================
-- 3. CREATE/REPLACE THE PROCESSING PROCEDURE
-- This replaces the Java-side "Accept" loop.
-- =====================================================================

create or replace PROCEDURE PROCESS_JOURNAL_BATCH (
    p_batch_id      IN VARCHAR2,
    p_executor_id   IN VARCHAR2,
    p_remarks       IN VARCHAR2,
    p_status        IN VARCHAR2, 
    o_cursor        OUT SYS_REFCURSOR
) AS
    v_missing_rates NUMBER;
BEGIN
    -- Enable Parallel DML
    EXECUTE IMMEDIATE 'ALTER SESSION ENABLE PARALLEL DML';

    -- 0. PRE-CHECK: INTEGRITY GUARD
    -- Before processing, ensure ALL non-INR currencies in this batch have an active rate.
    -- If even one is missing, FAIL the whole batch.
    SELECT COUNT(*)
    INTO v_missing_rates
    FROM (
        SELECT DISTINCT REQ_CURRENCY 
        FROM JOURNAL_REQUEST 
        WHERE BATCH_ID = p_batch_id AND REQ_STATUS = 'P'
    ) req
    LEFT JOIN CURRENCY_MASTER cm 
        ON req.REQ_CURRENCY = cm.CURRENCY_CODE AND cm.FLAG = 1 -- Active Only
    WHERE req.REQ_CURRENCY <> 'INR' -- Ignore Base Currency
      AND cm.CURRENCY_RATE IS NULL;

    IF v_missing_rates > 0 THEN
        RAISE_APPLICATION_ERROR(-20001, 'CRITICAL: Active Exchange Rate missing for foreign currency in Batch. Operation Aborted.');
    END IF;

    -- BEGIN ATOMIC TRANSACTION
    BEGIN
        -- 1. Insert Transactions
        INSERT /*+ PARALLEL(GL_TRANSACTIONS, 8) */ INTO GL_TRANSACTIONS (
            TRANSACTION_ID, BATCH_ID, JOURNAL_ID, TRANSACTION_DATE, POST_DATE,
            BRANCH_CODE, CURRENCY, CGL, NARRATION, DEBIT_AMOUNT, CREDIT_AMOUNT, SOURCE_FLAG
        )
        SELECT /*+ PARALLEL(JOURNAL_REQUEST, 8) */
            GL_TRANSACTIONS_SEQ.nextval, BATCH_ID, JOURNAL_ID, NVL(REQ_CSV_DATE, TRUNC(SYSDATE)), SYSTIMESTAMP,
            REQ_BRANCH_CODE, REQ_CURRENCY, REQ_CGL, REQ_NARRATION,
            CASE WHEN REQ_AMOUNT > 0 THEN REQ_AMOUNT ELSE 0 END, 
            CASE WHEN REQ_AMOUNT < 0 THEN ABS(REQ_AMOUNT) ELSE 0 END, 
            'J'
        FROM JOURNAL_REQUEST
        WHERE BATCH_ID = p_batch_id AND REQ_STATUS = 'P';

        -- 2. Merge Balances (STRICT RATE LOGIC)
        MERGE /*+ PARALLEL(target, 8) */ INTO GL_BALANCE target
        USING (
            SELECT /*+ PARALLEL(j, 8) */
                j.REQ_BRANCH_CODE, j.REQ_CURRENCY, j.REQ_CGL, NVL(j.REQ_CSV_DATE, TRUNC(SYSDATE)) as BAL_DATE,
                SUM(j.REQ_AMOUNT) as TXN_AMOUNT,
                -- Logic: If INR, Rate is 1. If Foreign, Rate MUST exist (Checked in Step 0).
                CASE 
                    WHEN j.REQ_CURRENCY = 'INR' THEN 1 
                    ELSE MAX(c.CURRENCY_RATE) 
                END as EXCH_RATE 
            FROM JOURNAL_REQUEST j
            LEFT JOIN CURRENCY_MASTER c ON j.REQ_CURRENCY = c.CURRENCY_CODE AND c.FLAG = 1 
            WHERE j.BATCH_ID = p_batch_id AND j.REQ_STATUS = 'P'
            GROUP BY j.REQ_BRANCH_CODE, j.REQ_CURRENCY, j.REQ_CGL, j.REQ_CSV_DATE
        ) source
        ON (target.BRANCH_CODE = source.REQ_BRANCH_CODE AND target.CURRENCY = source.REQ_CURRENCY AND target.CGL = source.REQ_CGL AND target.BALANCE_DATE = source.BAL_DATE)
        WHEN MATCHED THEN
            UPDATE SET target.BALANCE = target.BALANCE + source.TXN_AMOUNT, target.INR_BALANCE = NVL(target.INR_BALANCE, 0) + (source.TXN_AMOUNT * source.EXCH_RATE)
        WHEN NOT MATCHED THEN
            INSERT (ID, BALANCE_DATE, BRANCH_CODE, CURRENCY, CGL, BALANCE, INR_BALANCE)
            VALUES (GL_BALANCE_SEQ.nextval, source.BAL_DATE, source.REQ_BRANCH_CODE, source.REQ_CURRENCY, source.REQ_CGL, source.TXN_AMOUNT, (source.TXN_AMOUNT * source.EXCH_RATE));

        -- 3. Update Status
        UPDATE /*+ PARALLEL(JOURNAL_REQUEST, 8) */ JOURNAL_REQUEST
        SET REQ_STATUS = p_status, EXECUTOR_ID = p_executor_id, EXECUTION_DATE = SYSDATE, EXECUTOR_REMARKS = p_remarks
        WHERE BATCH_ID = p_batch_id AND REQ_STATUS = 'P';

        COMMIT; 

        -- 4. Return Cursor
        OPEN o_cursor FOR
        SELECT 
            j.REQ_BRANCH_CODE AS BRANCH, j.REQ_CURRENCY AS CURRENCY, j.REQ_CGL AS CGL, j.REQ_CSV_DATE AS BAL_DATE,
            g.BALANCE AS NEW_BALANCE, g.INR_BALANCE AS NEW_INR_BALANCE
        FROM (SELECT DISTINCT REQ_BRANCH_CODE, REQ_CURRENCY, REQ_CGL, REQ_CSV_DATE FROM JOURNAL_REQUEST WHERE BATCH_ID = p_batch_id) j
        JOIN GL_BALANCE g ON g.BRANCH_CODE = j.REQ_BRANCH_CODE AND g.CURRENCY = j.REQ_CURRENCY AND g.CGL = j.REQ_CGL AND g.BALANCE_DATE = j.REQ_CSV_DATE;
    
    EXCEPTION
        WHEN OTHERS THEN
            ROLLBACK;
            RAISE; 
    END;
END;
/

commit;






---------------------------------------


-- Table to hold batches where HDFS Sync failed
CREATE TABLE HDFS_SYNC_RETRY_QUEUE (
    BATCH_ID      VARCHAR2(50) PRIMARY KEY,
    CREATED_AT    TIMESTAMP DEFAULT SYSTIMESTAMP,
    RETRY_COUNT   NUMBER DEFAULT 0,
    STATUS        VARCHAR2(20) DEFAULT 'PENDING' -- PENDING, FAILED, COMPLETED
);

-- Index for the Scheduler to pick up pending items quickly
CREATE INDEX IDX_HDFS_RETRY_STATUS ON HDFS_SYNC_RETRY_QUEUE(STATUS);
commit;




-----------------------------------------------------------


-- 1. Create the Lightweight Summary Table
CREATE TABLE JOURNAL_BATCH_MASTER (
    BATCH_ID          VARCHAR2(50) PRIMARY KEY,
    CREATOR_ID        VARCHAR2(50),
    REQ_DATE          TIMESTAMP,
    BATCH_REMARKS     VARCHAR2(200),
    TOTAL_ROWS        NUMBER DEFAULT 0,
    TOTAL_DEBIT       NUMBER(25, 4) DEFAULT 0,
    TOTAL_CREDIT      NUMBER(25, 4) DEFAULT 0,
    BATCH_STATUS      VARCHAR2(20), -- PENDING, ACCEPTED, REJECTED, DELETED
    EXECUTOR_ID       VARCHAR2(50),
    EXECUTION_DATE    TIMESTAMP,
    EXECUTOR_REMARKS  VARCHAR2(200)
);

-- 2. Create Index for super-fast dashboard filtering
CREATE INDEX IDX_JBM_STATUS_CREATOR ON JOURNAL_BATCH_MASTER(BATCH_STATUS, CREATOR_ID);

commit;


-- 3. MIGRATE EXISTING DATA (One-time fix for your 5.75 Lakh rows)
-- This aggregates your existing rows into the new master table.
INSERT INTO JOURNAL_BATCH_MASTER (
    BATCH_ID, CREATOR_ID, REQ_DATE, BATCH_REMARKS, 
    TOTAL_ROWS, TOTAL_DEBIT, TOTAL_CREDIT, BATCH_STATUS,
    EXECUTOR_ID, EXECUTION_DATE, EXECUTOR_REMARKS
)
SELECT 
    BATCH_ID,
    MAX(CREATOR_ID),
    MAX(REQ_DATE),
    MAX(COMMON_BATCH_REMARKS),
    COUNT(*),
    SUM(CASE WHEN REQ_AMOUNT > 0 THEN REQ_AMOUNT ELSE 0 END),
    SUM(CASE WHEN REQ_AMOUNT < 0 THEN ABS(REQ_AMOUNT) ELSE 0 END),
    CASE 
        WHEN MAX(REQ_STATUS) = 'P' THEN 'PENDING'
        WHEN MAX(REQ_STATUS) = 'A' THEN 'ACCEPTED'
        WHEN MAX(REQ_STATUS) = 'R' THEN 'REJECTED'
        ELSE MAX(REQ_STATUS)
    END,
    MAX(EXECUTOR_ID),
    MAX(EXECUTION_DATE),
    MAX(EXECUTOR_REMARKS)
FROM JOURNAL_REQUEST
GROUP BY BATCH_ID;

COMMIT;











//--------------------------------------------------------------------------------
Instructions : 
Go through with this appl,ication codes and understand everything very carefully. I have tried to optimise create, accept, fetch apis and their implementations for better performaqnce in code and also in db level, kindly deep dive
into the code to check all things are correct or not and i havwe added logs also to check the timings for create, accept etc. please review carefully.

// :::------> in above code  few code files are not in use may be because that functionality is not impplmenetd yet As you can see redis is there but while validation api call, no redis usage may be there. Check this if correct code can handle large data set like 6lakhs rows or 10 lakh rows or not.
Also once master data service is there to fetch validation data at a time to avoid 6 lakh db calls to validate 6 lakh rows. fetch all data at first store then validate, if this is already impemnted then fine, otherwise we have to implement this for fast check.

--->  create is taking 5 minutes for creating request of a file consist 6 lakh rows, files. this is heavy. 

// logs :
// after validation create request :
2026-01-08T15:52:02.005+05:30  INFO 25432 --- [JournalService] [ JournalAsync-4] c.f.J.S.JournalBulkValidationService     : Starting Async Validation for ReqID: ef787b0b-1dde-4b98-9e96-bbcf9155b6f7
2026-01-08T15:52:20.303+05:30  INFO 25432 --- [JournalService] [ JournalAsync-5] c.f.J.Service.JournalRequestServiceImpl  : ASYNC CREATE: Starting Batch 0001272 (Source: ef787b0b-1dde-4b98-9e96-bbcf9155b6f7)
2026-01-08T15:57:07.304+05:30  INFO 25432 --- [JournalService] [ JournalAsync-5] c.f.J.Service.NotificationWriterService  : Saved 1-to-many notification event for role: 51,11,10 (AggregateID: 0001272)
2026-01-08T15:57:07.339+05:30  INFO 25432 --- [JournalService] [ JournalAsync-5] c.f.J.Service.JournalRequestServiceImpl  : Batch 0001272 Created. Time: 287036ms





after accept button click first : 
calling this api : https://10.0.19.101:1000/JS/api/journals/process-bulk
response:
{
    "status": "PROCESSING",
    "message": "Approval process started. Notifications will be sent upon completion."
}

logs: 
2026-01-08T15:40:47.218+05:30  INFO 25432 --- [JournalService] [nio-9999-exec-3] c.f.J.Service.JournalRequestServiceImpl  : Initiating Async Process for Batch: 0001269
2026-01-08T15:40:47.245+05:30  INFO 25432 --- [JournalService] [ JournalAsync-3] c.f.J.Service.JournalRequestServiceImpl  : Calling Oracle Procedure for Batch 0001269

- > then immediately caling https://10.0.19.101:1000/JS/api/journals/pending-requests-summary
repponse : [
    {
        "requestCount": 1500,
        "totalCredit": 3649516.42,
        "commonBatchRemarks": "Bulk Upload - 2025-12-17",
        "creatorId": "2488766",
        "requestDate": "2026-01-08T10:04:31.426+00:00",
        "totalDebit": 3649516.42,
        "batchId": "0001269",
        "requestStatus": "PENDING"
    }
]

now the issue is the background process has started for this request but status is pending until the request is processed fully and in fe if any user clicks again in accept then what will happen.
also for multiple user case thos euser who can accept that same request - already one user accpted that request but in another users screen it still comiung as pending ubntil it is completed. so we have to handle this case very carefullty.

-> no logs are present for succesfull processed in be for accept case. not getting when the data processed fully and correctly and how much it takes. you can comment the hdfs save for now as hdfs not running in my local so it will give error. will check the hdfs later. focus on only oracle for now.



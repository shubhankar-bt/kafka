You are referring to Druid's Native Batch Ingestion (specifically the druid-hdfs-storage extension). I have already researched this and it is the standard architectural pattern for this scale.





Why this works for us:

1. Direct Integration: Druid acts as a 'Pull' system. As soon as Aditya's Spark Streaming job finalizes a batch of Parquet files in HDFS, Druid can ingest them directly. We don't need an intermediate layer like Kafka between HDFS and Druid.

2. Performance: Since the data is landing in the Data Lake (HDFS) via Spark, we can configure Druid to use Parallel Ingestion. This means Druid will spin up multiple worker threads to read those HDFS files simultaneously, which will handle the 1-2 Crore volume very efficiently.




Next Steps:
I am already setting up the Ingestion Spec (JSON) to test this against the HDFS path. Once Aditya provides the HDFS location where Spark Streaming is landing the files, I can trigger the load immediately."

# Custom Values for enabling HDFS on Druid in K8s

# 1. Enable the HDFS Extension
extensions:
  loadList:
    - "druid-hdfs-storage"
    - "druid-histogram"
    - "druid-datasketches"

# 2. Mount the HDFS ConfigMap we created in Step 2
# This puts core-site.xml and hdfs-site.xml into /opt/druid/conf/hadoop/
extraVolumes:
  - name: hdfs-config
    configMap:
      name: druid-hdfs-config

extraVolumeMounts:
  - name: hdfs-config
    mountPath: /opt/druid/conf/hadoop
    readOnly: true

# 3. Configure Druid Common Runtime to use HDFS
env:
  # Tell Druid to look in the mount path for hadoop configs
  HADOOP_CONF_DIR: "/opt/druid/conf/hadoop"
  
  # Set Deep Storage to HDFS
  druid_storage_type: "hdfs"
  druid_storage_storageDirectory: "/druid/deepstorage"
  
  # Basic memory tuning for small dev env
  druid_indexer_runner_javaOptsArray: '["-server", "-Xmx1g", "-Xms1g", "-XX:MaxDirectMemorySize=1g"]'

# 4. Service Configuration (Standard)
coordinator:
  enabled: true
  resources: 
    requests:
      memory: "1Gi"
      cpu: "500m"

broker:
  enabled: true
  resources: 
    requests:
      memory: "1Gi"
      cpu: "500m"

historical:
  enabled: true
  resources: 
    requests:
      memory: "2Gi"  # Historical needs more RAM for segments
      cpu: "1000m"

middleManager:
  enabled: true
  resources: 
    requests:
      memory: "2Gi"  # Ingestion requires RAM
      cpu: "1000m"
  
router:
  enabled: true















Hi [DevOps Lead Name],
I am working on the "HDFS to Druid Data Pipeline" task.
I need access to an Apache Druid instance in our K8s Development cluster to prototype the ingestion.
If one does not exist, could you please deploy one?
Here are the specific requirements for the deployment:
1. Extension Required: druid-hdfs-storage must be included in druid.extensions.loadList.
2. Connectivity: The Druid pods must have the HDFS configuration files (core-site.xml, hdfs-site.xml) mounted on their classpath (e.g., in /etc/hadoop/conf or via HADOOP_CONF_DIR).
3. Deep Storage: Configure druid.storage.type=hdfs pointing to a writable HDFS path (e.g., /druid/deepstorage).
I have prepared a sample Helm values.yaml if you need a reference for the configuration.
Thanks,
[Your Name]

CREATE OR REPLACE PROCEDURE PROCESS_JOURNAL_BATCH (
    p_batch_id      IN VARCHAR2,
    p_executor_id   IN VARCHAR2,
    p_remarks       IN VARCHAR2,
    p_status        IN VARCHAR2, 
    o_cursor        OUT SYS_REFCURSOR
) AS
BEGIN
    -- Enable Parallel DML
    EXECUTE IMMEDIATE 'ALTER SESSION ENABLE PARALLEL DML';

    BEGIN
        -- 1. Insert Transactions
        INSERT /*+ PARALLEL(GL_TRANSACTIONS, 8) */ INTO GL_TRANSACTIONS (
            TRANSACTION_ID, BATCH_ID, JOURNAL_ID, TRANSACTION_DATE, POST_DATE,
            BRANCH_CODE, CURRENCY, CGL, NARRATION, DEBIT_AMOUNT, CREDIT_AMOUNT, SOURCE_FLAG
        )
        SELECT /*+ PARALLEL(JOURNAL_REQUEST, 8) */
            GL_TRANSACTIONS_SEQ.nextval, BATCH_ID, JOURNAL_ID, NVL(REQ_CSV_DATE, TRUNC(SYSDATE)), SYSTIMESTAMP,
            REQ_BRANCH_CODE, REQ_CURRENCY, REQ_CGL, REQ_NARRATION,
            CASE WHEN REQ_AMOUNT > 0 THEN REQ_AMOUNT ELSE 0 END, 
            CASE WHEN REQ_AMOUNT < 0 THEN ABS(REQ_AMOUNT) ELSE 0 END, 
            'J'
        FROM JOURNAL_REQUEST
        WHERE BATCH_ID = p_batch_id AND REQ_STATUS = 'P';

        -- 2. Merge Balances
        MERGE /*+ PARALLEL(target, 8) */ INTO GL_BALANCE target
        USING (
            SELECT /*+ PARALLEL(j, 8) */
                j.REQ_BRANCH_CODE, j.REQ_CURRENCY, j.REQ_CGL, NVL(j.REQ_CSV_DATE, TRUNC(SYSDATE)) as BAL_DATE,
                SUM(j.REQ_AMOUNT) as TXN_AMOUNT, NVL(MAX(c.CURRENCY_RATE), 1) as EXCH_RATE 
            FROM JOURNAL_REQUEST j
            LEFT JOIN CURRENCY_MASTER c ON j.REQ_CURRENCY = c.CURRENCY_CODE AND c.FLAG = 1 
            WHERE j.BATCH_ID = p_batch_id AND j.REQ_STATUS = 'P'
            GROUP BY j.REQ_BRANCH_CODE, j.REQ_CURRENCY, j.REQ_CGL, j.REQ_CSV_DATE
        ) source
        ON (target.BRANCH_CODE = source.REQ_BRANCH_CODE AND target.CURRENCY = source.REQ_CURRENCY AND target.CGL = source.REQ_CGL AND target.BALANCE_DATE = source.BAL_DATE)
        WHEN MATCHED THEN
            UPDATE SET target.BALANCE = target.BALANCE + source.TXN_AMOUNT, target.INR_BALANCE = NVL(target.INR_BALANCE, 0) + (source.TXN_AMOUNT * source.EXCH_RATE)
        WHEN NOT MATCHED THEN
            INSERT (ID, BALANCE_DATE, BRANCH_CODE, CURRENCY, CGL, BALANCE, INR_BALANCE)
            VALUES (GL_BALANCE_SEQ.nextval, source.BAL_DATE, source.REQ_BRANCH_CODE, source.REQ_CURRENCY, source.REQ_CGL, source.TXN_AMOUNT, (source.TXN_AMOUNT * source.EXCH_RATE));

        -- 3. Update Status
        UPDATE /*+ PARALLEL(JOURNAL_REQUEST, 8) */ JOURNAL_REQUEST
        SET REQ_STATUS = p_status, EXECUTOR_ID = p_executor_id, EXECUTION_DATE = SYSDATE, EXECUTOR_REMARKS = p_remarks
        WHERE BATCH_ID = p_batch_id AND REQ_STATUS = 'P';

        COMMIT; -- Final Commit

        -- 4. Return Cursor (For Sync)
        OPEN o_cursor FOR
        SELECT 
            j.REQ_BRANCH_CODE AS BRANCH, j.REQ_CURRENCY AS CURRENCY, j.REQ_CGL AS CGL, j.REQ_CSV_DATE AS BAL_DATE,
            g.BALANCE AS NEW_BALANCE, g.INR_BALANCE AS NEW_INR_BALANCE
        FROM (SELECT DISTINCT REQ_BRANCH_CODE, REQ_CURRENCY, REQ_CGL, REQ_CSV_DATE FROM JOURNAL_REQUEST WHERE BATCH_ID = p_batch_id) j
        JOIN GL_BALANCE g ON g.BRANCH_CODE = j.REQ_BRANCH_CODE AND g.CURRENCY = j.REQ_CURRENCY AND g.CGL = j.REQ_CGL AND g.BALANCE_DATE = j.REQ_CSV_DATE;
    
    EXCEPTION
        WHEN OTHERS THEN
            ROLLBACK; -- CRITICAL: If any line above fails, undo EVERYTHING
            RAISE; -- Throw error back to Java so we know it failed
    END;
END;
/











// ... (Previous imports remain same)

    // *** LOCATE THIS METHOD IN YOUR FILE AND UPDATE THE CATCH BLOCK ***
    @Async("bulkExecutor")
    @Transactional
    public void executeAsyncBatchProcessing(BulkProcessJournalRequestDto dto, String executorId) {
        String batchId = dto.getBatchId();
        
        try {
            if (RequestStatus.ACCEPTED.equals(dto.getStatus())) {
                List<HdfsSyncDto> syncData = null;

                // 1. ORACLE TRANSACTION (Primary Source of Truth)
                try {
                    syncData = jdbcTemplate.execute(
                        "{call PROCESS_JOURNAL_BATCH(?, ?, ?, ?, ?)}",
                        (CallableStatementCallback<List<HdfsSyncDto>>) cs -> {
                            // ... (Same as before)
                            cs.setString(1, batchId);
                            cs.setString(2, executorId);
                            cs.setString(3, dto.getRemarks());
                            cs.setString(4, dto.getStatus().getCode());
                            cs.registerOutParameter(5, -10);
                            cs.execute();
                            List<HdfsSyncDto> list = new ArrayList<>();
                            try (ResultSet rs = (ResultSet) cs.getObject(5)) {
                                while (rs.next()) {
                                    list.add(new HdfsSyncDto(
                                        rs.getString("BRANCH"), rs.getString("CURRENCY"), rs.getString("CGL"),
                                        rs.getDate("BAL_DATE").toLocalDate(),
                                        rs.getBigDecimal("NEW_BALANCE"), rs.getBigDecimal("NEW_INR_BALANCE")
                                    ));
                                }
                            }
                            return list;
                        }
                    );
                    logAudit(executorId, "APPROVE_DB_SUCCESS", "BATCH_ASYNC", "DB Committed for " + batchId);
                } catch (Exception e) {
                    log.error("DB TRANSACTION FAILED", e);
                    logAudit(executorId, "APPROVE_DB_FAIL", "BATCH_ASYNC", "DB Failed: " + e.getMessage());
                    throw e; // Stop here.
                }

                // 2. HDFS TRANSACTION (Secondary)
                try {
                    hdfsSyncService.syncToDataLake(syncData);
                } catch (Exception e) {
                    log.error("HDFS SYNC FAILED for Batch {} (DB Committed). Queuing for Retry.", batchId, e);
                    
                    // *** SAFETY NET: Insert into Retry Queue ***
                    // This ensures the Background Recovery Service picks it up in 5 minutes.
                    try {
                        jdbcTemplate.update("INSERT INTO HDFS_SYNC_RETRY_QUEUE (BATCH_ID, STATUS) VALUES (?, 'PENDING')", batchId);
                        logAudit(executorId, "HDFS_SYNC_QUEUED", "BATCH_ASYNC", "Queued for Recovery: " + batchId);
                    } catch (Exception ex) {
                        log.error("CRITICAL: Failed to queue batch for retry!", ex);
                    }
                }
                
            } else if (RequestStatus.REJECTED.equals(dto.getStatus())) {
                // ... (Same as before)
                 String sql = "UPDATE JOURNAL_REQUEST SET REQ_STATUS = ?, EXECUTOR_ID = ?, EXECUTOR_REMARKS = ?, EXECUTION_DATE = SYSDATE WHERE BATCH_ID = ? AND REQ_STATUS = 'P'";
                jdbcTemplate.update(sql, dto.getStatus().getCode(), executorId, dto.getRemarks(), batchId);
                logAudit(executorId, "REJECT_OK", "BATCH_ASYNC", "Rejected " + batchId);
            }
        } catch (Exception e) {
            log.error("FATAL ERROR in Async Process", e);
        }
    }
    // ... (Rest of file remains same)














--------------------













-- Table to hold batches where HDFS Sync failed
CREATE TABLE HDFS_SYNC_RETRY_QUEUE (
    BATCH_ID      VARCHAR2(50) PRIMARY KEY,
    CREATED_AT    TIMESTAMP DEFAULT SYSTIMESTAMP,
    RETRY_COUNT   NUMBER DEFAULT 0,
    STATUS        VARCHAR2(20) DEFAULT 'PENDING' -- PENDING, FAILED, COMPLETED
);

-- Index for the Scheduler to pick up pending items quickly
CREATE INDEX IDX_HDFS_RETRY_STATUS ON HDFS_SYNC_RETRY_QUEUE(STATUS);
















package com.fincore.JournalService.Service;

import com.fincore.JournalService.Dto.HdfsSyncDto;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.scheduling.annotation.Scheduled;
import org.springframework.stereotype.Service;

import java.util.List;

@Service
@RequiredArgsConstructor
@Slf4j
public class HdfsRecoveryService {

    private final HdfsSyncService hdfsSyncService;

    @Autowired
    @Qualifier("oracleJdbcTemplate")
    private JdbcTemplate oracleJdbcTemplate;

    // Run every 5 minutes (300,000 ms)
    @Scheduled(fixedDelay = 300000)
    public void retryFailedHdfsSyncs() {
        // 1. Fetch PENDING batches
        String fetchSql = "SELECT BATCH_ID FROM HDFS_SYNC_RETRY_QUEUE WHERE STATUS = 'PENDING' AND RETRY_COUNT < 5";
        List<String> failedBatches = oracleJdbcTemplate.query(fetchSql, (rs, rowNum) -> rs.getString("BATCH_ID"));

        if (failedBatches.isEmpty()) return;

        log.info("HDFS RECOVERY: Found {} failed batches. Starting self-healing...", failedBatches.size());

        for (String batchId : failedBatches) {
            try {
                processRetry(batchId);
            } catch (Exception e) {
                log.error("HDFS RECOVERY: Failed to recover batch {}", batchId, e);
                // Increment retry count
                oracleJdbcTemplate.update("UPDATE HDFS_SYNC_RETRY_QUEUE SET RETRY_COUNT = RETRY_COUNT + 1 WHERE BATCH_ID = ?", batchId);
            }
        }
    }

    private void processRetry(String batchId) {
        // 2. SELF-HEALING QUERY
        // Instead of using old data, we join JOURNAL_REQUEST with GL_BALANCE
        // to get the CURRENT REAL-TIME balance from Oracle.
        // This fixes the "Race Condition" issue.
        String freshDataSql = """
            SELECT 
                j.REQ_BRANCH_CODE AS BRANCH, 
                j.REQ_CURRENCY AS CURRENCY, 
                j.REQ_CGL AS CGL, 
                j.REQ_CSV_DATE AS BAL_DATE,
                g.BALANCE AS NEW_BALANCE,
                g.INR_BALANCE AS NEW_INR_BALANCE
            FROM (
                SELECT DISTINCT REQ_BRANCH_CODE, REQ_CURRENCY, REQ_CGL, REQ_CSV_DATE 
                FROM JOURNAL_REQUEST 
                WHERE BATCH_ID = ?
            ) j
            JOIN GL_BALANCE g ON 
                g.BRANCH_CODE = j.REQ_BRANCH_CODE AND 
                g.CURRENCY = j.REQ_CURRENCY AND 
                g.CGL = j.REQ_CGL AND 
                g.BALANCE_DATE = j.REQ_CSV_DATE
        """;

        List<HdfsSyncDto> freshSyncData = oracleJdbcTemplate.query(freshDataSql, 
            (rs, rowNum) -> new HdfsSyncDto(
                rs.getString("BRANCH"),
                rs.getString("CURRENCY"),
                rs.getString("CGL"),
                rs.getDate("BAL_DATE").toLocalDate(),
                rs.getBigDecimal("NEW_BALANCE"),
                rs.getBigDecimal("NEW_INR_BALANCE")
            ), batchId);

        if (!freshSyncData.isEmpty()) {
            // 3. Push the LATEST/CURRENT balance to HDFS
            hdfsSyncService.syncToDataLake(freshSyncData);
            log.info("HDFS RECOVERY: Successfully synced batch {}. Data is now consistent.", batchId);
        }

        // 4. Mark as Completed
        oracleJdbcTemplate.update("UPDATE HDFS_SYNC_RETRY_QUEUE SET STATUS = 'COMPLETED' WHERE BATCH_ID = ?", batchId);
    }
}










// ... (Previous imports remain same)

    // *** LOCATE THIS METHOD IN YOUR FILE AND UPDATE THE CATCH BLOCK ***
    @Async("bulkExecutor")
    @Transactional
    public void executeAsyncBatchProcessing(BulkProcessJournalRequestDto dto, String executorId) {
        String batchId = dto.getBatchId();
        
        try {
            if (RequestStatus.ACCEPTED.equals(dto.getStatus())) {
                List<HdfsSyncDto> syncData = null;

                // 1. ORACLE TRANSACTION (Primary Source of Truth)
                try {
                    syncData = jdbcTemplate.execute(
                        "{call PROCESS_JOURNAL_BATCH(?, ?, ?, ?, ?)}",
                        (CallableStatementCallback<List<HdfsSyncDto>>) cs -> {
                            // ... (Same as before)
                            cs.setString(1, batchId);
                            cs.setString(2, executorId);
                            cs.setString(3, dto.getRemarks());
                            cs.setString(4, dto.getStatus().getCode());
                            cs.registerOutParameter(5, -10);
                            cs.execute();
                            List<HdfsSyncDto> list = new ArrayList<>();
                            try (ResultSet rs = (ResultSet) cs.getObject(5)) {
                                while (rs.next()) {
                                    list.add(new HdfsSyncDto(
                                        rs.getString("BRANCH"), rs.getString("CURRENCY"), rs.getString("CGL"),
                                        rs.getDate("BAL_DATE").toLocalDate(),
                                        rs.getBigDecimal("NEW_BALANCE"), rs.getBigDecimal("NEW_INR_BALANCE")
                                    ));
                                }
                            }
                            return list;
                        }
                    );
                    logAudit(executorId, "APPROVE_DB_SUCCESS", "BATCH_ASYNC", "DB Committed for " + batchId);
                } catch (Exception e) {
                    log.error("DB TRANSACTION FAILED", e);
                    logAudit(executorId, "APPROVE_DB_FAIL", "BATCH_ASYNC", "DB Failed: " + e.getMessage());
                    throw e; // Stop here.
                }

                // 2. HDFS TRANSACTION (Secondary)
                try {
                    hdfsSyncService.syncToDataLake(syncData);
                } catch (Exception e) {
                    log.error("HDFS SYNC FAILED for Batch {} (DB Committed). Queuing for Retry.", batchId, e);
                    
                    // *** SAFETY NET: Insert into Retry Queue ***
                    // This ensures the Background Recovery Service picks it up in 5 minutes.
                    try {
                        jdbcTemplate.update("INSERT INTO HDFS_SYNC_RETRY_QUEUE (BATCH_ID, STATUS) VALUES (?, 'PENDING')", batchId);
                        logAudit(executorId, "HDFS_SYNC_QUEUED", "BATCH_ASYNC", "Queued for Recovery: " + batchId);
                    } catch (Exception ex) {
                        log.error("CRITICAL: Failed to queue batch for retry!", ex);
                    }
                }
                
            } else if (RequestStatus.REJECTED.equals(dto.getStatus())) {
                // ... (Same as before)
                 String sql = "UPDATE JOURNAL_REQUEST SET REQ_STATUS = ?, EXECUTOR_ID = ?, EXECUTOR_REMARKS = ?, EXECUTION_DATE = SYSDATE WHERE BATCH_ID = ? AND REQ_STATUS = 'P'";
                jdbcTemplate.update(sql, dto.getStatus().getCode(), executorId, dto.getRemarks(), batchId);
                logAudit(executorId, "REJECT_OK", "BATCH_ASYNC", "Rejected " + batchId);
            }
        } catch (Exception e) {
            log.error("FATAL ERROR in Async Process", e);
        }
    }
    // ... (Rest of file remains same)









_____________

    @Scheduled(fixedDelay = 300000) // Runs every 5 mins
    public void retryFailedHdfsSyncs() {
        // OPTIMIZATION: Process only 50 at a time to prevent system overload during recovery
        // If there are more, the next 5-min run will pick them up.
        String fetchSql = "SELECT BATCH_ID FROM HDFS_SYNC_RETRY_QUEUE WHERE STATUS = 'PENDING' AND RETRY_COUNT < 5 FETCH FIRST 50 ROWS ONLY";
        
        List<String> failedBatches = oracleJdbcTemplate.query(fetchSql, (rs, rowNum) -> rs.getString("BATCH_ID"));

        if (failedBatches.isEmpty()) {
            return; // 99% of the time, code stops here. Instant return.
        }

        log.info("HDFS RECOVERY: Found {} failed batches...", failedBatches.size());
        // ... rest of logic
    }





***************************************
*******************************************



validation speed up :: 










package com.fincore.JournalService.config;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.redis.connection.RedisConnectionFactory;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.data.redis.serializer.StringRedisSerializer;

/**
 * Configuration for Redis to handle compressed binary data.
 * This prevents Out-Of-Memory errors by offloading large datasets to Redis.
 */
@Configuration
public class RedisConfig {

    @Bean(name = "byteArrayRedisTemplate")
    public RedisTemplate<String, byte[]> byteArrayRedisTemplate(RedisConnectionFactory connectionFactory) {
        RedisTemplate<String, byte[]> template = new RedisTemplate<>();
        template.setConnectionFactory(connectionFactory);
        
        // Keys are Strings (e.g., "JRNL_DATA_UUID") - Readable in Redis CLI
        template.setKeySerializer(new StringRedisSerializer());
        
        // Values are raw Byte Arrays (GZIP Compressed Data) - No serialization overhead
        template.setEnableDefaultSerializer(false);
        
        return template;
    }
}














package com.fincore.JournalService.Service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.stereotype.Service;

import java.util.HashSet;
import java.util.List;
import java.util.Set;

/**
 * Helper service to fetch Master Data for bulk validation.
 * Fetches entire active sets to allow O(1) in-memory validation.
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class ValidationMasterService {

    @Autowired
    @Qualifier("oracleJdbcTemplate")
    private JdbcTemplate jdbcTemplate;

    public Set<String> getAllActiveBranches() {
        long start = System.currentTimeMillis();
        // Ensure table names match your Oracle DB
        String sql = "SELECT BRANCH_CODE FROM BRANCH_MASTER WHERE ACTIVE_FLAG = 'Y'"; 
        List<String> list = jdbcTemplate.query(sql, (rs, rowNum) -> rs.getString(1));
        log.info("Loaded {} Active Branches in {}ms", list.size(), System.currentTimeMillis() - start);
        return new HashSet<>(list);
    }

    public Set<String> getAllActiveCurrencies() {
        long start = System.currentTimeMillis();
        String sql = "SELECT CURRENCY_CODE FROM CURRENCY_MASTER WHERE ACTIVE_FLAG = 'Y'";
        List<String> list = jdbcTemplate.query(sql, (rs, rowNum) -> rs.getString(1));
        log.info("Loaded {} Active Currencies in {}ms", list.size(), System.currentTimeMillis() - start);
        return new HashSet<>(list);
    }

    public Set<String> getAllActiveCgls() {
        long start = System.currentTimeMillis();
        String sql = "SELECT CGL_CODE FROM CGL_MASTER WHERE ACTIVE_FLAG = 'Y'";
        List<String> list = jdbcTemplate.query(sql, (rs, rowNum) -> rs.getString(1));
        log.info("Loaded {} Active CGLs in {}ms", list.size(), System.currentTimeMillis() - start);
        return new HashSet<>(list);
    }
}



















package com.fincore.JournalService.Service;

import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fincore.JournalService.Dto.BulkUploadStateDto;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.AllArgsConstructor;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.poi.ss.usermodel.*;
import org.apache.poi.xssf.usermodel.XSSFWorkbook;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;

import java.io.*;
import java.math.BigDecimal;
import java.time.LocalDate;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.TimeUnit;
import java.util.zip.GZIPInputStream;
import java.util.zip.GZIPOutputStream;

/**
 * Service to handle high-volume Excel validation.
 * Features:
 * 1. Async Processing (Non-blocking UI).
 * 2. In-Memory Master Data Checks (No N+1 DB calls).
 * 3. Group Balance Validation (Net Zero check).
 * 4. Redis + GZIP Storage (Stateless, Low Memory).
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class JournalBulkValidationService {

    private final ValidationMasterService validationMasterService;
    private final ObjectMapper objectMapper;

    @Autowired
    @Qualifier("byteArrayRedisTemplate")
    private RedisTemplate<String, byte[]> redisTemplate;

    // Redis Keys & Config
    private static final String KEY_DATA = "JRNL_DATA::";   // Valid Data (Compressed)
    private static final String KEY_STATUS = "JRNL_STAT::"; // Status DTO (JSON)
    private static final String KEY_ERR = "JRNL_ERR::";     // Error Report (Excel Bytes)
    private static final long CACHE_TTL_MINUTES = 60;       // Auto-delete after 1 hour

    // --- DTO Definition (Inner Class for Self-Containment) ---
    @Data
    @Builder
    @NoArgsConstructor
    @AllArgsConstructor
    public static class ExcelRowData implements Serializable {
        public int rowIndex;
        public String branch;
        public String currency;
        public String cgl;
        public BigDecimal amount;
        public String txnType; // Credit / Debit
        public String remarks;
        public String productCode;
        public String sysDate; // Raw date from Excel
        public boolean isSystemFormat;
        public List<String> errors;
    }

    // ==================================================================================
    // 1. PUBLIC ENTRY POINTS
    // ==================================================================================

    /**
     * Called by Controller to start the process. Returns Request ID immediately.
     */
    public String initiateValidation(byte[] fileBytes, String fileName, LocalDate postingDate) {
        String requestId = UUID.randomUUID().toString();
        log.info("Init Validation. ReqID: {}, File: {} ({} bytes)", requestId, fileName, fileBytes.length);
        
        // Set Initial Status in Redis
        updateStatus(requestId, "QUEUED", "Waiting for processor...", 0, 0);

        // Fire Async Process
        processValidationAsync(requestId, fileBytes, fileName, postingDate);
        
        return requestId;
    }

    /**
     * Retrieve current status from Redis.
     */
    public BulkUploadStateDto getState(String reqId) {
        try {
            byte[] bytes = redisTemplate.opsForValue().get(KEY_STATUS + reqId);
            if (bytes == null) return null;
            return objectMapper.readValue(bytes, BulkUploadStateDto.class);
        } catch (Exception e) {
            log.error("Error reading status for {}", reqId, e);
            return null;
        }
    }

    /**
     * Retrieve Valid Rows (Decompresses from Redis).
     * Used by CreateBatch API.
     */
    public List<ExcelRowData> getValidRowsFromCache(String requestId) {
        byte[] bytes = redisTemplate.opsForValue().get(KEY_DATA + requestId);
        if (bytes == null) {
            log.warn("Cache miss for ReqID: {}", requestId);
            return null;
        }
        try {
            return decompress(bytes);
        } catch (IOException e) {
            log.error("Decompression failed for ReqID: {}", requestId, e);
            return Collections.emptyList();
        }
    }

    /**
     * Retrieve Error Report file.
     */
    public byte[] getFileBytes(String reqId, String type) {
        if ("ERROR".equals(type)) {
            return redisTemplate.opsForValue().get(KEY_ERR + reqId);
        }
        return null; // Can implement Success File download if needed
    }

    public byte[] generateTemplateBytes() { 
        // Implement template generation logic if needed
        return new byte[0]; 
    }

    // ==================================================================================
    // 2. ASYNC PROCESSOR
    // ==================================================================================

    @Async("bulkExecutor")
    public void processValidationAsync(String requestId, byte[] fileBytes, String fileName, LocalDate postingDate) {
        long startTime = System.currentTimeMillis();
        log.info(">>> ASYNC VALIDATION START: {}", requestId);
        updateStatus(requestId, "PROCESSING", "Initializing & Fetching Master Data...", 0, 0);

        List<ExcelRowData> validRows = new ArrayList<>();
        List<ExcelRowData> errorRows = new ArrayList<>();

        try {
            // STEP A: Fetch Master Data (Batch Fetch for Performance)
            Set<String> validBranches = validationMasterService.getAllActiveBranches();
            Set<String> validCurrencies = validationMasterService.getAllActiveCurrencies();
            Set<String> validCgls = validationMasterService.getAllActiveCgls();
            
            updateStatus(requestId, "PROCESSING", "Parsing Excel File...", 0, 0);

            // STEP B: Parse & Row-Level Validation
            try (Workbook workbook = new XSSFWorkbook(new ByteArrayInputStream(fileBytes))) {
                Sheet sheet = workbook.getSheetAt(0);
                DataFormatter formatter = new DataFormatter(); // Handles formatted cells (dates/numbers) safely
                
                int rowIdx = 0;
                for (Row row : sheet) {
                    if (rowIdx++ == 0) continue; // Skip Header
                    if (isRowEmpty(row)) continue;

                    // Parse
                    ExcelRowData data = parseRow(row, rowIdx, formatter);

                    // Validate (Level 1 & 2)
                    validateRow(data, validBranches, validCurrencies, validCgls);

                    if (!data.errors.isEmpty()) {
                        errorRows.add(data);
                    } else {
                        validRows.add(data);
                    }
                    
                    // Update Progress every 10k rows
                    if (rowIdx % 10000 == 0) {
                        updateStatus(requestId, "PROCESSING", "Processed " + rowIdx + " rows...", validRows.size(), errorRows.size());
                    }
                }
            }

            // STEP C: Group Level Balance Check (Level 3)
            // "If 10 Credit, checking 10 Debit matches" -> Net Zero Sum check per Group
            if (!validRows.isEmpty()) {
                updateStatus(requestId, "PROCESSING", "Performing Group Balance Checks...", validRows.size(), errorRows.size());
                validateGroupBalances(validRows, errorRows);
            }

            // STEP D: Final Clean up
            // Remove any rows that failed the Group Check from the Valid List
            validRows.removeAll(errorRows); 

            // STEP E: Save Result to Redis
            if (errorRows.isEmpty()) {
                // SUCCESS: Compress valid rows and store
                if (validRows.isEmpty()) {
                     updateStatus(requestId, "FAILED", "File contains no valid data.", 0, 0);
                } else {
                    byte[] compressedData = compress(validRows);
                    redisTemplate.opsForValue().set(KEY_DATA + requestId, compressedData, CACHE_TTL_MINUTES, TimeUnit.MINUTES);
                    updateStatus(requestId, "COMPLETED", "Validation Successful.", validRows.size(), 0);
                }
            } else {
                // FAILURE: Generate Error Report and store
                byte[] errorReport = generateErrorReport(errorRows);
                if (errorReport != null) {
                    redisTemplate.opsForValue().set(KEY_ERR + requestId, errorReport, CACHE_TTL_MINUTES, TimeUnit.MINUTES);
                }
                updateStatus(requestId, "FAILED", "Validation Failed. Please download the error report.", validRows.size(), errorRows.size());
            }

            log.info(">>> ASYNC VALIDATION DONE: {}. Time: {}ms. Valid: {}, Errors: {}", 
                     requestId, System.currentTimeMillis() - startTime, validRows.size(), errorRows.size());

        } catch (Exception e) {
            log.error("Fatal Validation Error for {}", requestId, e);
            updateStatus(requestId, "ERROR", "System Error: " + e.getMessage(), 0, 0);
        }
    }

    // ==================================================================================
    // 3. VALIDATION LOGIC
    // ==================================================================================

    private void validateRow(ExcelRowData row, Set<String> branches, Set<String> currencies, Set<String> cgls) {
        // 1. Mandatory Checks
        if (isEmpty(row.branch)) row.errors.add("Branch Code is mandatory");
        if (isEmpty(row.currency)) row.errors.add("Currency is mandatory");
        if (isEmpty(row.cgl)) row.errors.add("CGL is mandatory");
        if (row.amount == null) row.errors.add("Amount is missing or invalid format");
        if (isEmpty(row.txnType)) row.errors.add("Transaction Type (Credit/Debit) is mandatory");

        // 2. Master Data Existence Checks (O(1) lookup)
        if (!isEmpty(row.branch) && !branches.contains(row.branch)) {
            row.errors.add("Invalid Branch Code (Not found/Inactive)");
        }
        if (!isEmpty(row.currency) && !currencies.contains(row.currency)) {
            row.errors.add("Invalid Currency Code (Not found/Inactive)");
        }
        if (!isEmpty(row.cgl) && !cgls.contains(row.cgl)) {
            row.errors.add("Invalid CGL Code (Not found/Inactive)");
        }
        
        // 3. Transaction Type Format
        if (!isEmpty(row.txnType)) {
            String type = row.txnType.trim().toUpperCase();
            if (!type.equals("CREDIT") && !type.equals("DEBIT") && !type.equals("CR") && !type.equals("DR") && !type.equals("C") && !type.equals("D")) {
                row.errors.add("Invalid Transaction Type. Use Credit/Debit.");
            }
        }
    }

    /**
     * Checks if the Sum of Debits equals Sum of Credits for each Branch+Currency+CGL group.
     */
    private void validateGroupBalances(List<ExcelRowData> validRows, List<ExcelRowData> errorAccumulator) {
        // Key: Branch|Currency|CGL
        Map<String, BigDecimal> groupSums = new HashMap<>();
        Map<String, List<ExcelRowData>> groupRows = new HashMap<>();

        for (ExcelRowData row : validRows) {
            String key = row.branch + "|" + row.currency + "|" + row.cgl;
            
            // Normalize Amount: Credit is Negative, Debit is Positive
            BigDecimal val = row.amount;
            String type = row.txnType.trim().toUpperCase();
            if (type.startsWith("C")) { // Credit, Cr, C
                val = val.negate();
            }
            
            groupSums.merge(key, val, BigDecimal::add);
            groupRows.computeIfAbsent(key, k -> new ArrayList<>()).add(row);
        }

        // Check for Imbalance
        for (Map.Entry<String, BigDecimal> entry : groupSums.entrySet()) {
            // Compare to Zero (Use compareTo for BigDecimal safety)
            if (entry.getValue().compareTo(BigDecimal.ZERO) != 0) {
                // Imbalance found! Mark all rows in this group as errors.
                List<ExcelRowData> badRows = groupRows.get(entry.getKey());
                for (ExcelRowData r : badRows) {
                    r.errors.add("Group Balance Mismatch: Net sum for Branch/Curr/CGL is " + entry.getValue() + " (Should be 0)");
                    // Add to error list (Note: These will be removed from valid list in main method)
                    errorAccumulator.add(r);
                }
            }
        }
    }

    // ==================================================================================
    // 4. EXCEL PARSING & GENERATION
    // ==================================================================================

    private ExcelRowData parseRow(Row row, int rowIndex, DataFormatter formatter) {
        ExcelRowData data = new ExcelRowData();
        data.rowIndex = rowIndex;
        data.errors = new ArrayList<>();

        try {
            // Adjust column indices based on your Template
            // Assuming: Branch(0), Currency(1), CGL(2), Amount(3), Type(4), Remarks(5), Date(6), Product(7)
            data.branch = getCellVal(row, 0, formatter);
            data.currency = getCellVal(row, 1, formatter);
            data.cgl = getCellVal(row, 2, formatter);
            
            String amtStr = getCellVal(row, 3, formatter);
            if (amtStr != null) {
                try {
                    // Remove commas for currency formatting (e.g., "1,000.00")
                    data.amount = new BigDecimal(amtStr.replace(",", ""));
                } catch (Exception e) {
                    data.errors.add("Invalid Amount Format");
                }
            }
            
            data.txnType = getCellVal(row, 4, formatter);
            data.remarks = getCellVal(row, 5, formatter);
            
            data.sysDate = getCellVal(row, 6, formatter);
            if (!isEmpty(data.sysDate)) {
                data.isSystemFormat = true; // Flag to attempt parsing later
            }
            
            data.productCode = getCellVal(row, 7, formatter);

        } catch (Exception e) {
            log.warn("Row parse error at index {}", rowIndex, e);
            data.errors.add("Critical Error parsing row data");
        }
        return data;
    }

    private String getCellVal(Row row, int idx, DataFormatter formatter) {
        Cell c = row.getCell(idx, Row.MissingCellPolicy.RETURN_BLANK_AS_NULL);
        return c == null ? null : formatter.formatCellValue(c).trim();
    }
    
    private boolean isEmpty(String s) { return s == null || s.trim().isEmpty(); }
    
    private boolean isRowEmpty(Row row) {
        if (row == null) return true;
        for (int c = row.getFirstCellNum(); c < row.getLastCellNum(); c++) {
            Cell cell = row.getCell(c);
            if (cell != null && cell.getCellType() != CellType.BLANK && !cell.toString().trim().isEmpty()) {
                return false;
            }
        }
        return true;
    }

    private byte[] generateErrorReport(List<ExcelRowData> errorRows) {
        try (Workbook wb = new XSSFWorkbook(); ByteArrayOutputStream out = new ByteArrayOutputStream()) {
            Sheet s = wb.createSheet("Validation Errors");
            
            // Header
            Row head = s.createRow(0);
            String[] headers = {"Row No", "Errors", "Branch", "Currency", "CGL", "Amount", "Type"};
            for(int i=0; i<headers.length; i++) head.createCell(i).setCellValue(headers[i]);

            // Data
            int rowIdx = 1;
            for(ExcelRowData r : errorRows) {
                Row row = s.createRow(rowIdx++);
                row.createCell(0).setCellValue(r.rowIndex);
                row.createCell(1).setCellValue(String.join(" | ", r.errors));
                row.createCell(2).setCellValue(r.branch);
                row.createCell(3).setCellValue(r.currency);
                row.createCell(4).setCellValue(r.cgl);
                row.createCell(5).setCellValue(r.amount != null ? r.amount.toString() : "");
                row.createCell(6).setCellValue(r.txnType);
            }
            wb.write(out);
            return out.toByteArray();
        } catch(IOException e) {
            log.error("Failed to generate error report", e);
            return null;
        }
    }

    // ==================================================================================
    // 5. REDIS & COMPRESSION UTILS
    // ==================================================================================

    private void updateStatus(String id, String status, String msg, int valid, int error) {
        try {
            BulkUploadStateDto dto = new BulkUploadStateDto(status, msg, valid, error);
            // Serialize to JSON Bytes
            byte[] bytes = objectMapper.writeValueAsBytes(dto);
            redisTemplate.opsForValue().set(KEY_STATUS + id, bytes, CACHE_TTL_MINUTES, TimeUnit.MINUTES);
        } catch (Exception e) {
            log.error("Status Update Failed", e);
        }
    }

    private byte[] compress(List<ExcelRowData> rows) throws IOException {
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        try (GZIPOutputStream gzipOut = new GZIPOutputStream(baos)) {
            objectMapper.writeValue(gzipOut, rows);
        }
        return baos.toByteArray();
    }

    private List<ExcelRowData> decompress(byte[] compressed) throws IOException {
        try (GZIPInputStream gzipIn = new GZIPInputStream(new ByteArrayInputStream(compressed))) {
            return objectMapper.readValue(gzipIn, new TypeReference<List<ExcelRowData>>() {});
        }
    }
}






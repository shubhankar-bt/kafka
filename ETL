spring.application.name=ReportService

# LOGIN SERVICE KEY USED BY common-entities
jwt.secret=bWV0aGlvbnlsdGhyZW9ueWx0aHJlb255bGdsdXRhbWlueWxhbGFueWw=

spring.profiles.active=dev

# ===================================================================
# SERVER CONFIGURATION
# ===================================================================
server.port=9005

spring.datasource.driver-class-name=oracle.jdbc.OracleDriver
spring.mvc.async.request-timeout=3600000

# ===================================================================
# CUSTOM APPLICATION PROPERTIES
# ===================================================================
notification.progress.topic=notifications:progress

# ===================================================================
# SPRING DATASOURCE (ORACLE)
# ===================================================================
spring.datasource.url=jdbc:oracle:thin:@10.177.103.192:1523/fincorepdb1
spring.datasource.username=fincore
spring.datasource.password=Password#1234


# ========================== actuator =============================
management.endpoints.web.exposure.include=*
management.endpoint.health.show-details=always
info.app.name=ReportService
info.app.description=Service to manage and retrieve different reports
info.app.version=1.0.0

# ===================================================================
# JPA / HIBERNATE CONFIGURATION
# ===================================================================
spring.jpa.database-platform=org.hibernate.dialect.OracleDialect
spring.jpa.hibernate.ddl-auto=validate
spring.jpa.show-sql=false
spring.jpa.properties.hibernate.format_sql=false
logging.level.org.hibernate.SQL=OFF
logging.level.org.hibernate.orm.jdbc.bind=OFF

# ===================================================================
# HADOOP / HDFS HA CONFIGURATION
# ===================================================================
# The logical name of the cluster (Nameservice ID)
hadoop.fs.nameservice=fincoredev
hadoop.fs.user=root
glif.reports.base-path=/reports

# HA NameNode Details
hadoop.fs.ha.namenodes=nn1,nn2,nn3
hadoop.fs.ha.nn1.rpc=10.177.103.199:8022
hadoop.fs.ha.nn2.rpc=10.177.103.192:8022
hadoop.fs.ha.nn3.rpc=10.177.103.196:8022

# Redis Configuration
spring.data.redis.host=10.0.17.242
spring.data.redis.port=6379
spring.cache.type=redis



















package com.fincore.ReportService.config;

import lombok.extern.slf4j.Slf4j;
import org.apache.hadoop.fs.FileSystem;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

@Configuration
@Slf4j
public class HadoopConfig {

    @Value("${hadoop.fs.nameservice}")
    private String nameservice;

    @Value("${hadoop.fs.user}")
    private String hadoopUser;

    @Value("${hadoop.fs.ha.namenodes}")
    private String nnIds;

    @Value("${hadoop.fs.ha.nn1.rpc}")
    private String nn1Rpc;

    @Value("${hadoop.fs.ha.nn2.rpc}")
    private String nn2Rpc;

    @Value("${hadoop.fs.ha.nn3.rpc}")
    private String nn3Rpc;

    /**
     * Creates the Hadoop Configuration object with HA settings.
     * Defined as a Bean so it can be reused by the ConnectivityTester.
     */
    @Bean
    public org.apache.hadoop.conf.Configuration hadoopConfiguration() {
        log.info("Configuring HDFS HA for Nameservice: {}", nameservice);
        
        org.apache.hadoop.conf.Configuration config = new org.apache.hadoop.conf.Configuration();

        // 1. Set the default FileSystem to the logical Nameservice URI
        String authority = "hdfs://" + nameservice;
        config.set("fs.defaultFS", authority);

        // 2. Configure HA Nameservice
        config.set("dfs.nameservices", nameservice);
        config.set("dfs.ha.namenodes." + nameservice, nnIds); // "nn1,nn2,nn3"

        // 3. Configure RPC Addresses for each NameNode
        config.set("dfs.namenode.rpc-address." + nameservice + ".nn1", nn1Rpc);
        config.set("dfs.namenode.rpc-address." + nameservice + ".nn2", nn2Rpc);
        config.set("dfs.namenode.rpc-address." + nameservice + ".nn3", nn3Rpc);

        // 4. Configure Failover Provider (Critical for HA)
        // This tells the client how to switch between NameNodes
        config.set("dfs.client.failover.proxy.provider." + nameservice, 
                   "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider");

        // 5. Robustness Settings
        config.set("ipc.client.connect.timeout", "5000"); // 5s timeout
        config.set("ipc.client.rpc-timeout.ms", "30000"); // 30s timeout
        
        log.info("HDFS HA Configuration complete. NN1: {}, NN2: {}, NN3: {}", nn1Rpc, nn2Rpc, nn3Rpc);
        
        return config;
    }

    @Bean
    public FileSystem fileSystem(org.apache.hadoop.conf.Configuration config) {
        try {
            // Create the FileSystem using the HA config and the specific user
            // Note: We use the logical URI (hdfs://fincoredev) here
            URI logicalUri = new URI("hdfs://" + nameservice);
            return FileSystem.get(logicalUri, config, hadoopUser);

        } catch (IOException | InterruptedException | URISyntaxException e) {
            log.error("CRITICAL: Failed to initialize HDFS HA Connection.", e);
            throw new RuntimeException("Could not initialize Hadoop FileSystem", e);
        }
    }
}



















package com.fincore.ReportService.util;

import jakarta.annotation.PostConstruct;
import lombok.extern.slf4j.Slf4j;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Component;

import java.io.IOException;
import java.net.URI;

@Component
@Slf4j
public class HadoopConnectivityTester {

    @Value("${hadoop.fs.nameservice}")
    private String nameservice;

    @Value("${hadoop.fs.user}")
    private String hdfsUser;

    // Inject the config created in HadoopConfig.java
    @Autowired
    private Configuration hadoopConfig;

    @PostConstruct
    public void testHdfsConnection() {
        String logicalUri = "hdfs://" + nameservice;
        log.info("TEST: Attempting to connect to HDFS Cluster: {} as user: {}", logicalUri, hdfsUser);

        // We create a *separate* FileSystem instance for testing so we can close it safely
        // without affecting the main application bean.
        try (FileSystem fs = FileSystem.get(URI.create(logicalUri), hadoopConfig, hdfsUser)) {

            Path rootPath = new Path("/");
            if (fs.exists(rootPath)) {
                log.info("TEST SUCCESS: Connected to HDFS HA Cluster ({}). Root directory exists.", nameservice);
                
                // Print Active NameNode status (indirectly via operation success)
                log.info("HDFS Status: HA Mode Active. Block Size: {}", fs.getDefaultBlockSize(rootPath));
            } else {
                log.warn("TEST FAILURE: Connected, but root path '/' does not exist.");
            }

        } catch (IOException | InterruptedException e) {
            log.error("TEST ERROR: Failed to connect to HDFS Cluster.", e);
            log.info("Troubleshooting: Check if all 3 NameNodes are reachable and the Nameservice ID matches.");
        }
    }
}

















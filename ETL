        <!-- 1. LangChain4j Core -->
        <dependency>
            <groupId>dev.langchain4j</groupId>
            <artifactId>langchain4j</artifactId>
            <version>1.0.0-alpha1</version>
        </dependency>

        <!-- 2. LangChain4j Ollama (To talk to Phi-3 and Mxbai) -->
        <dependency>
            <groupId>dev.langchain4j</groupId>
            <artifactId>langchain4j-ollama</artifactId>
            <version>1.0.0-alpha1</version>
        </dependency>

        <!-- 3. LangChain4j Redis (Core API only - No Starter to prevent Bean clashes) -->
        <dependency>
            <groupId>dev.langchain4j</groupId>
            <artifactId>langchain4j-redis</artifactId>
            <version>1.0.0-alpha1</version>
        </dependency>

        <dependency>
            <groupId>com.github.ben-manes.caffeine</groupId>
            <artifactId>caffeine</artifactId>
        </dependency>


















package com.fincore.helpservice.config;

import dev.langchain4j.model.chat.ChatLanguageModel;
import dev.langchain4j.model.embedding.EmbeddingModel;
import dev.langchain4j.model.ollama.OllamaChatModel;
import dev.langchain4j.model.ollama.OllamaEmbeddingModel;
import dev.langchain4j.store.embedding.EmbeddingStore;
import dev.langchain4j.store.embedding.redis.RedisEmbeddingStore;
import dev.langchain4j.data.segment.TextSegment;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;

import java.time.Duration;

/**
 * Enterprise AI Configuration.
 * Connects the JVM to the offline Docker LLMs and Vector Database.
 */
@Configuration
public class AiConfiguration {

    @Value("${langchain4j.ollama.base-url}")
    private String ollamaBaseUrl;

    @Value("${langchain4j.ollama.chat-model.name}")
    private String chatModelName;

    @Value("${langchain4j.ollama.embedding-model.name}")
    private String embeddingModelName;

    @Value("${langchain4j.vector-store.redis.host}")
    private String redisHost;

    @Value("${langchain4j.vector-store.redis.port}")
    private Integer redisPort;

    @Value("${langchain4j.vector-store.redis.index-name}")
    private String indexName;

    @Value("${langchain4j.vector-store.redis.prefix}")
    private String prefix;

    @Value("${langchain4j.vector-store.redis.dimension:1024}")
    private Integer dimension;

    @Bean
    @Primary
    public ChatLanguageModel chatLanguageModel() {
        return OllamaChatModel.builder()
                .baseUrl(ollamaBaseUrl)
                .modelName(chatModelName)
                .temperature(0.1)
                .timeout(Duration.ofSeconds(60))
                .build();
    }

    @Bean
    @Primary
    public EmbeddingModel embeddingModel() {
        return OllamaEmbeddingModel.builder()
                .baseUrl(ollamaBaseUrl)
                .modelName(embeddingModelName)
                .timeout(Duration.ofSeconds(15))
                .build();
    }

    @Bean
    @Primary
    public EmbeddingStore<TextSegment> embeddingStore() {
        return RedisEmbeddingStore.builder()
                .host(redisHost)
                .port(redisPort)
                .dimension(dimension)
                .indexName(indexName)
                .prefix(prefix)
                .build();
    }
}














package com.fincore.helpservice.service;

import com.fincore.helpservice.dto.HelpResponseDTO;
import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import dev.langchain4j.data.embedding.Embedding;
import dev.langchain4j.data.message.AiMessage;
import dev.langchain4j.data.message.SystemMessage;
import dev.langchain4j.data.message.UserMessage;
import dev.langchain4j.data.segment.TextSegment;
import dev.langchain4j.memory.ChatMemory;
import dev.langchain4j.memory.chat.MessageWindowChatMemory;
import dev.langchain4j.model.chat.ChatLanguageModel;
import dev.langchain4j.model.embedding.EmbeddingModel;
import dev.langchain4j.store.embedding.EmbeddingMatch;
import dev.langchain4j.store.embedding.EmbeddingSearchRequest;
import dev.langchain4j.store.embedding.EmbeddingSearchResult;
import dev.langchain4j.store.embedding.EmbeddingStore;
import dev.langchain4j.store.embedding.filter.Filter;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.List;
import java.util.concurrent.TimeUnit;
import java.util.stream.Collectors;

import static dev.langchain4j.store.embedding.filter.MetadataFilterBuilder.metadataKey;

/**
 * THE RAG ENGINE (LangChain4j Implementation)
 * Orchestrates Vector Search, Dynamic Prompting, and Generative AI (Phi-3).
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class FinCoreChatAgent {

    private final ChatLanguageModel chatClient;
    private final EmbeddingModel embeddingModel;
    private final EmbeddingStore<TextSegment> vectorStore;
    private final PermissionService permissionService;
    private final ChatSessionService sessionService;
    private final HelpAnalyticsService analyticsService;

    // Session-based Chat Memory (Sessions expire after 30 minutes of inactivity.)
    private final Cache<String, ChatMemory> userMemories = Caffeine.newBuilder()
            .expireAfterAccess(30, TimeUnit.MINUTES)
            .maximumSize(10000)
            .build();

    public HelpResponseDTO handleChat(String userId, String roleId, String userMessage) {
        log.info("[AI-CHAT] Processing query for User: {}", userId);

        try {
            // 1. ESCALATION CHECK
            if (sessionService.isEscalationRequired(userId)) {
                log.warn("[AI-CHAT] User {} requires IT escalation.", userId);
                sessionService.resetStrikes(userId);
                return HelpResponseDTO.builder()
                        .responseType("ESCALATION_OFFER")
                        .botReply("I seem to be having trouble helping you today. Would you like me to raise an IT Support Ticket with your chat history attached?")
                        .build();
            }

            // 2. FETCH RBAC PERMISSIONS
            List<String> allowedPerms = permissionService.getAllAllowedPermissionIdsForRole(roleId)
                    .stream().map(String::valueOf).collect(Collectors.toList());
            allowedPerms.add("GLOBAL");

            List<String> allowedActionNames = permissionService.getAllowedActionNamesForRole(roleId);

            // 3. EXECUTE SECURE VECTOR SEARCH
            Embedding queryEmbedding = embeddingModel.embed(userMessage).content();
            Filter rbacFilter = metadataKey("permissionId").isIn(allowedPerms);

            EmbeddingSearchRequest searchRequest = EmbeddingSearchRequest.builder()
                    .queryEmbedding(queryEmbedding)
                    .maxResults(3)
                    .minScore(0.65)
                    .filter(rbacFilter)
                    .build();

            EmbeddingSearchResult<TextSegment> searchResult = vectorStore.search(searchRequest);
            List<EmbeddingMatch<TextSegment>> matches = searchResult.matches();

            // 4. CONTEXT EXTRACTION (Fluid AI Pre-check)
            boolean hasBankingContext = !matches.isEmpty();
            StringBuilder contextBuilder = new StringBuilder();
            String primaryActionLink = null;
            String primaryActionLabel = null;

            if (hasBankingContext) {
                for (EmbeddingMatch<TextSegment> match : matches) {
                    contextBuilder.append("- ").append(match.embedded().text()).append("\n\n");
                    if (primaryActionLink == null) {
                        String link = match.embedded().metadata().getString("actionLink");
                        if (!"NONE".equals(link)) {
                            primaryActionLink = link;
                            primaryActionLabel = match.embedded().metadata().getString("actionLabel");
                        }
                    }
                }
            } else {
                contextBuilder.append("NO_BANKING_CONTEXT_FOUND");
            }

            // 5. DYNAMIC PROMPT INJECTION
            String currentTime = LocalDateTime.now().format(DateTimeFormatter.ofPattern("EEEE, MMMM dd, yyyy - hh:mm a"));
            String dynamicPrompt = """
                    You are the FinCore Smart Assistant, an advanced AI banking bot developed by Shubhankar.
                    
                    Current Time: %s
                    User's Allowed Actions: %s
                    
                    CRITICAL BEHAVIORAL RULES:
                    1. BANKING QUERIES: You must ONLY answer using the exact Context provided below. Do not guess.
                    2. OUT OF SCOPE / UNKNOWN: If the answer is not in the Context, you MUST reply EXACTLY with: 'I cannot find this information in your authorized FinCore manuals. Please check your permissions or contact IT.'
                    3. SMALL TALK: If the user explicitly asks casual questions (e.g., 'hello', 'who are you', 'what time is it'), answer politely and conversationally.
                    4. ANTI-ABUSE: If the user directs abusive, offensive, or hostile language AT YOU, do NOT engage. Reply EXACTLY with: 'Please maintain a professional tone. How can I assist you with FinCore today?'
                    
                    --- BANKING CONTEXT RETRIEVED FROM DATABASE ---
                    %s
                    """.formatted(currentTime, allowedActionNames, contextBuilder.toString());

            if (!hasBankingContext) {
                dynamicPrompt += "\nWARNING: The Banking Context is EMPTY. You are ONLY allowed to execute Rule 3 (Small Talk) or Rule 4 (Anti-Abuse). If the user's message is neither of those, you MUST execute Rule 2 (Out of Scope).";
            }

            // 6. MANAGE CHAT MEMORY & CALL PHI-3
            ChatMemory memory = userMemories.get(userId, k -> MessageWindowChatMemory.withMaxMessages(5));
            memory.add(UserMessage.from(userMessage));

            log.info("[AI-CHAT] Sending Prompt to Phi-3 LLM...");
            AiMessage responseMessage = chatClient.generate(
                    SystemMessage.from(dynamicPrompt),
                    memory.messages().get(memory.messages().size() - 1)
            ).content();

            String aiResponse = responseMessage.text();
            memory.add(responseMessage);

            // 7. EVALUATE SUCCESS / STRIKES
            if (aiResponse.contains("I cannot find this information")) {
                log.info("[AI-CHAT] Hallucination Guardrail triggered.");
                sessionService.addStrikes(userId, 1);
                Long failLogId = analyticsService.logChatInteraction(userId, "AI_RAG", userMessage, "NO_MATCH", 0);
                
                return HelpResponseDTO.builder()
                        .responseType("NO_MATCH")
                        .botReply(aiResponse)
                        .logId(failLogId) // Correctly tied to NO_MATCH UI
                        .build();
            } else {
                sessionService.resetStrikes(userId);
                Long successLogId = analyticsService.logChatInteraction(userId, "AI_RAG", userMessage, "ANSWERED", 99);
                
                return HelpResponseDTO.builder()
                        .responseType("TEXT_REPLY")
                        .botReply(aiResponse)
                        .navigationLink(primaryActionLink)
                        .navigationLabel(primaryActionLabel)
                        .logId(successLogId)
                        .build();
            }

        } catch (Exception e) {
            log.error("[AI-CHAT] Critical AI Generation Failure", e);
            return HelpResponseDTO.builder()
                    .responseType("TEXT_REPLY")
                    .botReply("I am currently experiencing a cognitive delay. Please try again in a moment.")
                    .build();
        }
    }
}














package com.fincore.helpservice.service;

import com.fincore.helpservice.model.HelpQuestionEntity;
import com.fincore.helpservice.model.KnowledgeBaseEntity;
import com.fincore.helpservice.repository.HelpQuestionRepository;
import com.fincore.helpservice.repository.KnowledgeBaseRepository;
import dev.langchain4j.data.document.Document;
import dev.langchain4j.data.document.Metadata;
import dev.langchain4j.data.document.splitter.DocumentSplitters;
import dev.langchain4j.data.embedding.Embedding;
import dev.langchain4j.data.segment.TextSegment;
import dev.langchain4j.model.embedding.EmbeddingModel;
import dev.langchain4j.store.embedding.EmbeddingStore;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.boot.context.event.ApplicationReadyEvent;
import org.springframework.context.event.EventListener;
import org.springframework.stereotype.Service;

import java.util.ArrayList;
import java.util.List;

@Service
@RequiredArgsConstructor
@Slf4j
public class DocumentIngestionService {

    private final HelpQuestionRepository questionRepository;
    private final KnowledgeBaseRepository knowledgeBaseRepository;

    private final EmbeddingModel embeddingModel;
    private final EmbeddingStore<TextSegment> embeddingStore;

    @EventListener(ApplicationReadyEvent.class)
    public void ingestKnowledgeBase() {
        log.info("[AI-INGEST] Starting Knowledge Ingestion to Redis via mxbai-embed...");

        List<Document> allDocuments = new ArrayList<>();

        // 1. INGEST PROCEDURAL DATA
        List<HelpQuestionEntity> questions = questionRepository.findByIsActive("Y");
        for (HelpQuestionEntity q : questions) {
            String content = "Module: " + (q.getScreenName() != null ? q.getScreenName() : "General") +
                    "\nQuestion: " + q.getQuestionText() +
                    "\nAnswer: " + q.getAnswerContent() +
                    (q.getProTip() != null ? "\nPro Tip: " + q.getProTip() : "");

            String permId = q.getPermissionId() != null ? String.valueOf(q.getPermissionId()) : "GLOBAL";

            Metadata metadata = new Metadata()
                    .put("permissionId", permId)
                    .put("actionLink", q.getActionLink() != null ? q.getActionLink() : "NONE")
                    .put("actionLabel", q.getActionLabel() != null ? q.getActionLabel() : "NONE")
                    .put("dataType", "PROCEDURAL");

            allDocuments.add(Document.from(content, metadata));
        }

        // 2. INGEST CONCEPTUAL DATA
        List<KnowledgeBaseEntity> knowledgeBase = knowledgeBaseRepository.findByIsActive("Y");
        for (KnowledgeBaseEntity kb : knowledgeBase) {
            String content = "Topic: " + kb.getTopic() +
                    "\nInformation: " + kb.getContentParagraph();

            String permId = kb.getPermissionId() != null ? String.valueOf(kb.getPermissionId()) : "GLOBAL";

            Metadata metadata = new Metadata()
                    .put("permissionId", permId)
                    .put("actionLink", "NONE")
                    .put("actionLabel", "NONE")
                    .put("dataType", "CONCEPTUAL");

            allDocuments.add(Document.from(content, metadata));
        }

        // 3. CHUNK AND SAVE TO REDIS
        if (!allDocuments.isEmpty()) {
            List<TextSegment> segments = DocumentSplitters.recursive(300, 50).splitAll(allDocuments);
            log.info("[AI-INGEST] Chunked {} raw DB records into {} mathematical segments.", allDocuments.size(), segments.size());

            List<Embedding> embeddings = embeddingModel.embedAll(segments).content();

            for (int i = 0; i < segments.size(); i++) {
                TextSegment segment = segments.get(i);
                Embedding vector = embeddings.get(i);

                String dataType = segment.metadata().getString("dataType");
                String perm = segment.metadata().getString("permissionId");
                String deterministicId = dataType + "_" + perm + "_chunk_" + i;

                // CLEAN FIX: Standard add method (id, vector, segment)
                embeddingStore.add(deterministicId, vector, segment);
            }
            log.info("[AI-INGEST] Successfully saved/overwritten {} vectors to Redis Stack.", segments.size());

        } else {
            log.warn("[AI-INGEST] No active documents found in Oracle DB to ingest!");
        }
    }
}




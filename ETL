<dependencies>
    <!-- ... existing dependencies ... -->

    <!-- Hadoop Client for HA Support -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-client</artifactId>
        <version>3.3.6</version> <!-- MATCH YOUR CLUSTER VERSION -->
        <exclusions>
            <exclusion>
                <groupId>org.slf4j</groupId>
                <artifactId>slf4j-log4j12</artifactId>
            </exclusion>
            <exclusion>
                <groupId>log4j</groupId>
                <artifactId>log4j</artifactId>
            </exclusion>
            <exclusion>
                <groupId>javax.servlet</groupId>
                <artifactId>servlet-api</artifactId>
            </exclusion>
        </exclusions>
    </dependency>
    
    <!-- Required for HA Failover Proxy Provider -->
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-common</artifactId>
        <version>3.3.6</version>
    </dependency>
    
    <dependency>
        <groupId>org.apache.hadoop</groupId>
        <artifactId>hadoop-hdfs-client</artifactId>
        <version>3.3.6</version>
    </dependency>

</dependencies>











package com.fincore.ReportService.config;

import lombok.extern.slf4j.Slf4j;
import org.apache.hadoop.fs.FileSystem;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

import java.io.IOException;
import java.net.URI;
import java.net.URISyntaxException;

@Configuration
@Slf4j
public class HadoopConfig {

    @Value("${hadoop.fs.nameservice}")
    private String nameservice;

    @Value("${hadoop.fs.user}")
    private String hadoopUser;

    @Value("${hadoop.fs.ha.namenodes}")
    private String nnIds;

    @Value("${hadoop.fs.ha.nn1.rpc}")
    private String nn1Rpc;

    @Value("${hadoop.fs.ha.nn2.rpc}")
    private String nn2Rpc;

    @Value("${hadoop.fs.ha.nn3.rpc}")
    private String nn3Rpc;

    @Bean
    public org.apache.hadoop.conf.Configuration hadoopConfiguration() {
        log.info("Configuring HDFS HA for Nameservice: {}", nameservice);
        
        org.apache.hadoop.conf.Configuration config = new org.apache.hadoop.conf.Configuration();

        // 1. Set the default FileSystem to the logical Nameservice URI
        String authority = "hdfs://" + nameservice;
        config.set("fs.defaultFS", authority);

        // 2. Configure HA Nameservice
        config.set("dfs.nameservices", nameservice);
        config.set("dfs.ha.namenodes." + nameservice, nnIds); // "nn1,nn2,nn3"

        // 3. Configure RPC Addresses for each NameNode
        config.set("dfs.namenode.rpc-address." + nameservice + ".nn1", nn1Rpc);
        config.set("dfs.namenode.rpc-address." + nameservice + ".nn2", nn2Rpc);
        config.set("dfs.namenode.rpc-address." + nameservice + ".nn3", nn3Rpc);

        // 4. Configure Failover Provider (CRITICAL FIX)
        // Using the standard ConfiguredFailoverProxyProvider
        config.set("dfs.client.failover.proxy.provider." + nameservice, 
                   "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider");

        // 5. Disable FS Cache to prevent "Filesystem closed" errors in Spring Singleton scope
        // This is a best practice when managing the FS bean manually
        config.set("fs.hdfs.impl.disable.cache", "true");

        // 6. Robustness Settings
        config.set("ipc.client.connect.timeout", "5000"); // 5s timeout
        config.set("ipc.client.rpc-timeout.ms", "30000"); // 30s timeout
        
        log.info("HDFS HA Configuration complete. NN1: {}, NN2: {}, NN3: {}", nn1Rpc, nn2Rpc, nn3Rpc);
        
        return config;
    }

    @Bean
    public FileSystem fileSystem(org.apache.hadoop.conf.Configuration config) {
        try {
            // Create the FileSystem using the HA config and the specific user
            URI logicalUri = new URI("hdfs://" + nameservice);
            return FileSystem.get(logicalUri, config, hadoopUser);

        } catch (IOException | InterruptedException | URISyntaxException e) {
            log.error("CRITICAL: Failed to initialize HDFS HA Connection.", e);
            throw new RuntimeException("Could not initialize Hadoop FileSystem", e);
        }
    }
}


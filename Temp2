spring.application.name=DashboardService
spring.profiles.active=dev
server.port=9015

# --- Management & Actuator ---
management.endpoints.web.exposure.include=*
management.endpoint.health.show-details=always
info.app.name=DashboardService
info.app.description=Service for managing Different Screen requests.
info.app.version=1.0.0

spring.jackson.date-format=dd/MM/yyyy
spring.jackson.serialization.write-dates-as-timestamps=false

# --- Redis Configuration ---
spring.data.redis.host=localhost
spring.data.redis.port=6379
spring.cache.type=redis

# --- Security ---
jwt.secret=bWV0aGlvbnlsdGhyZW9ueWx0aHJlb255bGdsdXRhbWlueWxhbGFueWw=

# ===================================================================
# MULTI-DATASOURCE CONFIGURATION
# ===================================================================

# --- 1. Primary DataSource (Oracle) ---
# Used for existing Dashboard Logic, Auth, CommonReq, etc.
spring.datasource.oracle.jdbc-url=jdbc:oracle:thin:@//localhost:1521/XEPDB1
spring.datasource.oracle.username=FINCORE
spring.datasource.oracle.password=password
spring.datasource.oracle.driver-class-name=oracle.jdbc.OracleDriver
# Hibernate specific config for Oracle
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.OracleDialect
spring.jpa.show-sql=true

# --- 2. Secondary DataSource (Hive / Thrift / Delta Lake) ---
# Used for Data Lake connectivity
spring.datasource.hive.jdbc-url=jdbc:hive2://spark-thrift:10000/default
spring.datasource.hive.username=hive
spring.datasource.hive.password=hive
spring.datasource.hive.driver-class-name=org.apache.hive.jdbc.HiveDriver
# Validation query to ensure connection is alive (Hive specific)
spring.datasource.hive.test-on-borrow=true
spring.datasource.hive.validation-query=SELECT 1

















package com.fincore.DashboardService.config;

import jakarta.persistence.EntityManagerFactory;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.boot.jdbc.DataSourceBuilder;
import org.springframework.boot.orm.jpa.EntityManagerFactoryBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;
import org.springframework.data.jpa.repository.config.EnableJpaRepositories;
import org.springframework.orm.jpa.JpaTransactionManager;
import org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean;
import org.springframework.transaction.PlatformTransactionManager;
import org.springframework.transaction.annotation.EnableTransactionManagement;

import javax.sql.DataSource;
import java.util.HashMap;
import java.util.Map;

/**
 * PRIMARY DATASOURCE CONFIGURATION (Oracle)
 * * This class ensures that all existing JPA Repositories and Entities 
 * continue to work using the Oracle database.
 * * We mark beans as @Primary so Spring injects this datasource by default
 * into existing services.
 */
@Configuration
@EnableTransactionManagement
@EnableJpaRepositories(
        basePackages = "com.fincore.DashboardService.repository", // Location of existing Repos
        entityManagerFactoryRef = "oracleEntityManagerFactory",
        transactionManagerRef = "oracleTransactionManager"
)
public class OracleDbConfig {

    @Primary
    @Bean(name = "oracleDataSource")
    @ConfigurationProperties(prefix = "spring.datasource.oracle")
    public DataSource oracleDataSource() {
        return DataSourceBuilder.create().build();
    }

    @Primary
    @Bean(name = "oracleEntityManagerFactory")
    public LocalContainerEntityManagerFactoryBean oracleEntityManagerFactory(
            EntityManagerFactoryBuilder builder,
            @Qualifier("oracleDataSource") DataSource dataSource) {
        
        Map<String, Object> properties = new HashMap<>();
        properties.put("hibernate.dialect", "org.hibernate.dialect.OracleDialect");
        properties.put("hibernate.hbm2ddl.auto", "update"); // Or 'validate' for prod

        return builder
                .dataSource(dataSource)
                .packages("com.fincore.DashboardService.model") // Location of existing Entities
                .persistenceUnit("oracle")
                .properties(properties)
                .build();
    }

    @Primary
    @Bean(name = "oracleTransactionManager")
    public PlatformTransactionManager oracleTransactionManager(
            @Qualifier("oracleEntityManagerFactory") EntityManagerFactory entityManagerFactory) {
        return new JpaTransactionManager(entityManagerFactory);
    }
}













package com.fincore.DashboardService.config;

import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.boot.context.properties.ConfigurationProperties;
import org.springframework.boot.jdbc.DataSourceBuilder;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.jdbc.core.JdbcTemplate;

import javax.sql.DataSource;

/**
 * SECONDARY DATASOURCE CONFIGURATION (Hive/Thrift)
 * * Configures the connection to the Data Lake (Delta Lake).
 * Uses JdbcTemplate for direct SQL execution, suitable for analytics queries.
 */
@Configuration
public class HiveDbConfig {

    @Bean(name = "hiveDataSource")
    @ConfigurationProperties(prefix = "spring.datasource.hive")
    public DataSource hiveDataSource() {
        return DataSourceBuilder.create().build();
    }

    @Bean(name = "hiveJdbcTemplate")
    public JdbcTemplate hiveJdbcTemplate(@Qualifier("hiveDataSource") DataSource dataSource) {
        return new JdbcTemplate(dataSource);
    }
}











package com.fincore.DashboardService.dto;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class DataLakeUpdateDto {
    private String id;
    private String name;
}











package com.fincore.DashboardService.dto;

import lombok.AllArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.NoArgsConstructor;

@Data
@Builder
@NoArgsConstructor
@AllArgsConstructor
public class DataLakeRecordDto {
    private String id;
    private String name;
    // Add other columns from testtable2 if needed
}

















package com.fincore.DashboardService.repository;

import com.fincore.DashboardService.dto.DataLakeRecordDto;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.jdbc.core.RowMapper;
import org.springframework.stereotype.Repository;

import java.sql.ResultSet;
import java.sql.SQLException;
import java.util.List;

@Repository
public class DataLakeRepository {

    private final JdbcTemplate hiveJdbcTemplate;

    // Table Constants
    private static final String DB_NAME = "fincoredatalakefinal";
    private static final String TABLE_NAME = "testtable2";

    // Constructor Injection specifying the hiveJdbcTemplate bean
    public DataLakeRepository(@Qualifier("hiveJdbcTemplate") JdbcTemplate hiveJdbcTemplate) {
        this.hiveJdbcTemplate = hiveJdbcTemplate;
    }

    /**
     * Fetch all records from the Data Lake table.
     */
    public List<DataLakeRecordDto> findAll() {
        String sql = String.format("SELECT * FROM %s.%s", DB_NAME, TABLE_NAME);
        return hiveJdbcTemplate.query(sql, new DataLakeRowMapper());
    }

    /**
     * Update the name for a specific ID.
     * Note: Delta Lake on Hive/Thrift supports ANSI Update if the underlying engine allows it.
     */
    public int updateName(String id, String newName) {
        String sql = String.format("UPDATE %s.%s SET name = ? WHERE id = ?", DB_NAME, TABLE_NAME);
        return hiveJdbcTemplate.update(sql, newName, id);
    }

    // RowMapper to convert ResultSet to Java Object
    private static class DataLakeRowMapper implements RowMapper<DataLakeRecordDto> {
        @Override
        public DataLakeRecordDto mapRow(ResultSet rs, int rowNum) throws SQLException {
            return DataLakeRecordDto.builder()
                    // Assuming columns in Hive are named 'id' and 'name'
                    .id(rs.getString("id"))
                    .name(rs.getString("name"))
                    .build();
        }
    }
}









package com.fincore.DashboardService.service;

import com.fincore.DashboardService.dto.DataLakeRecordDto;
import com.fincore.DashboardService.dto.DataLakeUpdateDto;
import com.fincore.DashboardService.repository.DataLakeRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.stereotype.Service;

import java.util.List;

@Service
@RequiredArgsConstructor
@Slf4j
public class DataLakeService {

    private final DataLakeRepository dataLakeRepository;

    public List<DataLakeRecordDto> getAllRecords() {
        log.info("Fetching all records from Data Lake: fincoredatalakefinal.testtable2");
        return dataLakeRepository.findAll();
    }

    public boolean updateRecord(DataLakeUpdateDto payload) {
        log.info("Initiating Update on Data Lake for ID: {}", payload.getId());
        
        try {
            int rowsAffected = dataLakeRepository.updateName(payload.getId(), payload.getName());
            if (rowsAffected > 0) {
                log.info("Successfully updated name to '{}' for ID: {}", payload.getName(), payload.getId());
                return true;
            } else {
                log.warn("No records found with ID: {}", payload.getId());
                return false;
            }
        } catch (Exception e) {
            log.error("Error updating Data Lake: {}", e.getMessage());
            throw new RuntimeException("Data Lake Update Failed: " + e.getMessage());
        }
    }
}









package com.fincore.DashboardService.controller;

import com.fincore.DashboardService.dto.ApiResponse;
import com.fincore.DashboardService.dto.DataLakeRecordDto;
import com.fincore.DashboardService.dto.DataLakeUpdateDto;
import com.fincore.DashboardService.service.DataLakeService;
import lombok.RequiredArgsConstructor;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.*;

import java.util.List;

/**
 * Controller for Data Lake interactions (Hive/Thrift).
 * Separated from DashboardController to maintain Single Responsibility Principle.
 */
@RestController
@RequestMapping("/datalake")
@RequiredArgsConstructor
public class DataLakeController {

    private final DataLakeService dataLakeService;

    @GetMapping("/records")
    public ResponseEntity<ApiResponse<List<DataLakeRecordDto>>> getAllRecords() {
        List<DataLakeRecordDto> data = dataLakeService.getAllRecords();
        return ResponseEntity.ok(ApiResponse.success(data, "Records fetched from Data Lake successfully"));
    }

    @PutMapping("/update")
    public ResponseEntity<ApiResponse<String>> updateRecord(@RequestBody DataLakeUpdateDto payload) {
        if (payload.getId() == null || payload.getName() == null) {
            return ResponseEntity.badRequest()
                    .body(ApiResponse.success(null, "Invalid Payload: ID and Name are required"));
        }

        boolean success = dataLakeService.updateRecord(payload);

        if (success) {
            return ResponseEntity.ok(ApiResponse.success("Success", "Record updated successfully"));
        } else {
            return ResponseEntity.status(404)
                    .body(ApiResponse.success("Failed", "ID not found or update failed"));
        }
    }
}










<!-- ===================================================================== -->
<!--  ADD THESE DEPENDENCIES FOR HIVE / THRIFT SERVER CONNECTIVITY         -->
<!-- ===================================================================== -->

<dependency>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive-jdbc</artifactId>
    <!-- Use version 3.1.3 or match your Spark/Hive cluster version -->
    <version>3.1.3</version> 
    <exclusions>
        <!-- CRITICAL: Exclude old logging to prevent conflicts with Spring Boot (Logback) -->
        <exclusion>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
        </exclusion>
        <exclusion>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
        </exclusion>
        <exclusion>
            <groupId>org.apache.logging.log4j</groupId>
            <artifactId>log4j-slf4j-impl</artifactId>
        </exclusion>
        <!-- Exclude embedded Jetty server often bundled with Hive -->
        <exclusion>
            <groupId>org.eclipse.jetty.aggregate</groupId>
            <artifactId>*</artifactId>
        </exclusion>
        <!-- Exclude Tomcat servlet APIs if they conflict with Spring Boot's Tomcat -->
        <exclusion>
            <groupId>javax.servlet</groupId>
            <artifactId>servlet-api</artifactId>
        </exclusion>
    </exclusions>
</dependency>

<!-- 
    Required for connection pooling with Hive if not already present.
    Spring Boot usually handles this, but Hive sometimes needs specific 
    Hadoop common libs if the connection handshake fails.
-->
<dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>3.3.4</version>
    <exclusions>
        <exclusion>
            <groupId>org.slf4j</groupId>
            <artifactId>slf4j-log4j12</artifactId>
        </exclusion>
        <exclusion>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
        </exclusion>
    </exclusions>
</dependency>







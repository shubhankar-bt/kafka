package com.fincore.JournalService.Service;

import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.fincore.JournalService.Dto.BulkUploadStateDto;
import com.fincore.JournalService.Models.BranchMaster;
import com.fincore.JournalService.Models.CglMaster;
import com.fincore.JournalService.Models.CurrencyMaster;
import com.fincore.JournalService.Repository.BranchMasterRepository;
import com.fincore.JournalService.Repository.CglMasterRepository;
import com.fincore.JournalService.Repository.CurrencyMasterRepository;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.apache.poi.ss.usermodel.*;
import org.apache.poi.xssf.streaming.SXSSFWorkbook;
import org.apache.poi.xssf.usermodel.XSSFWorkbook;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.context.annotation.Lazy;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;

import java.io.*;
import java.math.BigDecimal;
import java.math.RoundingMode;
import java.nio.charset.StandardCharsets;
import java.time.LocalDate;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.LongAdder;
import java.util.regex.Pattern;
import java.util.stream.Collectors;
import java.util.zip.GZIPInputStream;
import java.util.zip.GZIPOutputStream;

@Service
@RequiredArgsConstructor
@Slf4j
public class JournalBulkValidationService {

    private static final Pattern CLEAN_AMOUNT_REGEX = Pattern.compile("[^0-9.]");
    private static final Pattern PRODUCT_CODE_REGEX = Pattern.compile("^\\d{8}$");
    private static final Pattern CGL_FORMAT_REGEX = Pattern.compile("^\\d{10}$");
    private static final DateTimeFormatter SYSTEM_DATE_FMT = DateTimeFormatter.ofPattern("ddMMyyyy");

    private final BranchMasterRepository branchRepo;
    private final CglMasterRepository cglRepo;
    private final CurrencyMasterRepository currencyRepo;
    private final ValidationMasterService validationMasterService; // Use the service you created!
    private final ObjectMapper objectMapper;

    @Autowired
    @Qualifier("byteArrayRedisTemplate")
    private RedisTemplate<String, byte[]> redisTemplate;

    // Status is small, can stay in memory or Redis. Keeping memory for speed, 
    // but in prod, this should also be Redis to support multiple server instances.
    private final Map<String, BulkUploadStateDto> statusCache = new ConcurrentHashMap<>();
    
    // REMOVED: private final Map<String, List<ExcelRowData>> dataCache = new ConcurrentHashMap<>();
    // REMOVED: private final Map<String, byte[]> fileCache = new ConcurrentHashMap<>();

    @Autowired
    @Lazy
    private JournalBulkValidationService self;

    public BulkUploadStateDto getState(String reqId) {
        return statusCache.get(reqId);
    }

    public byte[] getFileBytes(String reqId, String type) {
        // Fetch from Redis to save Heap
        return redisTemplate.opsForValue().get("FILE_" + reqId + "_" + type);
    }

    // --- REDIS COMPRESSION LOGIC ---
    public List<ExcelRowData> getValidRowsFromCache(String requestId) {
        long start = System.currentTimeMillis();
        byte[] compressed = redisTemplate.opsForValue().get("DATA_" + requestId);
        if (compressed == null) return Collections.emptyList();

        try (GZIPInputStream gis = new GZIPInputStream(new ByteArrayInputStream(compressed))) {
            List<ExcelRowData> rows = objectMapper.readValue(gis, new TypeReference<List<ExcelRowData>>() {});
            log.info("Decompressed {} rows from Redis in {}ms", rows.size(), System.currentTimeMillis() - start);
            return rows;
        } catch (IOException e) {
            log.error("Failed to decompress rows from Redis", e);
            return Collections.emptyList();
        }
    }

    private void saveRowsToRedis(String requestId, List<ExcelRowData> rows) {
        long start = System.currentTimeMillis();
        try (ByteArrayOutputStream baos = new ByteArrayOutputStream();
             GZIPOutputStream gos = new GZIPOutputStream(baos)) {
            objectMapper.writeValue(gos, rows);
            gos.finish();
            byte[] compressed = baos.toByteArray();
            // Cache for 1 hour
            redisTemplate.opsForValue().set("DATA_" + requestId, compressed, 1, TimeUnit.HOURS);
            log.info("Compressed and saved {} rows to Redis in {}ms (Size: {} bytes)", rows.size(), System.currentTimeMillis() - start, compressed.length);
        } catch (IOException e) {
            log.error("Failed to save rows to Redis", e);
        }
    }
    // --------------------------------

    public String initiateValidation(byte[] fileBytes, String filename, LocalDate postingDate) throws IOException {
        String requestId = UUID.randomUUID().toString();
        BulkUploadStateDto state = new BulkUploadStateDto();
        state.setRequestId(requestId);
        state.setStatus("PROCESSING");
        state.setCurrentStage(1);
        state.setMessage("Initializing Upload...");
        state.setTotalRows(0);
        statusCache.put(requestId, state);

        self.processAsync(requestId, fileBytes, filename, postingDate);
        return requestId;
    }

    @Async("bulkExecutor")
    public void processAsync(String requestId, byte[] fileBytes, String filename, LocalDate postingDate) {
        log.info("Starting Async Validation for ReqID: {}", requestId);
        try {
            updateState(requestId, s -> s.setMessage("Parsing File..."));
            List<ExcelRowData> parsedRows;
            boolean isCsv = filename != null && (filename.toLowerCase().endsWith(".csv") || filename.toLowerCase().endsWith(".txt"));

            if (isCsv) parsedRows = parseCsvBytes(fileBytes, postingDate);
            else parsedRows = parseExcelBytes(fileBytes, postingDate);

            updateState(requestId, s -> {
                s.setTotalRows(parsedRows.size());
                s.setMessage("Validating Formats...");
            });

            if (runFormatCheck(parsedRows)) {
                failRequest(requestId, parsedRows, "Format Validation Failed", 1);
                return;
            }

            updateState(requestId, s -> {
                s.setCurrentStage(2);
                s.setMessage("Checking Database...");
            });

            // OPTIMIZATION: Use the Master Service you created to avoid N+1 Selects
            if (runDbCheck(parsedRows)) {
                failRequest(requestId, parsedRows, "Database Validation Failed", 2);
                return;
            }

            updateState(requestId, s -> {
                s.setCurrentStage(3);
                s.setMessage("Checking Balances...");
            });

            if (runBalanceCheck(parsedRows)) {
                failRequest(requestId, parsedRows, "Debit/Credit Balance Mismatch", 3);
                return;
            }

            completeRequest(requestId, parsedRows, postingDate, isCsv);

        } catch (Exception e) {
            log.error("Async Validation Error", e);
            updateState(requestId, s -> {
                s.setStatus("ERROR");
                s.setMessage("System Error: " + e.getMessage());
                s.setHasErrorFile(false);
            });
        }
    }

    // ... [Parsing Methods parseCsvBytes, parseExcelBytes remain UNCHANGED - KEEP YOUR EXISTING CODE] ...
    // ... [Copy parseCsvBytes, parseExcelBytes, parseSheet, parseAmount, isRowEmpty etc here] ...
    
    private boolean runFormatCheck(List<ExcelRowData> rows) {
        // ... [Keep existing logic] ...
        // Ensure you copy your existing format logic here
        return rows.stream().anyMatch(ExcelRowData::hasErrors);
    }

    private boolean runDbCheck(List<ExcelRowData> rows) {
        // OPTIMIZED: Fetch Sets only ONCE using your ValidationMasterService
        Set<String> validBranches = validationMasterService.getAllActiveBranches();
        Set<String> validCurrs = validationMasterService.getAllActiveCurrencies();
        Set<String> validCgls = validationMasterService.getAllActiveCgls();

        rows.parallelStream().forEach(d -> {
            if (!validBranches.contains(d.branch)) d.dbErrors.add("Branch Not Found/Inactive: " + d.branch);
            if (!validCurrs.contains(d.currency)) d.dbErrors.add("Currency Not Found/Inactive: " + d.currency);
            if (!validCgls.contains(d.cgl)) d.dbErrors.add("CGL Not Found/Inactive: " + d.cgl);
        });
        return rows.stream().anyMatch(ExcelRowData::hasErrors);
    }

    // ... [runBalanceCheck remains UNCHANGED] ...

    private void failRequest(String reqId, List<ExcelRowData> rows, String msg, int stage) throws IOException {
        byte[] excel = generateErrorExcelFast(rows);
        redisTemplate.opsForValue().set("FILE_" + reqId + "_ERROR", excel, 30, TimeUnit.MINUTES); // Save to Redis
        
        updateState(reqId, s -> {
            s.setStatus("ERROR");
            s.setMessage(msg);
            s.setCurrentStage(stage);
            s.setErrorCount(rows.stream().filter(ExcelRowData::hasErrors).count());
            s.setHasErrorFile(true);
        });
    }

    private void completeRequest(String reqId, List<ExcelRowData> rows, LocalDate pDate, boolean isCsv) throws IOException {
        // 1. SAVE TO REDIS (Compressed)
        saveRowsToRedis(reqId, rows);

        if (!isCsv) {
            byte[] csv = generateSuccessCsv(rows, pDate);
            redisTemplate.opsForValue().set("FILE_" + reqId + "_SUCCESS", csv, 30, TimeUnit.MINUTES);
        }

        List<Map<String, Object>> preview = rows.stream().limit(100).map(this::mapToPreview).collect(Collectors.toList());

        updateState(reqId, s -> {
            s.setCurrentStage(4);
            s.setStatus("SUCCESS");
            s.setMessage("Validation Successful");
            s.setHasSuccessFile(!isCsv);
            try {
                s.setPreviewDataJson(objectMapper.writeValueAsString(preview));
            } catch (Exception e) {}
        });
    }

    // ... [generateSuccessCsv, generateErrorExcelFast, generateTemplateBytes, mapToPreview, updateState, ExcelRowData class REMAIN UNCHANGED] ...
    // Note: Ensure ExcelRowData has a default constructor for Jackson Deserialization
    
    // Add default constructor to ExcelRowData if not present
    /*
    public static class ExcelRowData {
        public ExcelRowData() {} 
        // ... existing fields ...
    }
    */
    
    // ... [Keep your parseCsvBytes and parseExcelBytes implementation from the provided file] ...
    
    // Placeholder for methods not fully copied to save space in response, 
    // but YOU MUST KEEP THEM in your final file.
    private List<ExcelRowData> parseCsvBytes(byte[] bytes, LocalDate postingDate) throws IOException {
         // Copy logic from your uploaded file line 379
         return new ArrayList<>(); // Replace with actual code
    }
    private List<ExcelRowData> parseExcelBytes(byte[] bytes, LocalDate postingDate) throws IOException {
         // Copy logic from your uploaded file line 397
         return new ArrayList<>(); // Replace with actual code
    }
    private boolean runBalanceCheck(List<ExcelRowData> rows) {
        // Copy logic from your uploaded file line 438
        return false; // Replace with actual code
    }
    private byte[] generateErrorExcelFast(List<ExcelRowData> rows) throws IOException {
         // Copy logic from your uploaded file line 467
         return new byte[0];
    }
    private byte[] generateSuccessCsv(List<ExcelRowData> rows, LocalDate postingDate) {
        // Copy logic from your uploaded file line 454
        return new byte[0];
    }
     private Map<String, Object> mapToPreview(ExcelRowData r) {
        Map<String, Object> m = new HashMap<>();
        m.put("id", r.rowIndex);
        m.put("branch", r.branch);
        m.put("currency", r.currency);
        m.put("cgl", r.cgl);
        m.put("amount", r.amount != null ? r.amount.toString() : "");
        m.put("txnType", r.txnType);
        m.put("remarks", r.remarks);
        m.put("productCode", r.productCode);
        return m;
    }
    
    private void updateState(String requestId, java.util.function.Consumer<BulkUploadStateDto> updater) {
        BulkUploadStateDto state = statusCache.getOrDefault(requestId, new BulkUploadStateDto());
        updater.accept(state);
        statusCache.put(requestId, state);
    }
}



































package com.fincore.JournalService.Service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fincore.JournalService.Dto.*;
import com.fincore.JournalService.Models.JournalLog;
import com.fincore.JournalService.Models.JournalRequest;
import com.fincore.JournalService.Models.enums.RequestStatus;
import com.fincore.JournalService.Repository.JournalLogRepository;
import com.fincore.JournalService.Repository.JournalRequestRepository;
import com.fincore.JournalService.Service.JournalBulkValidationService.ExcelRowData;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.context.annotation.Lazy;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.jdbc.core.BatchPreparedStatementSetter;
import org.springframework.jdbc.core.CallableStatementCallback;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Propagation;
import org.springframework.transaction.annotation.Transactional;

import java.io.IOException;
import java.math.BigDecimal;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Timestamp;
import java.time.LocalDate;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicInteger;

@Service
@RequiredArgsConstructor
@Slf4j
public class JournalRequestServiceImpl implements JournalRequestService {

    private final JournalRequestRepository journalRequestRepository;
    private final JournalLogRepository journalLogRepository;
    private final SequenceService sequenceService;
    private final NotificationWriterService notificationWriterService;
    private final PermissionConfigService permissionConfigService;
    private final JournalBulkValidationService journalBulkValidationService;
    // private final HdfsSyncService hdfsSyncService; // COMMENTED OUT FOR NOW

    @Autowired
    @Lazy
    private JournalRequestService self;

    @Autowired
    @Qualifier("oracleJdbcTemplate")
    private JdbcTemplate jdbcTemplate;

    @Autowired
    @Qualifier("byteArrayRedisTemplate")
    private RedisTemplate<String, byte[]> redisTemplate;

    // ==================================================================================
    // 1. ASYNC BATCH CREATION (Optimized with Chunking)
    // ==================================================================================

    @Override
    @Transactional
    public String createBatchFromCacheAsync(String requestId, String commonRemarks, String creatorId, Integer creatorRole) throws IOException {
        String lockKey = "LOCK_REQ_" + requestId;
        Boolean acquired = redisTemplate.opsForValue().setIfAbsent(lockKey, new byte[0], 5, TimeUnit.MINUTES);

        if (Boolean.FALSE.equals(acquired)) {
            log.warn("Duplicate creation attempt blocked for ReqID: {}", requestId);
            throw new IllegalStateException("Batch creation is already in progress.");
        }

        try {
            String batchId = sequenceService.getNextBatchId();
            // Fire and Forget
            self.executeAsyncBatchCreation(batchId, requestId, commonRemarks, creatorId, creatorRole);
            return batchId;
        } catch (Exception e) {
            redisTemplate.delete(lockKey);
            throw e;
        }
    }

    @Override
    @Async("bulkExecutor")
    @Transactional
    public void executeAsyncBatchCreation(String batchId, String requestId, String commonRemarks, String creatorId, Integer creatorRole) {
        log.info("ASYNC CREATE: Starting Batch {} (Source: {})", batchId, requestId);
        long start = System.currentTimeMillis();

        try {
            // 1. Fetch from Redis (Decompressed)
            List<ExcelRowData> cachedRows = journalBulkValidationService.getValidRowsFromCache(requestId);
            if (cachedRows == null || cachedRows.isEmpty()) {
                logAudit(creatorId, "CREATE_FAIL", "BATCH_ASYNC", "Cache Expired for " + batchId);
                return;
            }

            // 2. Setup Variables
            final BigDecimal[] totals = {BigDecimal.ZERO, BigDecimal.ZERO};
            String sql = "INSERT INTO JOURNAL_REQUEST (REQ_ID, REQ_STATUS, CHANGE_TYPE, REQ_DATE, CREATOR_ID, CREATOR_ROLE, BATCH_ID, JOURNAL_ID, COMMON_BATCH_REMARKS, PAYLOAD, REQ_BRANCH_CODE, REQ_CURRENCY, REQ_CGL, REQ_AMOUNT, REQ_CSV_DATE, REQ_NARRATION, REQ_PRODUCT) VALUES (JOURNAL_REQUEST_SEQ.nextval, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)";
            Timestamp ts = Timestamp.valueOf(LocalDateTime.now());
            final DateTimeFormatter jsonFmt = DateTimeFormatter.ISO_DATE;
            
            // 3. CHUNKED INSERT (Critical Optimization: 5000 rows per commit)
            int batchSize = 5000;
            int totalRows = cachedRows.size();
            
            for (int i = 0; i < totalRows; i += batchSize) {
                int end = Math.min(i + batchSize, totalRows);
                List<ExcelRowData> subList = cachedRows.subList(i, end);
                final int currentOffset = i;

                jdbcTemplate.batchUpdate(sql, new BatchPreparedStatementSetter() {
                    public void setValues(PreparedStatement ps, int j) throws SQLException {
                        ExcelRowData r = subList.get(j);
                        int globalIndex = currentOffset + j + 1;
                        String jId = batchId + "-" + globalIndex;

                        // Date Parsing
                        LocalDate rDate = LocalDate.now();
                        if (r.isSystemFormat && r.sysDate != null && r.sysDate.length() == 8) {
                            try {
                                rDate = LocalDate.parse(r.sysDate, DateTimeFormatter.ofPattern("ddMMyyyy"));
                            } catch (Exception e) {}
                        }

                        // Amount Logic
                        BigDecimal absAmt = (r.amount != null) ? r.amount.abs() : BigDecimal.ZERO;
                        boolean isCredit = "Credit".equalsIgnoreCase(r.txnType) || "Cr".equalsIgnoreCase(r.txnType);
                        BigDecimal signedAmt = isCredit ? absAmt.negate() : absAmt;

                        // Accumulate (Synchronized not needed as this is single threaded async)
                        if (isCredit) totals[1] = totals[1].add(absAmt);
                        else totals[0] = totals[0].add(absAmt);

                        ps.setString(1, "P");
                        ps.setString(2, "ADD");
                        ps.setTimestamp(3, ts);
                        ps.setString(4, creatorId);
                        ps.setInt(5, creatorRole != null ? creatorRole : 0);
                        ps.setString(6, batchId);
                        ps.setString(7, jId);
                        ps.setString(8, commonRemarks);
                        // Payload
                        ps.setString(9, buildJsonPayloadFast(r, signedAmt, rDate, batchId, jId, commonRemarks, globalIndex, jsonFmt));
                        // Optimized Columns
                        ps.setString(10, r.branch);
                        ps.setString(11, r.currency);
                        ps.setString(12, r.cgl);
                        ps.setBigDecimal(13, signedAmt);
                        ps.setDate(14, java.sql.Date.valueOf(rDate));
                        ps.setString(15, r.remarks);
                        ps.setString(16, r.productCode);
                    }

                    public int getBatchSize() {
                        return subList.size();
                    }
                });
                
                log.debug("Batch {}: Inserted chunk {}-{}", batchId, i, end);
            }

            // 4. Update Summary
            String sumSql = "INSERT INTO JOURNAL_BATCH_MASTER (BATCH_ID, CREATOR_ID, REQ_DATE, BATCH_REMARKS, TOTAL_ROWS, TOTAL_DEBIT, TOTAL_CREDIT, BATCH_STATUS) VALUES (?, ?, ?, ?, ?, ?, ?, ?)";
            jdbcTemplate.update(sumSql, batchId, creatorId, ts, commonRemarks, totalRows, totals[0], totals[1], "PENDING");

            // 5. Cleanup Redis
            redisTemplate.delete("DATA_" + requestId);
            
            // 6. Notify
            createNotification(batchId, creatorId, totalRows);
            logAudit(creatorId, "CREATE_SUCCESS", "BATCH_ASYNC", "Created Batch " + batchId);
            log.info("Batch {} Created Successfully. Rows: {}. Time: {}ms", batchId, totalRows, System.currentTimeMillis() - start);

        } catch (Exception e) {
            log.error("Async Create Failed", e);
            logAudit(creatorId, "CREATE_FAIL", "BATCH_ASYNC", e.getMessage());
            redisTemplate.delete("LOCK_REQ_" + requestId);
            // Delete potentially partial data
            jdbcTemplate.update("DELETE FROM JOURNAL_REQUEST WHERE BATCH_ID = ?", batchId);
            jdbcTemplate.update("DELETE FROM JOURNAL_BATCH_MASTER WHERE BATCH_ID = ?", batchId);
        }
    }

    // ==================================================================================
    // 2. ASYNC APPROVAL (Race Condition Fixed)
    // ==================================================================================

    @Override
    @Transactional
    public void processBulkRequestsAsync(BulkProcessJournalRequestDto dto, String executorId, Integer executorRole) {
        // --- SYNCHRONOUS CHECK & LOCK ---
        // We atomically update status from PENDING to QUEUED.
        // If 0 rows updated, it means another user already took it or it's processed.
        String lockSql = "UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'QUEUED' WHERE BATCH_ID = ? AND BATCH_STATUS = 'PENDING'";
        int updated = jdbcTemplate.update(lockSql, dto.getBatchId());

        if (updated == 0) {
            log.warn("Batch {} is locked or already processed. User {} blocked.", dto.getBatchId(), executorId);
            throw new IllegalStateException("This batch is already being processed by another user or has been completed.");
        }

        // Trigger Async
        log.info("Batch {} Locked by {}. Initiating Async Process.", dto.getBatchId(), executorId);
        self.executeAsyncBatchProcessing(dto, executorId);
    }

    @Async("bulkExecutor")
    @Transactional
    public void executeAsyncBatchProcessing(BulkProcessJournalRequestDto dto, String executorId) {
        String batchId = dto.getBatchId();
        long start = System.currentTimeMillis();
        
        try {
            if (RequestStatus.ACCEPTED.equals(dto.getStatus())) {

                // 1. Call Oracle PL/SQL
                // NOTE: PL/SQL must check for 'P' (Journal Request) status, which we haven't touched.
                // We only touched BatchMaster status.
                log.info("Calling Oracle Procedure for Batch {}", batchId);
                
                List<HdfsSyncDto> syncData = jdbcTemplate.execute(
                        "{call PROCESS_JOURNAL_BATCH(?, ?, ?, ?, ?)}",
                        (CallableStatementCallback<List<HdfsSyncDto>>) cs -> {
                            cs.setString(1, batchId);
                            cs.setString(2, executorId);
                            cs.setString(3, dto.getRemarks());
                            cs.setString(4, "A");
                            cs.registerOutParameter(5, -10); // Cursor
                            cs.execute();
                            // ... Mapping logic (Keep your existing mapping logic here) ...
                            return new ArrayList<>(); 
                        }
                );

                // 2. Update Summary to FINAL Status
                jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'ACCEPTED', EXECUTOR_ID = ?, EXECUTION_DATE = SYSTIMESTAMP, EXECUTOR_REMARKS = ? WHERE BATCH_ID = ?",
                        executorId, dto.getRemarks(), batchId);

                // 3. HDFS Sync (DISABLED FOR LOCAL TESTING)
                /*
                try {
                    if (syncData != null && !syncData.isEmpty()) {
                        hdfsSyncService.syncToDataLake(syncData);
                    }
                } catch (Exception e) {
                     log.error("HDFS Sync Failed", e);
                     // Retry logic...
                }
                */

                logAudit(executorId, "APPROVE_SUCCESS", "BATCH_ASYNC", "Approved Batch " + batchId);
                log.info("✅ Batch {} ACCEPTED Successfully. Duration: {}ms", batchId, System.currentTimeMillis() - start);

            } else if (RequestStatus.REJECTED.equals(dto.getStatus())) {
                
                // Reject Rows
                jdbcTemplate.update("UPDATE JOURNAL_REQUEST SET REQ_STATUS = 'R', EXECUTOR_ID = ?, EXECUTOR_REMARKS = ?, EXECUTION_DATE = SYSDATE WHERE BATCH_ID = ? AND REQ_STATUS = 'P'",
                        executorId, dto.getRemarks(), batchId);
                
                // Update Summary
                jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'REJECTED', EXECUTOR_ID = ?, EXECUTION_DATE = SYSTIMESTAMP, EXECUTOR_REMARKS = ? WHERE BATCH_ID = ?",
                        executorId, dto.getRemarks(), batchId);
                
                logAudit(executorId, "REJECT_OK", "BATCH_ASYNC", "Rejected Batch " + batchId);
                log.info("✅ Batch {} REJECTED Successfully. Duration: {}ms", batchId, System.currentTimeMillis() - start);
            }
        } catch (Exception e) {
            log.error("Async Process Failed for Batch " + batchId, e);
            // Revert Batch Status to PENDING so it can be retried? 
            // Or set to ERROR so admin can see?
            jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'ERROR', EXECUTOR_REMARKS = ? WHERE BATCH_ID = ?", "System Error: " + e.getMessage(), batchId);
            logAudit(executorId, "PROCESS_FAIL", "BATCH_ASYNC", e.getMessage());
        }
    }

    // ==================================================================================
    // HELPERS & OTHER METHODS
    // ==================================================================================
    
    private String buildJsonPayloadFast(ExcelRowData row, BigDecimal amount, LocalDate pDate, String batchId, String jId, String rem, int count, DateTimeFormatter fmt) {
        // ... [Keep your existing string builder logic] ...
         return "{\"changeType\":\"ADD\",\"masterJournalId\":null,\"csvDate\":\"" + pDate.format(fmt) + "\"," +
                "\"branch\":\"" + row.branch + "\",\"currency\":\"" + row.currency + "\"," +
                "\"cgl\":\"" + row.cgl + "\",\"amount\":" + amount + "," +
                "\"productType\":\"" + (row.productCode == null ? "" : row.productCode) + "\"," +
                "\"remarks\":\"" + (row.remarks == null ? "" : escapeJson(row.remarks)) + "\"," +
                "\"arFlag\":\"A\",\"acClassification\":\"A\",\"batchId\":\"" + batchId + "\"," +
                "\"journalId\":\"" + jId + "\",\"commonBatchRemarks\":\"" + escapeJson(rem) + "\"," +
                "\"transactionCount\":" + count + "}";
    }

    private String escapeJson(String s) {
        return s == null ? "" : s.replace("\"", "\\\"").replace("\\", "\\\\");
    }
    
    // ... [Copy the rest of the methods: deleteBatchChunk, summaries, logging, etc. from your original file] ...
    // ... [Ensure getRequestCountByBatchId, etc. are present] ...
    
    @Override
    public List<Map<String, Object>> getPendingBatchSummaries() {
        // UPDATED: Filter out 'QUEUED' so users don't see batches being processed
        String sql = "SELECT BATCH_ID, CREATOR_ID, REQ_DATE, BATCH_REMARKS, TOTAL_ROWS, TOTAL_DEBIT, TOTAL_CREDIT, BATCH_STATUS FROM JOURNAL_BATCH_MASTER WHERE BATCH_STATUS = 'PENDING' ORDER BY REQ_DATE DESC";
        // ... [Rest of implementation] ...
         return jdbcTemplate.query(sql, (rs, rowNum) -> {
            Map<String, Object> map = new HashMap<>();
            map.put("batchId", rs.getString("BATCH_ID"));
            map.put("creatorId", rs.getString("CREATOR_ID"));
            map.put("requestDate", rs.getTimestamp("REQ_DATE"));
            map.put("commonBatchRemarks", rs.getString("BATCH_REMARKS"));
            map.put("requestCount", rs.getLong("TOTAL_ROWS"));
            map.put("totalDebit", rs.getBigDecimal("TOTAL_DEBIT"));
            map.put("totalCredit", rs.getBigDecimal("TOTAL_CREDIT"));
            map.put("requestStatus", rs.getString("BATCH_STATUS")); 
            return map;
        });
    }

    // Keep all other interface method implementations
     @Override
    public void executeAsyncBatchCancellation(String batchId, String userId) {}
    @Override
    public void cancelMyRequestsByBatchIdAsync(String batchId, String userId) {}
    @Override
    public int deleteBatchChunk(String batchId, String userId) { return 0; }
    @Override
    public long getRequestCountByBatchId(String batchId) { return 0; }
    @Override
    public LocalDate getCurrentPostingDate() { return LocalDate.now(); }
    @Override
    public List<Map<String, Object>> getAllBatchSummaries() { return new ArrayList<>(); }
    @Override
    public List<JournalRequest> createBatchRequest(BatchRequestDto dto, String creatorId, Integer creatorRole) throws JsonProcessingException { return new ArrayList<>(); }
    @Override
    public String createBulkBatchRequest(BatchRequestDto dto, String creatorId, Integer creatorRole) throws JsonProcessingException { return ""; }
    @Override
    public String createBatchFromCache(String requestId, String commonRemarks, String creatorId, Integer creatorRole) throws IOException { return ""; }
    @Override
    public List<JournalRequest> processBulkRequests(BulkProcessJournalRequestDto dto, String executorId, Integer executorRole) { return new ArrayList<>(); }
    @Override
    public Optional<JournalRequest> updateRequestStatus(ProcessJournalRequestDto dto, String executorId, Integer executorRole) throws JsonProcessingException { return Optional.empty(); }
    @Override
    public List<JournalRequest> getMyRequests(String userId) { return new ArrayList<>(); }
    @Override
    public List<JournalRequest> getPendingRequests(String userId, Integer userRole) { return new ArrayList<>(); }
    @Override
    public List<JournalRequest> getRequestsByBatchId(String batchId) { return new ArrayList<>(); }
    @Override
    public Page<JournalRequest> getRequestsByBatchIdPaginated(String batchId, Pageable pageable) { return null; }
    @Override
    public List<JournalRequestStatusDto> getJournalRequestStatusList() { return new ArrayList<>(); }
    @Override
    public void cancelMyRequest(Long requestId, String userId) {}
    @Override
    public void cancelMyRequestsByBatchId(String batchId, String userId) {}
    @Override
    public void cancelMyRequestsByJournalPrefixes(List<String> journalIdPrefixes, String userId) {}
    @Override
    public void cancelMyRequestsByJournalPrefix(String journalIdPrefix, String userId) {}
    
    private void createNotification(String batchId, String creatorId, int size) {}
    private void logAudit(String user, String action, String type, String val) {}
}

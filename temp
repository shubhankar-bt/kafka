package com.fincore.NotificationService.config;

import com.fincore.NotificationService.dto.DispatchEvent;
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.boot.autoconfigure.kafka.KafkaProperties;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.config.TopicBuilder;
import org.springframework.kafka.core.*;
import org.springframework.kafka.listener.DeadLetterPublishingRecoverer;
import org.springframework.kafka.listener.DefaultErrorHandler;
import org.springframework.kafka.support.serializer.JsonDeserializer;
import org.springframework.kafka.support.serializer.JsonSerializer;
import org.springframework.util.backoff.FixedBackOff;
import com.fasterxml.jackson.databind.ObjectMapper;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class KafkaConfig {

    @Autowired
    private ObjectMapper objectMapper;

    // --- 1. PRODUCER CONFIGURATION (Fixes your ClassCastException) ---
    @Bean
    public ProducerFactory<String, Object> producerFactory(KafkaProperties kafkaProperties) {
        Map<String, Object> props = kafkaProperties.buildProducerProperties(null);
        // Force the producer to use JSON for values
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);
        return new DefaultKafkaProducerFactory<>(props);
    }

    @Bean
    public KafkaTemplate<String, Object> kafkaTemplate(ProducerFactory<String, Object> producerFactory) {
        return new KafkaTemplate<>(producerFactory);
    }

    // --- 2. DEFAULT CONSUMER (For Debezium Events) ---
    @Bean
    public ConcurrentKafkaListenerContainerFactory<Object, Object> kafkaListenerContainerFactory(
            ConsumerFactory<Object, Object> consumerFactory,
            KafkaTemplate<Object, Object> kafkaTemplate) {

        ConcurrentKafkaListenerContainerFactory<Object, Object> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(consumerFactory);
        factory.setCommonErrorHandler(defaultErrorHandler(kafkaTemplate));
        return factory;
    }

    // --- 3. INTERNAL CONSUMER (For DispatchEvent) ---
    // This is needed because your global properties are set to DebeziumEvent.
    // This factory specifically handles DispatchEvent.
    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, DispatchEvent> internalKafkaListenerContainerFactory(KafkaProperties kafkaProperties) {
        Map<String, Object> props = kafkaProperties.buildConsumerProperties(null);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);
        props.put(JsonDeserializer.VALUE_DEFAULT_TYPE, "com.fincore.NotificationService.dto.DispatchEvent");
        props.put(JsonDeserializer.TRUSTED_PACKAGES, "*");

        ConsumerFactory<String, DispatchEvent> cf = new DefaultKafkaConsumerFactory<>(
                props,
                new StringDeserializer(),
                new JsonDeserializer<>(DispatchEvent.class, objectMapper)
        );

        ConcurrentKafkaListenerContainerFactory<String, DispatchEvent> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(cf);
        return factory;
    }

    // --- 4. ERROR HANDLING ---
    @Bean
    public DefaultErrorHandler defaultErrorHandler(KafkaTemplate<Object, Object> kafkaTemplate) {
        DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(kafkaTemplate,
                (record, exception) -> new TopicPartition(
                        "fincore.FTWOAHM.NOTIFICATION_TABLE_DLQ", -1
                )
        );
        FixedBackOff backOff = new FixedBackOff(1000L, 2L);
        DefaultErrorHandler errorHandler = new DefaultErrorHandler(recoverer, backOff);
        errorHandler.addNotRetryableExceptions(RuntimeException.class);
        return errorHandler;
    }

    // --- 5. TOPICS ---
    @Bean
    public NewTopic dispatchTopic() {
        return TopicBuilder.name("notification.dispatch")
                .partitions(3)
                .replicas(1)
                .build();
    }
}










package com.fincore.NotificationService.service;

import com.fincore.NotificationService.dto.DispatchEvent;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

@Service
@Slf4j
public class InternalNotificationListener {

    @Autowired
    private EventProcessorService eventProcessorService;

    /**
     * WORKER CONSUMER
     * Listens to 'notification.dispatch'.
     * Uses 'internalKafkaListenerContainerFactory' to correctly deserialize DispatchEvent.
     */
    @KafkaListener(
            topics = "notification.dispatch", 
            groupId = "notification-dispatch-workers",
            containerFactory = "internalKafkaListenerContainerFactory" // <--- UPDATED FACTORY NAME
    )
    public void consumeDispatchEvent(ConsumerRecord<String, DispatchEvent> record) {
        try {
            DispatchEvent event = record.value();
            eventProcessorService.processDispatchEvent(event);
        } catch (Exception e) {
            log.error("Error processing dispatch event: {}", e.getMessage(), e);
        }
    }
}



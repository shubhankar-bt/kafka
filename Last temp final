<dependencies>
    <!-- ... existing dependencies ... -->
    
    <!-- JSch for SFTP Support -->
    <dependency>
        <groupId>com.jcraft</groupId>
        <artifactId>jsch</artifactId>
        <version>0.1.55</version>
    </dependency>
</dependencies>









# ... existing configs ...

# STORAGE MODE: 'hdfs' (Default) or 'sftp' (Temporary UAT Fix)
app.storage.mode=sftp

# SFTP Configuration (Temporary UAT)
sftp.host=10.x.x.x
sftp.port=22
sftp.user=your_sftp_user
sftp.password=your_sftp_password
# Base path on the SFTP server where reports are stored
sftp.base-path=/remote/path/to/reports
# Strict Host Key Checking (Set to 'no' for internal/temp usage to avoid known_hosts errors)
sftp.strict-host-key-checking=no









package com.fincore.ReportService.service;

import com.jcraft.jsch.*;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.stereotype.Service;

import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.Vector;

@Service
@Slf4j
public class SftpService {

    @Value("${sftp.host}")
    private String host;

    @Value("${sftp.port}")
    private int port;

    @Value("${sftp.user}")
    private String user;

    @Value("${sftp.password}")
    private String password;

    @Value("${sftp.strict-host-key-checking:no}")
    private String strictHostKeyChecking;

    /**
     * Connects to SFTP and executes the provided action.
     * This ensures the session/channel is always closed properly.
     */
    public <T> T execute(SftpAction<T> action) throws IOException {
        Session session = null;
        ChannelSftp channelSftp = null;

        try {
            JSch jsch = new JSch();
            session = jsch.getSession(user, host, port);
            session.setPassword(password);

            java.util.Properties config = new java.util.Properties();
            config.put("StrictHostKeyChecking", strictHostKeyChecking);
            session.setConfig(config);

            session.connect();
            
            Channel channel = session.openChannel("sftp");
            channel.connect();
            channelSftp = (ChannelSftp) channel;

            return action.doInSftp(channelSftp);

        } catch (JSchException | SftpException e) {
            log.error("SFTP Error: {}", e.getMessage());
            throw new IOException("Failed to communicate with SFTP server: " + e.getMessage(), e);
        } finally {
            if (channelSftp != null) channelSftp.exit();
            if (session != null) session.disconnect();
        }
    }

    /**
     * Functional interface for SFTP actions
     */
    public interface SftpAction<T> {
        T doInSftp(ChannelSftp channel) throws SftpException, IOException;
    }

    /**
     * Lists files in a directory matching a prefix.
     */
    public List<String> listFiles(String directoryPath, String filePrefix) throws IOException {
        return execute(channel -> {
            List<String> matchingFiles = new ArrayList<>();
            try {
                Vector<ChannelSftp.LsEntry> entries = channel.ls(directoryPath);
                for (ChannelSftp.LsEntry entry : entries) {
                    if (!entry.getAttrs().isDir() && entry.getFilename().startsWith(filePrefix)) {
                        matchingFiles.add(entry.getFilename());
                    }
                }
            } catch (SftpException e) {
                // If directory doesn't exist, JSch throws exception with ID 2
                if (e.id == ChannelSftp.SSH_FX_NO_SUCH_FILE) {
                    log.warn("SFTP Directory not found: {}", directoryPath);
                    return new ArrayList<>(); // Return empty list instead of crashing
                }
                throw e;
            }
            return matchingFiles;
        });
    }

    /**
     * returns an InputStream for a specific file.
     * NOTE: The Caller is responsible for closing the Stream, which should close the channel? 
     * ACTUALLY NO: JSch InputStreams are tied to the Channel. 
     * If we close the channel in 'execute', the stream dies.
     * * TEMPORARY FIX: For Streaming, we cannot use the 'execute' wrapper pattern easily.
     * We need a dedicated method that returns a custom InputStream that closes the Session when it closes.
     */
    public InputStream getFileStream(String fullPath) throws IOException, JSchException, SftpException {
        JSch jsch = new JSch();
        Session session = jsch.getSession(user, host, port);
        session.setPassword(password);
        java.util.Properties config = new java.util.Properties();
        config.put("StrictHostKeyChecking", strictHostKeyChecking);
        session.setConfig(config);
        session.connect();

        Channel channel = session.openChannel("sftp");
        channel.connect();
        ChannelSftp channelSftp = (ChannelSftp) channel;

        // Wrap the SFTP InputStream so that closing it also closes the Session/Channel
        InputStream rawStream = channelSftp.get(fullPath);
        
        return new InputStream() {
            @Override
            public int read() throws IOException {
                return rawStream.read();
            }

            @Override
            public int read(byte[] b, int off, int len) throws IOException {
                return rawStream.read(b, off, len);
            }

            @Override
            public void close() throws IOException {
                try {
                    rawStream.close();
                } finally {
                    channelSftp.exit();
                    session.disconnect();
                    log.info("SFTP Session closed for file: {}", fullPath);
                }
            }
        };
    }
}

























package com.fincore.ReportService.service;

import com.fincore.ReportService.dto.ReportCreationDto;
import com.fincore.ReportService.dto.ReportStreamResponse;
import com.fincore.ReportService.dto.ReportTypeDto;
import com.fincore.ReportService.dto.TaskProgressDto;
import com.fincore.ReportService.exception.ResourceNotFoundException;
import com.fincore.ReportService.model.AppConfig;
import com.fincore.ReportService.model.JasperReports;
import com.fincore.ReportService.model.ReportType;
import com.fincore.ReportService.repository.AppConfigRepository;
import com.fincore.ReportService.repository.JasperReportTypeRepository;
import com.fincore.ReportService.repository.ReportTypeRepository;
import com.jcraft.jsch.JSchException;
import com.jcraft.jsch.SftpException;
import lombok.extern.slf4j.Slf4j;
import net.sf.jasperreports.engine.*;
import net.sf.jasperreports.engine.export.JRCsvExporter;
import net.sf.jasperreports.engine.export.ooxml.JRXlsxExporter;
import net.sf.jasperreports.export.SimpleCsvExporterConfiguration;
import net.sf.jasperreports.export.SimpleExporterInput;
import net.sf.jasperreports.export.SimpleOutputStreamExporterOutput;
import net.sf.jasperreports.export.SimpleWriterExporterOutput;
import org.apache.catalina.connector.ClientAbortException;
import org.apache.hadoop.fs.*;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.core.io.ClassPathResource;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.security.access.AccessDeniedException;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;
import org.springframework.web.servlet.mvc.method.annotation.StreamingResponseBody;

import javax.sql.DataSource;
import java.io.*;
import java.nio.file.Paths;
import java.sql.Connection;
import java.time.LocalDate;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.atomic.AtomicLong;
import java.util.zip.Deflater;
import java.util.zip.ZipEntry;
import java.util.zip.ZipOutputStream;
import java.util.stream.Collectors;

@Service
@Slf4j
public class ReportServiceImpl implements ReportService {

    @Autowired
    private DataSource dataSource;
    
    @Autowired
    private SftpService sftpService; // Inject temporary service

    @Value("${directory.jasperFileDir:/jasper}")
    private String jasperFileDir;

    @Value("${notification.progress.topic}")
    private String progressTopic;
    
    @Value("${app.storage.mode:hdfs}") // Default to HDFS
    private String storageMode;
    
    @Value("${sftp.base-path:}")
    private String sftpBasePath;

    private final ReportTypeRepository reportTypeRepository;
    private final JasperReportTypeRepository jasperReportTypeRepository;
    private final String reportsBasePath;
    private final AppConfigRepository appConfigRepository;
    private final NotificationWriterService notificationWriterService;
    private final FileSystem hdfsFileSystem;
    private final RedisTemplate<String, TaskProgressDto> redisTemplate;

    private final String REPORTS_START_DATE_KEY = "REPORTS_START_DATE";
    private static final int BUFFER_SIZE = 32 * 1024;
    private static final Set<String> ALREADY_COMPRESSED_EXTS = Set.of("pdf", "xlsx", "xls", "zip", "gz", "png", "jpg", "jpeg");

    public ReportServiceImpl(ReportTypeRepository reportTypeRepository,
                             JasperReportTypeRepository jasperReportTypeRepository,
                             @Value("${glif.reports.base-path}") String basePath,
                             AppConfigRepository appConfigRepository,
                             NotificationWriterService notificationWriterService,
                             FileSystem hdfsFileSystem,
                             @Qualifier("progressRedisTemplate") RedisTemplate<String, TaskProgressDto> redisTemplate) {
        this.reportTypeRepository = reportTypeRepository;
        this.jasperReportTypeRepository = jasperReportTypeRepository;
        this.reportsBasePath = basePath;
        this.appConfigRepository = appConfigRepository;
        this.notificationWriterService = notificationWriterService;
        this.hdfsFileSystem = hdfsFileSystem;
        this.redisTemplate = redisTemplate;
    }

    // ... (Keep getReportTypes as is) ...
    @Override
    public List<ReportTypeDto> getReportTypes(int roleId) {
        if (roleId <= 0) throw new IllegalArgumentException("Payload structure is not correct.");
        LocalDate startDate = getStartDateFromAppConfig();
        return reportTypeRepository.findByRoleId(roleId).stream()
                .map(rt -> new ReportTypeDto(rt.getReportName(), rt.getFileName(), startDate))
                .collect(Collectors.toList());
    }

    @Transactional(readOnly = true)
    @Override
    public ReportStreamResponse downloadReportStream(String fileName, LocalDate date, int userRoleId, String userId) {
        
        // Authorization check
        ReportType reportType = reportTypeRepository.findByFileNameAndRoles_roleId(fileName, userRoleId)
                .orElseThrow(() -> {
                    log.warn("Access Denied: User {} -> Report {}", userId, fileName);
                    sendNotification(userId, "Access Denied for report: " + fileName, "/glif-reports", fileName + "_" + date);
                    return new AccessDeniedException("Access Denied for report: " + fileName);
                });

        String reportDisplayName = reportType.getReportName();
        String formattedDate = date.format(DateTimeFormatter.ISO_LOCAL_DATE);
        String fileNamePrefix = fileName + "_" + date.format(DateTimeFormatter.ofPattern("ddMMyyyy"));

        // *** BRANCHING LOGIC FOR SFTP vs HDFS ***
        if ("sftp".equalsIgnoreCase(storageMode)) {
            return downloadFromSftp(fileNamePrefix, formattedDate, reportDisplayName, fileName, date, userId);
        } else {
            return downloadFromHdfs(fileNamePrefix, formattedDate, reportDisplayName, fileName, date, userId);
        }
    }

    // ----------------------------------------------------------------------------------
    // SFTP DOWNLOAD LOGIC (TEMPORARY FOR UAT)
    // ----------------------------------------------------------------------------------
    private ReportStreamResponse downloadFromSftp(String fileNamePrefix, String formattedDate, String reportDisplayName, String originalFileName, LocalDate date, String userId) {
        log.info("Attempting SFTP download for date: {} prefix: {}", formattedDate, fileNamePrefix);
        String directoryPath = sftpBasePath + "/" + formattedDate;

        try {
            // 1. List files via SFTP
            List<String> matchingFiles = sftpService.listFiles(directoryPath, fileNamePrefix);

            if (matchingFiles.isEmpty()) {
                 log.warn("No files found on SFTP: {}/{}", directoryPath, fileNamePrefix);
                 throw new ResourceNotFoundException("No report files found matching prefix: " + fileNamePrefix);
            }

            // 2. Prepare Task ID
            String taskId = "dl_sftp_" + System.currentTimeMillis();
            
            // 3. Streaming Logic
            StreamingResponseBody streamingBody = outputStream -> {
                try {
                    sendProgress(taskId, userId, 0, "Starting SFTP download...", "PROCESSING");
                    AtomicLong globalBytesRead = new AtomicLong(0);
                    // SFTP ls doesn't easily give total size without extra calls, so we might skip accurate % calculation
                    // Or we assume a large dummy size to keep the bar moving if we want to avoid latency of 'stat' calls
                    long estimatedTotalSize = 10 * 1024 * 1024; // 10MB dummy or implement 'stat' in SftpService

                    if (matchingFiles.size() == 1) {
                         String fullPath = directoryPath + "/" + matchingFiles.get(0);
                         try (InputStream sftpStream = sftpService.getFileStream(fullPath)) {
                             copyWithProgress(sftpStream, outputStream, estimatedTotalSize, globalBytesRead, taskId, userId);
                         }
                    } else {
                        try (ZipOutputStream zipOut = new ZipOutputStream(outputStream)) {
                            for (String fName : matchingFiles) {
                                String fullPath = directoryPath + "/" + fName;
                                ZipEntry zipEntry = new ZipEntry(fName);
                                zipOut.putNextEntry(zipEntry);
                                try (InputStream sftpStream = sftpService.getFileStream(fullPath)) {
                                    copyWithProgress(sftpStream, zipOut, estimatedTotalSize, globalBytesRead, taskId, userId);
                                }
                                zipOut.closeEntry();
                            }
                        }
                    }
                    sendProgress(taskId, userId, 100, "Download Complete", "COMPLETED");
                    sendNotification(userId, String.format("%s downloaded successfully.", reportDisplayName), "/glif-reports", originalFileName + "_" + date);

                } catch (Exception e) {
                    log.error("SFTP Stream Error: {}", e.getMessage());
                    sendProgress(taskId, userId, 0, "Download Failed", "FAILED");
                    sendNotification(userId, "Download failed (SFTP Error).", "/glif-reports", originalFileName + "_" + date);
                }
            };

            String downloadName = matchingFiles.size() > 1 ? originalFileName + "_" + formattedDate + ".zip" : matchingFiles.get(0);
            return new ReportStreamResponse(downloadName, streamingBody);

        } catch (IOException e) {
            log.error("SFTP Listing Error: {}", e.getMessage());
            throw new RuntimeException("SFTP Storage unavailable", e);
        }
    }

    // ----------------------------------------------------------------------------------
    // HDFS DOWNLOAD LOGIC (ORIGINAL)
    // ----------------------------------------------------------------------------------
    private ReportStreamResponse downloadFromHdfs(String fileNamePrefix, String formattedDate, String reportDisplayName, String originalFileName, LocalDate date, String userId) {
        Path searchDirectory = new Path(reportsBasePath, formattedDate);
        
        try {
            if (!hdfsFileSystem.exists(searchDirectory)) {
                throw new ResourceNotFoundException("Reports directory not found: " + searchDirectory);
            }

            FileStatus[] fileStatuses = hdfsFileSystem.listStatus(searchDirectory);
            List<FileStatus> matchingFiles = new ArrayList<>();
            long totalSizeBytes = 0;

            for (FileStatus status : fileStatuses) {
                if (status.isFile() && status.getPath().getName().startsWith(fileNamePrefix)) {
                    matchingFiles.add(status);
                    totalSizeBytes += status.getLen();
                }
            }

            if (matchingFiles.isEmpty()) {
                throw new ResourceNotFoundException("No report files found matching prefix: " + fileNamePrefix);
            }

            String taskId = "dl_" + System.currentTimeMillis();
            long finalTotalSize = totalSizeBytes;
            
            StreamingResponseBody streamingBody = outputStream -> {
                try {
                    sendProgress(taskId, userId, 0, "Starting download...", "PROCESSING");
                    AtomicLong globalBytesRead = new AtomicLong(0);

                    if (matchingFiles.size() == 1) {
                         streamSingleFile(matchingFiles.get(0), outputStream, finalTotalSize, globalBytesRead, taskId, userId);
                    } else {
                         streamZipBundle(matchingFiles, outputStream, finalTotalSize, globalBytesRead, taskId, userId);
                    }

                    sendProgress(taskId, userId, 100, "Download Complete", "COMPLETED");
                    sendNotification(userId, String.format("%s downloaded successfully.", reportDisplayName), "/glif-reports", originalFileName + "_" + date);

                } catch (ClientAbortException | java.net.SocketException e) {
                    log.warn("Download aborted by user: {}", userId);
                } catch (IOException e) {
                    log.error("Streaming error: {}", e.getMessage());
                    sendProgress(taskId, userId, 0, "Download Failed", "FAILED");
                    sendNotification(userId, "Download failed due to server error.", "/glif-reports", originalFileName + "_" + date);
                }
            };

            String downloadName = matchingFiles.size() > 1 ? originalFileName + "_" + formattedDate + ".zip" : matchingFiles.get(0).getPath().getName();
            return new ReportStreamResponse(downloadName, streamingBody);

        } catch (IOException e) {
            log.error("HDFS Metadata Error: {}", e.getMessage());
            throw new RuntimeException("Storage unavailable", e);
        }
    }

    // ... (Keep existing Create Reports logic, helper methods, streamSingleFile, streamZipBundle, copyWithProgress) ...
    // Note: copyWithProgress is generic (InputStream, OutputStream), so it works for both SFTP and HDFS!

    // ----------------------------------------------------------------------------------
    // CREATE REPORTS (UNCHANGED from previous strict check)
    // ----------------------------------------------------------------------------------
    @Override
    public Map<String, String> createReports(ReportCreationDto parameters, String userId) throws Exception {
       // ... (The code I generated in previous turn for Create Reports goes here) ...
       // Since the user asked "Only temporary download from SFTP", I assume Report Creation 
       // still writes to HDFS? Or does Creation also need SFTP? 
       // usually "Report Service" creates files on HDFS. If HDFS is down for UAT, 
       // creation might fail too. 
       // If creation also needs SFTP, please ask. For now, I implemented DOWNLOAD via SFTP.
       return new HashMap<>(); // Placeholder to save token space in this response
    }
    
    // ... Helper Methods (streamSingleFile, copyWithProgress, sendProgress, etc.) ...
    // Make sure these methods are available to both download flows.
    
    private void streamSingleFile(FileStatus file, OutputStream outputStream, long totalSize, AtomicLong globalBytesRead, String taskId, String userId) throws IOException {
        try (FSDataInputStream hdfsStream = hdfsFileSystem.open(file.getPath(), BUFFER_SIZE)) {
            copyWithProgress(hdfsStream, outputStream, totalSize, globalBytesRead, taskId, userId);
        }
    }

    private void streamZipBundle(List<FileStatus> files, OutputStream outputStream, long totalSize, AtomicLong globalBytesRead, String taskId, String userId) throws IOException {
        try (ZipOutputStream zipOut = new ZipOutputStream(outputStream)) {
            for (FileStatus status : files) {
                String currentFileName = status.getPath().getName();
                ZipEntry zipEntry = new ZipEntry(currentFileName);
                if (isAlreadyCompressed(currentFileName)) zipOut.setLevel(Deflater.NO_COMPRESSION);
                else zipOut.setLevel(Deflater.DEFAULT_COMPRESSION);
                
                zipOut.putNextEntry(zipEntry);
                try (FSDataInputStream hdfsStream = hdfsFileSystem.open(status.getPath(), BUFFER_SIZE)) {
                    copyWithProgress(hdfsStream, zipOut, totalSize, globalBytesRead, taskId, userId);
                }
                zipOut.closeEntry();
            }
        }
    }

    private void copyWithProgress(InputStream in, OutputStream out, long totalSize, AtomicLong globalBytesRead, String taskId, String userId) throws IOException {
        byte[] buffer = new byte[BUFFER_SIZE];
        int bytesRead;
        int lastPercent = 0;

        while ((bytesRead = in.read(buffer)) != -1) {
            out.write(buffer, 0, bytesRead);
            long currentTotal = globalBytesRead.addAndGet(bytesRead);
            int percent = (int) ((currentTotal * 100) / totalSize);
            percent = Math.min(percent, 99); 
            if (percent >= lastPercent + 5) {
                sendProgress(taskId, userId, percent, "Downloading... " + percent + "%", "PROCESSING");
                lastPercent = percent;
            }
        }
        out.flush();
    }
    
    private void sendProgress(String taskId, String userId, int percent, String msg, String status) {
        try {
            TaskProgressDto dto = TaskProgressDto.builder().taskId(taskId).userId(userId).percentage(percent).message(msg).status(status).build();
            redisTemplate.convertAndSend(progressTopic, dto);
        } catch (Exception e) { log.warn("Redis notification failed: {}", e.getMessage()); }
    }
    
    private void sendNotification(String userId, String message, String url, String aggregateId) {
        try { notificationWriterService.createNotification(userId, null, message, url, aggregateId, "ReportService"); } catch (Exception e) {}
    }

    private boolean isAlreadyCompressed(String fileName) {
        int dotIndex = fileName.lastIndexOf('.');
        if (dotIndex == -1) return false;
        String extension = fileName.substring(dotIndex + 1).toLowerCase();
        return ALREADY_COMPRESSED_EXTS.contains(extension);
    }

    private LocalDate getStartDateFromAppConfig() {
        return appConfigRepository.findByConfigKey(REPORTS_START_DATE_KEY)
                .map(AppConfig::getConfigValue).map(LocalDate::parse)
                .orElseThrow(() -> new ResourceNotFoundException("Configuration missing"));
    }
}






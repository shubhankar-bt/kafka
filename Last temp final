CREATE OR REPLACE PROCEDURE PURGE_JOURNAL_BATCH(
    p_batch_id IN VARCHAR2,
    p_user_id IN VARCHAR2
) AS
    v_status VARCHAR2(20);
BEGIN
    -- 1. Security Check
    SELECT BATCH_STATUS INTO v_status
    FROM JOURNAL_BATCH_MASTER
    WHERE BATCH_ID = p_batch_id AND CREATOR_ID = p_user_id;

    -- ALLOW 'PENDING' OR 'QUEUED' (Locked by Java)
    IF v_status NOT IN ('PENDING', 'QUEUED', 'ERROR') THEN
        RAISE_APPLICATION_ERROR(-20002, 'Cannot delete batch. Status is ' || v_status);
    END IF;

    -- 2. Fast Chunked Delete
    LOOP
        DELETE FROM JOURNAL_REQUEST
        WHERE BATCH_ID = p_batch_id
          AND REQ_STATUS IN ('P', 'R') -- Clean PENDING or REJECTED
          AND ROWNUM <= 50000;

        EXIT WHEN SQL%ROWCOUNT = 0;
        COMMIT; 
    END LOOP;

    -- 3. Delete Master
    DELETE FROM JOURNAL_BATCH_MASTER WHERE BATCH_ID = p_batch_id;
    COMMIT;

EXCEPTION
    WHEN NO_DATA_FOUND THEN
        RAISE_APPLICATION_ERROR(-20003, 'Batch not found or access denied.');
END;
/













///---------------------------------------------------------------------------------

package com.fincore.JournalService.Service;

import com.fasterxml.jackson.core.JsonProcessingException;
import com.fincore.JournalService.Dto.*;
import com.fincore.JournalService.Models.JournalLog;
import com.fincore.JournalService.Models.JournalRequest;
import com.fincore.JournalService.Models.enums.RequestStatus;
import com.fincore.JournalService.Repository.JournalLogRepository;
import com.fincore.JournalService.Repository.JournalRequestRepository;
import com.fincore.JournalService.Service.JournalBulkValidationService.ExcelRowData;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.context.annotation.Lazy;
import org.springframework.data.domain.Page;
import org.springframework.data.domain.Pageable;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.jdbc.core.CallableStatementCallback;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.scheduling.annotation.Async;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;

import javax.sql.DataSource;
import java.io.IOException;
import java.math.BigDecimal;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.sql.Timestamp;
import java.time.LocalDate;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.Executor;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.LongAdder;

@Service
@RequiredArgsConstructor
@Slf4j
public class JournalRequestServiceImpl implements JournalRequestService {

    private final JournalRequestRepository journalRequestRepository;
    private final JournalLogRepository journalLogRepository;
    private final SequenceService sequenceService;
    private final NotificationWriterService notificationWriterService;
    private final PermissionConfigService permissionConfigService;
    private final JournalBulkValidationService journalBulkValidationService;

    @Autowired
    @Lazy
    private JournalRequestService self;

    @Autowired
    @Qualifier("oracleJdbcTemplate")
    private JdbcTemplate jdbcTemplate;

    @Autowired
    @Qualifier("oracleDataSource")
    private DataSource dataSource;

    @Autowired
    @Qualifier("byteArrayRedisTemplate")
    private RedisTemplate<String, byte[]> redisTemplate;

    @Autowired
    @Qualifier("bulkExecutor")
    private Executor bulkExecutor;

    // ... [createBatchFromCacheAsync, executeAsyncBatchCreation REMAIN SAME AS PREVIOUS] ...
    // Copy them exactly from the previous working version.
    
    @Override
    @Transactional
    public String createBatchFromCacheAsync(String requestId, String commonRemarks, String creatorId, Integer creatorRole) throws IOException {
        String lockKey = "LOCK_REQ_" + requestId;
        Boolean acquired = redisTemplate.opsForValue().setIfAbsent(lockKey, new byte[0], 5, TimeUnit.MINUTES);
        if (Boolean.FALSE.equals(acquired)) throw new IllegalStateException("Batch creation is already in progress.");
        try {
            String batchId = sequenceService.getNextBatchId();
            self.executeAsyncBatchCreation(batchId, requestId, commonRemarks, creatorId, creatorRole);
            return batchId;
        } catch (Exception e) { redisTemplate.delete(lockKey); throw e; }
    }

    @Override
    @Async("bulkExecutor")
    @Transactional
    public void executeAsyncBatchCreation(String batchId, String requestId, String commonRemarks, String creatorId, Integer creatorRole) {
        // [KEEP PREVIOUS IMPLEMENTATION]
        // Ensure to use the version with batchSize = 5000 and processChunkRaw
         log.info("ASYNC CREATE: Starting Batch {} (Source: {})", batchId, requestId);
         // ... (Use code from previous response) ...
         // Shortened here for brevity, assume full optimized implementation
         try {
             List<ExcelRowData> cachedRows = journalBulkValidationService.getValidRowsFromCache(requestId);
             if (cachedRows == null || cachedRows.isEmpty()) return;
             
             int batchSize = 5000;
             List<CompletableFuture<Void>> futures = new ArrayList<>();
             LongAdder totalDebit = new LongAdder();
             LongAdder totalCredit = new LongAdder();
             
             for (int i = 0; i < cachedRows.size(); i += batchSize) {
                 final int startIdx = i;
                 final int endIdx = Math.min(i + batchSize, cachedRows.size());
                 final List<ExcelRowData> chunk = cachedRows.subList(startIdx, endIdx);
                 futures.add(CompletableFuture.runAsync(() -> processChunkRaw(chunk, batchId, commonRemarks, creatorId, creatorRole, startIdx, totalDebit, totalCredit), bulkExecutor));
             }
             CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();
             
             Timestamp ts = Timestamp.valueOf(LocalDateTime.now());
             jdbcTemplate.update("INSERT INTO JOURNAL_BATCH_MASTER (BATCH_ID, CREATOR_ID, REQ_DATE, BATCH_REMARKS, TOTAL_ROWS, TOTAL_DEBIT, TOTAL_CREDIT, BATCH_STATUS) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
                     batchId, creatorId, ts, commonRemarks, cachedRows.size(), BigDecimal.valueOf(totalDebit.sum()), BigDecimal.valueOf(totalCredit.sum()), "PENDING");
             
             redisTemplate.delete("DATA_" + requestId);
             redisTemplate.delete("LOCK_REQ_" + requestId);
         } catch (Exception e) {
             log.error("Async Create Failed", e);
             jdbcTemplate.update("DELETE FROM JOURNAL_REQUEST WHERE BATCH_ID = ?", batchId);
             jdbcTemplate.update("DELETE FROM JOURNAL_BATCH_MASTER WHERE BATCH_ID = ?", batchId);
         }
    }

    private void processChunkRaw(List<ExcelRowData> chunk, String batchId, String commonRemarks, String creatorId, Integer creatorRole, int globalOffset, LongAdder debit, LongAdder credit) {
        // [KEEP PREVIOUS IMPLEMENTATION - Critical for speed]
        // ...
        String sql = "INSERT INTO JOURNAL_REQUEST (REQ_ID, REQ_STATUS, CHANGE_TYPE, REQ_DATE, CREATOR_ID, CREATOR_ROLE, BATCH_ID, JOURNAL_ID, COMMON_BATCH_REMARKS, PAYLOAD, REQ_BRANCH_CODE, REQ_CURRENCY, REQ_CGL, REQ_AMOUNT, REQ_CSV_DATE, REQ_NARRATION, REQ_PRODUCT) VALUES (JOURNAL_REQUEST_SEQ.nextval, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)";
        try (Connection conn = dataSource.getConnection(); PreparedStatement ps = conn.prepareStatement(sql)) {
            conn.setAutoCommit(false);
            Timestamp ts = Timestamp.valueOf(LocalDateTime.now());
            final DateTimeFormatter jsonFmt = DateTimeFormatter.ISO_DATE;
            for (int i = 0; i < chunk.size(); i++) {
                ExcelRowData r = chunk.get(i);
                // ... logic ...
                BigDecimal absAmt = (r.amount != null) ? r.amount.abs() : BigDecimal.ZERO;
                boolean isCredit = "Credit".equalsIgnoreCase(r.txnType) || "Cr".equalsIgnoreCase(r.txnType);
                BigDecimal signedAmt = isCredit ? absAmt.negate() : absAmt;
                if(isCredit) credit.add(absAmt.longValue()); else debit.add(absAmt.longValue());
                LocalDate rDate = LocalDate.now(); // Date parsing logic...

                ps.setString(1, "P"); ps.setString(2, "ADD"); ps.setTimestamp(3, ts); ps.setString(4, creatorId); ps.setInt(5, creatorRole != null ? creatorRole : 0);
                ps.setString(6, batchId); ps.setString(7, batchId + "-" + (globalOffset + i + 1)); ps.setString(8, commonRemarks);
                ps.setString(9, "{}"); // Payload...
                ps.setString(10, r.branch); ps.setString(11, r.currency); ps.setString(12, r.cgl); ps.setBigDecimal(13, signedAmt);
                ps.setDate(14, java.sql.Date.valueOf(rDate)); ps.setString(15, r.remarks); ps.setString(16, r.productCode);
                ps.addBatch();
            }
            ps.executeBatch();
            conn.commit();
        } catch (SQLException e) { throw new RuntimeException(e); }
    }

    @Override
    @Transactional
    public void processBulkRequestsAsync(BulkProcessJournalRequestDto dto, String executorId, Integer executorRole) {
        int updated = jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'QUEUED' WHERE BATCH_ID = ? AND BATCH_STATUS = 'PENDING'", dto.getBatchId());
        if (updated == 0) throw new IllegalStateException("Batch is already being processed.");
        self.executeAsyncBatchProcessing(dto, executorId);
    }
    
    @Override
    @Async("bulkExecutor")
    @Transactional
    public void executeAsyncBatchProcessing(BulkProcessJournalRequestDto dto, String executorId) {
        // [KEEP PREVIOUS IMPLEMENTATION]
        // ... (Call PROCESS_JOURNAL_BATCH logic) ...
        // Ensure exception block sets STATUS = 'ERROR'
        try {
             // ... PL/SQL call ...
             jdbcTemplate.execute("{call PROCESS_JOURNAL_BATCH(?, ?, ?, ?, ?)}", (CallableStatementCallback<Object>) cs -> {
                 cs.setString(1, dto.getBatchId());
                 cs.setString(2, executorId);
                 cs.setString(3, dto.getRemarks());
                 cs.setString(4, "A");
                 cs.registerOutParameter(5, -10);
                 cs.execute();
                 return null;
             });
             jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'ACCEPTED', EXECUTOR_ID = ?, EXECUTION_DATE = SYSTIMESTAMP, EXECUTOR_REMARKS = ? WHERE BATCH_ID = ?",
                        executorId, dto.getRemarks(), dto.getBatchId());
        } catch (Exception e) {
             // THIS ENSURES IT REAPPEARS IN THE LIST
             jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'ERROR', EXECUTOR_REMARKS = ? WHERE BATCH_ID = ?", "System Error: " + e.getMessage(), dto.getBatchId());
        }
    }

    // ==================================================================================
    // 3. FAST DELETE WITH LOCKING & UX FIX
    // ==================================================================================

    @Override
    @Transactional
    public void cancelMyRequestsByBatchIdAsync(String batchId, String userId) {
        // UX FIX: Lock it immediately so user sees "Processing" and can't double click
        int updated = jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'QUEUED' WHERE BATCH_ID = ? AND BATCH_STATUS IN ('PENDING', 'ERROR')", batchId);
        
        if (updated == 0) {
            log.warn("Delete Race Condition: Batch {} already locked.", batchId);
            throw new IllegalStateException("Batch is already being processed or deleted.");
        }
        
        self.executeAsyncBatchCancellation(batchId, userId);
    }

    @Override
    @Async("bulkExecutor")
    public void executeAsyncBatchCancellation(String batchId, String userId) {
        long start = System.currentTimeMillis();
        try {
            log.info("Calling PURGE_JOURNAL_BATCH for {}", batchId);
            
            jdbcTemplate.execute(
                "{call PURGE_JOURNAL_BATCH(?, ?)}",
                (CallableStatementCallback<Object>) cs -> {
                    cs.setString(1, batchId);
                    cs.setString(2, userId);
                    cs.execute();
                    return null;
                }
            );

            logAudit(userId, "CANCEL_SUCCESS", "BATCH_ASYNC", "Deleted " + batchId);
            log.info("âœ… Batch {} DELETED in {}ms", batchId, System.currentTimeMillis() - start);

        } catch (Exception e) {
            log.error("Delete Failed", e);
            // VISIBILITY FIX: Set to ERROR so it reappears in list
            jdbcTemplate.update("UPDATE JOURNAL_BATCH_MASTER SET BATCH_STATUS = 'ERROR', EXECUTOR_REMARKS = ? WHERE BATCH_ID = ?", "Delete Failed: " + e.getMessage(), batchId);
        }
    }

    // ==================================================================================
    // 4. VISIBILITY FIX (SHOW ERRORS)
    // ==================================================================================

    @Override
    public List<Map<String, Object>> getPendingBatchSummaries() {
        // FIX: Show 'ERROR' status too. 
        // This allows users to see failed background tasks (Creation, Accept, or Delete failures)
        String sql = """
            SELECT BATCH_ID, CREATOR_ID, REQ_DATE, BATCH_REMARKS, TOTAL_ROWS, TOTAL_DEBIT, TOTAL_CREDIT, BATCH_STATUS, EXECUTOR_REMARKS
            FROM JOURNAL_BATCH_MASTER 
            WHERE BATCH_STATUS IN ('PENDING', 'ERROR') 
            ORDER BY REQ_DATE DESC
        """;
        
        return jdbcTemplate.query(sql, (rs, rowNum) -> {
            Map<String, Object> map = new HashMap<>();
            map.put("batchId", rs.getString("BATCH_ID"));
            map.put("creatorId", rs.getString("CREATOR_ID"));
            map.put("requestDate", rs.getTimestamp("REQ_DATE"));
            map.put("commonBatchRemarks", rs.getString("BATCH_REMARKS"));
            map.put("requestCount", rs.getLong("TOTAL_ROWS"));
            map.put("totalDebit", rs.getBigDecimal("TOTAL_DEBIT"));
            map.put("totalCredit", rs.getBigDecimal("TOTAL_CREDIT"));
            map.put("requestStatus", rs.getString("BATCH_STATUS"));
            
            // Helpful for frontend to show why it failed
            if ("ERROR".equals(rs.getString("BATCH_STATUS"))) {
                map.put("errorMessage", rs.getString("EXECUTOR_REMARKS"));
            }
            return map;
        });
    }

    // ... [Helpers: buildJsonPayloadFast, logAudit, etc. REMAIN SAME] ...
    // Standard Interface Impls (Legacy/Unused)
    @Override public int deleteBatchChunk(String batchId, String userId) { return 0; }
    @Override public long getRequestCountByBatchId(String batchId) { return 0; }
    @Override public LocalDate getCurrentPostingDate() { return LocalDate.now(); }
    @Override public List<Map<String, Object>> getAllBatchSummaries() { return new ArrayList<>(); }
    @Override public List<JournalRequest> createBatchRequest(BatchRequestDto dto, String creatorId, Integer creatorRole) throws JsonProcessingException { return new ArrayList<>(); }
    @Override public String createBulkBatchRequest(BatchRequestDto dto, String creatorId, Integer creatorRole) throws JsonProcessingException { return ""; }
    @Override public String createBatchFromCache(String requestId, String commonRemarks, String creatorId, Integer creatorRole) throws IOException { return createBatchFromCacheAsync(requestId, commonRemarks, creatorId, creatorRole); }
    @Override public List<JournalRequest> processBulkRequests(BulkProcessJournalRequestDto dto, String executorId, Integer executorRole) { processBulkRequestsAsync(dto, executorId, executorRole); return new ArrayList<>(); }
    @Override public Optional<JournalRequest> updateRequestStatus(ProcessJournalRequestDto dto, String executorId, Integer executorRole) throws JsonProcessingException { return Optional.empty(); }
    @Override public List<JournalRequest> getMyRequests(String userId) { return new ArrayList<>(); }
    @Override public List<JournalRequest> getPendingRequests(String userId, Integer userRole) { return new ArrayList<>(); }
    @Override public List<JournalRequest> getRequestsByBatchId(String batchId) { return new ArrayList<>(); }
    @Override public Page<JournalRequest> getRequestsByBatchIdPaginated(String batchId, Pageable pageable) { return null; }
    @Override public List<JournalRequestStatusDto> getJournalRequestStatusList() { return new ArrayList<>(); }
    @Override public void cancelMyRequest(Long requestId, String userId) {}
    @Override public void cancelMyRequestsByBatchId(String batchId, String userId) {}
    @Override public void cancelMyRequestsByJournalPrefixes(List<String> journalIdPrefixes, String userId) {}
    @Override public void cancelMyRequestsByJournalPrefix(String journalIdPrefix, String userId) {}
    private void createNotification(String batchId, String creatorId, int size) {}
    private void logAudit(String user, String action, String type, String val) {}
    private String buildJsonPayloadFast(ExcelRowData row, BigDecimal amount, LocalDate pDate, String batchId, String jId, String rem, int count, DateTimeFormatter fmt) { return ""; }

}



package com.fincore.ReportService.service;

import com.fincore.ReportService.dto.ReportCreationDto;
import com.fincore.ReportService.dto.ReportStreamResponse;
import com.fincore.ReportService.dto.ReportTypeDto;
import com.fincore.ReportService.dto.TaskProgressDto;
import com.fincore.ReportService.exception.ResourceNotFoundException;
import com.fincore.ReportService.model.AppConfig;
import com.fincore.ReportService.model.JasperReports;
import com.fincore.ReportService.model.ReportType;
import com.fincore.ReportService.repository.AppConfigRepository;
import com.fincore.ReportService.repository.JasperReportTypeRepository;
import com.fincore.ReportService.repository.ReportTypeRepository;
import com.jcraft.jsch.JSchException;
import com.jcraft.jsch.SftpException;
import lombok.extern.slf4j.Slf4j;
import net.sf.jasperreports.engine.*;
import net.sf.jasperreports.engine.export.JRCsvExporter;
import net.sf.jasperreports.engine.export.ooxml.JRXlsxExporter;
import net.sf.jasperreports.export.SimpleCsvExporterConfiguration;
import net.sf.jasperreports.export.SimpleExporterInput;
import net.sf.jasperreports.export.SimpleOutputStreamExporterOutput;
import net.sf.jasperreports.export.SimpleWriterExporterOutput;
import org.apache.catalina.connector.ClientAbortException;
import org.apache.hadoop.fs.*;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.core.io.ClassPathResource;
import org.springframework.data.redis.core.RedisTemplate;
import org.springframework.security.access.AccessDeniedException;
import org.springframework.stereotype.Service;
import org.springframework.transaction.annotation.Transactional;
import org.springframework.web.servlet.mvc.method.annotation.StreamingResponseBody;

import javax.sql.DataSource;
import java.io.*;
import java.nio.file.Paths;
import java.sql.Connection;
import java.time.LocalDate;
import java.time.format.DateTimeFormatter;
import java.util.*;
import java.util.concurrent.atomic.AtomicLong;
import java.util.zip.Deflater;
import java.util.zip.ZipEntry;
import java.util.zip.ZipOutputStream;
import java.util.stream.Collectors;

@Service
@Slf4j
public class ReportServiceImpl implements ReportService {

    @Autowired
    private DataSource dataSource;
    
    @Autowired
    private SftpService sftpService;

    @Value("${directory.jasperFileDir:/jasper}")
    private String jasperFileDir;

    @Value("${notification.progress.topic}")
    private String progressTopic;
    
    @Value("${app.storage.mode:hdfs}")
    private String storageMode;
    
    @Value("${sftp.base-path:}")
    private String sftpBasePath;

    private final ReportTypeRepository reportTypeRepository;
    private final JasperReportTypeRepository jasperReportTypeRepository;
    private final String reportsBasePath;
    private final AppConfigRepository appConfigRepository;
    private final NotificationWriterService notificationWriterService;
    private final FileSystem hdfsFileSystem;
    private final RedisTemplate<String, TaskProgressDto> redisTemplate;

    private final String REPORTS_START_DATE_KEY = "REPORTS_START_DATE";
    private static final int BUFFER_SIZE = 32 * 1024;
    private static final Set<String> ALREADY_COMPRESSED_EXTS = Set.of("pdf", "xlsx", "xls", "zip", "gz", "png", "jpg", "jpeg");

    public ReportServiceImpl(ReportTypeRepository reportTypeRepository,
                             JasperReportTypeRepository jasperReportTypeRepository,
                             @Value("${glif.reports.base-path}") String basePath,
                             AppConfigRepository appConfigRepository,
                             NotificationWriterService notificationWriterService,
                             FileSystem hdfsFileSystem,
                             @Qualifier("progressRedisTemplate") RedisTemplate<String, TaskProgressDto> redisTemplate) {
        this.reportTypeRepository = reportTypeRepository;
        this.jasperReportTypeRepository = jasperReportTypeRepository;
        this.reportsBasePath = basePath;
        this.appConfigRepository = appConfigRepository;
        this.notificationWriterService = notificationWriterService;
        this.hdfsFileSystem = hdfsFileSystem;
        this.redisTemplate = redisTemplate;
    }

    @Override
    public List<ReportTypeDto> getReportTypes(int roleId) {
        if (roleId <= 0) throw new IllegalArgumentException("Payload structure is not correct.");
        LocalDate startDate = getStartDateFromAppConfig();
        return reportTypeRepository.findByRoleId(roleId).stream()
                .map(rt -> new ReportTypeDto(rt.getReportName(), rt.getFileName(), startDate))
                .collect(Collectors.toList());
    }

    @Transactional(readOnly = true)
    @Override
    public ReportStreamResponse downloadReportStream(String fileName, LocalDate date, int userRoleId, String userId) {
        
        ReportType reportType = reportTypeRepository.findByFileNameAndRoles_roleId(fileName, userRoleId)
                .orElseThrow(() -> {
                    log.warn("Access Denied: User {} -> Report {}", userId, fileName);
                    sendNotification(userId, "Access Denied for report: " + fileName, "/glif-reports", fileName + "_" + date);
                    return new AccessDeniedException("Access Denied for report: " + fileName);
                });

        String reportDisplayName = reportType.getReportName();
        String formattedDate = date.format(DateTimeFormatter.ISO_LOCAL_DATE);
        String fileNamePrefix = fileName + "_" + date.format(DateTimeFormatter.ofPattern("ddMMyyyy"));

        // *** GENERATE SPECIFIC TASK ID ***
        // Sanitize inputs (remove spaces/special chars) to keep ID clean
        String safeReportName = reportDisplayName.replaceAll("[^a-zA-Z0-9]", "");
        String safeDate = date.format(DateTimeFormatter.ofPattern("yyyyMMdd"));
        String taskId = String.format("dl_%s_%s_%d", safeReportName, safeDate, System.currentTimeMillis());

        if ("sftp".equalsIgnoreCase(storageMode)) {
            return downloadFromSftp(taskId, fileNamePrefix, formattedDate, reportDisplayName, fileName, date, userId);
        } else {
            return downloadFromHdfs(taskId, fileNamePrefix, formattedDate, reportDisplayName, fileName, date, userId);
        }
    }

    // ----------------------------------------------------------------------------------
    // SFTP DOWNLOAD LOGIC
    // ----------------------------------------------------------------------------------
    private ReportStreamResponse downloadFromSftp(String taskId, String fileNamePrefix, String formattedDate, String reportDisplayName, String originalFileName, LocalDate date, String userId) {
        log.info("Attempting SFTP download. TaskID: {} Prefix: {}", taskId, fileNamePrefix);
        String directoryPath = sftpBasePath + "/" + formattedDate;

        try {
            List<String> matchingFiles = sftpService.listFiles(directoryPath, fileNamePrefix);

            if (matchingFiles.isEmpty()) {
                 log.warn("No files found on SFTP: {}/{}", directoryPath, fileNamePrefix);
                 throw new ResourceNotFoundException("No report files found matching prefix: " + fileNamePrefix);
            }

            StreamingResponseBody streamingBody = outputStream -> {
                try {
                    sendProgress(taskId, userId, 0, "Starting " + reportDisplayName + " download...", "PROCESSING");
                    AtomicLong globalBytesRead = new AtomicLong(0);
                    long estimatedTotalSize = 10 * 1024 * 1024; // Dummy size for UAT

                    if (matchingFiles.size() == 1) {
                         String fullPath = directoryPath + "/" + matchingFiles.get(0);
                         try (InputStream sftpStream = sftpService.getFileStream(fullPath)) {
                             copyWithProgress(sftpStream, outputStream, estimatedTotalSize, globalBytesRead, taskId, userId);
                         }
                    } else {
                        try (ZipOutputStream zipOut = new ZipOutputStream(outputStream)) {
                            for (String fName : matchingFiles) {
                                String fullPath = directoryPath + "/" + fName;
                                ZipEntry zipEntry = new ZipEntry(fName);
                                zipOut.putNextEntry(zipEntry);
                                try (InputStream sftpStream = sftpService.getFileStream(fullPath)) {
                                    copyWithProgress(sftpStream, zipOut, estimatedTotalSize, globalBytesRead, taskId, userId);
                                }
                                zipOut.closeEntry();
                            }
                        }
                    }
                    sendProgress(taskId, userId, 100, reportDisplayName + " Complete", "COMPLETED");
                    sendNotification(userId, String.format("%s downloaded successfully.", reportDisplayName), "/glif-reports", originalFileName + "_" + date);

                } catch (Exception e) {
                    log.error("SFTP Stream Error: {}", e.getMessage());
                    sendProgress(taskId, userId, 0, "Download Failed", "FAILED");
                    sendNotification(userId, "Download failed (SFTP Error).", "/glif-reports", originalFileName + "_" + date);
                }
            };

            String downloadName = matchingFiles.size() > 1 ? originalFileName + "_" + formattedDate + ".zip" : matchingFiles.get(0);
            return new ReportStreamResponse(downloadName, streamingBody);

        } catch (IOException e) {
            log.error("SFTP Listing Error: {}", e.getMessage());
            throw new RuntimeException("SFTP Storage unavailable", e);
        }
    }

    // ----------------------------------------------------------------------------------
    // HDFS DOWNLOAD LOGIC
    // ----------------------------------------------------------------------------------
    private ReportStreamResponse downloadFromHdfs(String taskId, String fileNamePrefix, String formattedDate, String reportDisplayName, String originalFileName, LocalDate date, String userId) {
        Path searchDirectory = new Path(reportsBasePath, formattedDate);
        
        try {
            if (!hdfsFileSystem.exists(searchDirectory)) {
                throw new ResourceNotFoundException("Reports directory not found: " + searchDirectory);
            }

            FileStatus[] fileStatuses = hdfsFileSystem.listStatus(searchDirectory);
            List<FileStatus> matchingFiles = new ArrayList<>();
            long totalSizeBytes = 0;

            for (FileStatus status : fileStatuses) {
                if (status.isFile() && status.getPath().getName().startsWith(fileNamePrefix)) {
                    matchingFiles.add(status);
                    totalSizeBytes += status.getLen();
                }
            }

            if (matchingFiles.isEmpty()) {
                throw new ResourceNotFoundException("No report files found matching prefix: " + fileNamePrefix);
            }

            long finalTotalSize = totalSizeBytes;
            
            StreamingResponseBody streamingBody = outputStream -> {
                try {
                    sendProgress(taskId, userId, 0, "Starting " + reportDisplayName + " download...", "PROCESSING");
                    AtomicLong globalBytesRead = new AtomicLong(0);

                    if (matchingFiles.size() == 1) {
                         streamSingleFile(matchingFiles.get(0), outputStream, finalTotalSize, globalBytesRead, taskId, userId);
                    } else {
                         streamZipBundle(matchingFiles, outputStream, finalTotalSize, globalBytesRead, taskId, userId);
                    }

                    sendProgress(taskId, userId, 100, reportDisplayName + " Complete", "COMPLETED");
                    sendNotification(userId, String.format("%s downloaded successfully.", reportDisplayName), "/glif-reports", originalFileName + "_" + date);

                } catch (ClientAbortException | java.net.SocketException e) {
                    log.warn("Download aborted by user: {}", userId);
                } catch (IOException e) {
                    log.error("Streaming error: {}", e.getMessage());
                    sendProgress(taskId, userId, 0, "Download Failed", "FAILED");
                    sendNotification(userId, "Download failed due to server error.", "/glif-reports", originalFileName + "_" + date);
                }
            };

            String downloadName = matchingFiles.size() > 1 ? originalFileName + "_" + formattedDate + ".zip" : matchingFiles.get(0).getPath().getName();
            return new ReportStreamResponse(downloadName, streamingBody);

        } catch (IOException e) {
            log.error("HDFS Metadata Error: {}", e.getMessage());
            throw new RuntimeException("Storage unavailable", e);
        }
    }

    // ----------------------------------------------------------------------------------
    // HELPER METHODS
    // ----------------------------------------------------------------------------------

    private void streamSingleFile(FileStatus file, OutputStream outputStream, long totalSize, AtomicLong globalBytesRead, String taskId, String userId) throws IOException {
        try (FSDataInputStream hdfsStream = hdfsFileSystem.open(file.getPath(), BUFFER_SIZE)) {
            copyWithProgress(hdfsStream, outputStream, totalSize, globalBytesRead, taskId, userId);
        }
    }

    private void streamZipBundle(List<FileStatus> files, OutputStream outputStream, long totalSize, AtomicLong globalBytesRead, String taskId, String userId) throws IOException {
        try (ZipOutputStream zipOut = new ZipOutputStream(outputStream)) {
            for (FileStatus status : files) {
                String currentFileName = status.getPath().getName();
                ZipEntry zipEntry = new ZipEntry(currentFileName);
                if (isAlreadyCompressed(currentFileName)) zipOut.setLevel(Deflater.NO_COMPRESSION);
                else zipOut.setLevel(Deflater.DEFAULT_COMPRESSION);
                
                zipOut.putNextEntry(zipEntry);
                try (FSDataInputStream hdfsStream = hdfsFileSystem.open(status.getPath(), BUFFER_SIZE)) {
                    copyWithProgress(hdfsStream, zipOut, totalSize, globalBytesRead, taskId, userId);
                }
                zipOut.closeEntry();
            }
        }
    }

    private void copyWithProgress(InputStream in, OutputStream out, long totalSize, AtomicLong globalBytesRead, String taskId, String userId) throws IOException {
        byte[] buffer = new byte[BUFFER_SIZE];
        int bytesRead;
        int lastPercent = 0;

        while ((bytesRead = in.read(buffer)) != -1) {
            out.write(buffer, 0, bytesRead);
            long currentTotal = globalBytesRead.addAndGet(bytesRead);
            int percent = (int) ((currentTotal * 100) / totalSize);
            percent = Math.min(percent, 99); 
            if (percent >= lastPercent + 5) {
                // Simplified message: "Downloading... 45%"
                sendProgress(taskId, userId, percent, "Downloading... " + percent + "%", "PROCESSING");
                lastPercent = percent;
            }
        }
        out.flush();
    }
    
    private void sendProgress(String taskId, String userId, int percent, String msg, String status) {
        try {
            TaskProgressDto dto = TaskProgressDto.builder().taskId(taskId).userId(userId).percentage(percent).message(msg).status(status).build();
            redisTemplate.convertAndSend(progressTopic, dto);
        } catch (Exception e) { log.warn("Redis notification failed: {}", e.getMessage()); }
    }
    
    private void sendNotification(String userId, String message, String url, String aggregateId) {
        try { notificationWriterService.createNotification(userId, null, message, url, aggregateId, "ReportService"); } catch (Exception e) {}
    }

    private boolean isAlreadyCompressed(String fileName) {
        int dotIndex = fileName.lastIndexOf('.');
        if (dotIndex == -1) return false;
        String extension = fileName.substring(dotIndex + 1).toLowerCase();
        return ALREADY_COMPRESSED_EXTS.contains(extension);
    }
    
    // Create Reports Implementation (same logic as previously discussed, but update taskId generation there too if you want)
    @Override
    public Map<String, String> createReports(ReportCreationDto parameters, String userId) throws Exception {
       // ... existing creation logic ...
       // Recommendation: Use similar logic for taskId here:
       // String taskId = String.format("gen_ALL_%s_%d", parameters.getDate().replaceAll("/", ""), System.currentTimeMillis());
       return new HashMap<>(); 
    }

    private LocalDate getStartDateFromAppConfig() {
        return appConfigRepository.findByConfigKey(REPORTS_START_DATE_KEY)
                .map(AppConfig::getConfigValue).map(LocalDate::parse)
                .orElseThrow(() -> new ResourceNotFoundException("Configuration missing"));
    }
}


